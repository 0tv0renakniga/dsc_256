{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77af0a43",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc0b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "\n",
    "def readJSON(path):\n",
    "  for l in gzip.open(path, 'rt'):\n",
    "    d = eval(l)\n",
    "    u = d['userID']\n",
    "    try:\n",
    "      g = d['gameID']\n",
    "    except Exception as e:\n",
    "      g = None\n",
    "    yield u,g,d\n",
    "\n",
    "train_json = '/home/scotty/dsc_256/fall_25/make_up/assignment1/train.json.gz'\n",
    "pairs_hours = '/home/scotty/dsc_256/fall_25/make_up/assignment1/pairs_Hours.csv'\n",
    "pairs_played = '/home/scotty/dsc_256/fall_25/make_up/assignment1/pairs_Played.csv'\n",
    "\n",
    "# create containers for users and games\n",
    "user_dict = defaultdict(list)\n",
    "game_dict = defaultdict(list)\n",
    "train_data = []\n",
    "\n",
    "# read train.json.gz and populate user_dict and game_dict\n",
    "for u,g,d in readJSON(train_json):\n",
    "    user_dict[u].append(g)\n",
    "    game_dict[g].append(u)\n",
    "    train_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23766cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from surprise import SVD, Dataset, Reader\n",
    "\n",
    "# --- 1. Setup ---\n",
    "df = pd.DataFrame(train_data)\n",
    "df['hours_transformed'] = np.log2(df['hours'] + 1)\n",
    "\n",
    "# --- 2. Ridge Feature (The \"Bias\" Expert) ---\n",
    "print(\"Generating Ridge (Bias) Features...\")\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "sparse_ids = ohe.fit_transform(df[['userID', 'gameID']])\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=3000, stop_words='english')\n",
    "sparse_text = tfidf.fit_transform(df['text'].fillna(''))\n",
    "\n",
    "X_sparse = hstack([sparse_ids, sparse_text])\n",
    "y_target = df['hours_transformed'].values\n",
    "\n",
    "# OOF Ridge\n",
    "ridge_preds = np.zeros(len(df))\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_sparse):\n",
    "    # Lower alpha to 0.1 to reduce underfitting\n",
    "    model = Ridge(alpha=0.1, solver='sag', random_state=42)\n",
    "    model.fit(X_sparse[train_idx], y_target[train_idx])\n",
    "    ridge_preds[val_idx] = model.predict(X_sparse[val_idx])\n",
    "\n",
    "df['ridge_score'] = ridge_preds\n",
    "\n",
    "# --- 3. SVD Feature (The \"Interaction\" Expert) ---\n",
    "print(\"Generating SVD (Interaction) Features...\")\n",
    "# We use Surprise here because it handles explicit rating interactions naturally\n",
    "svd_preds = np.zeros(len(df))\n",
    "reader = Reader(rating_scale=(0, df['hours_transformed'].max()))\n",
    "\n",
    "for train_idx, val_idx in kf.split(df):\n",
    "    fold_train = df.iloc[train_idx]\n",
    "    fold_val = df.iloc[val_idx]\n",
    "    \n",
    "    data_train = Dataset.load_from_df(fold_train[['userID', 'gameID', 'hours_transformed']], reader)\n",
    "    trainset = data_train.build_full_trainset()\n",
    "    \n",
    "    # Standard SVD params\n",
    "    algo = SVD(n_factors=20, n_epochs=20, lr_all=0.005, reg_all=0.02)\n",
    "    algo.fit(trainset)\n",
    "    \n",
    "    svd_preds[val_idx] = [algo.predict(row['userID'], row['gameID']).est for _, row in fold_val.iterrows()]\n",
    "\n",
    "df['svd_score'] = svd_preds\n",
    "\n",
    "# --- 4. The Final Stack (XGBoost) ---\n",
    "print(\"Training Ensemble...\")\n",
    "\n",
    "features = ['ridge_score', 'svd_score'] # The two experts\n",
    "X = df[features]\n",
    "y = df['hours_transformed']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# We let XGBoost decide how much to trust Bias (Ridge) vs Interaction (SVD)\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "train_p = xgb_model.predict(X_train)\n",
    "val_p = xgb_model.predict(X_val)\n",
    "\n",
    "print(f\"\\nFinal Train MSE: {mean_squared_error(y_train, train_p):.4f}\")\n",
    "print(f\"Final Val MSE:   {mean_squared_error(y_val, val_p):.4f}\")\n",
    "\n",
    "# --- 5. Generate Test Submission ---\n",
    "print(\"Generating Test Predictions...\")\n",
    "pairs_hours = pd.read_csv('pairs_Hours.csv')\n",
    "\n",
    "# A. Final Ridge Model (Full Data)\n",
    "final_ridge = Ridge(alpha=0.1, solver='sag', random_state=42)\n",
    "final_ridge.fit(X_sparse, y_target)\n",
    "\n",
    "# B. Final SVD Model (Full Data)\n",
    "full_data = Dataset.load_from_df(df[['userID', 'gameID', 'hours_transformed']], reader)\n",
    "full_trainset = full_data.build_full_trainset()\n",
    "final_svd = SVD(n_factors=20, n_epochs=20, lr_all=0.005, reg_all=0.02)\n",
    "final_svd.fit(full_trainset)\n",
    "\n",
    "# C. Predict Test Features\n",
    "# Ridge Prep\n",
    "test_ids = ohe.transform(pairs_hours[['userID', 'gameID']])\n",
    "test_text = csr_matrix((len(pairs_hours), 3000)) # Empty text for test\n",
    "X_test_sparse = hstack([test_ids, test_text])\n",
    "pairs_hours['ridge_score'] = final_ridge.predict(X_test_sparse)\n",
    "\n",
    "# SVD Prep\n",
    "pairs_hours['svd_score'] = [final_svd.predict(u, g).est for u, g in zip(pairs_hours['userID'], pairs_hours['gameID'])]\n",
    "\n",
    "# XGBoost Final Predict\n",
    "final_preds = xgb_model.predict(pairs_hours[features])\n",
    "\n",
    "# Clip\n",
    "min_rating = 0\n",
    "max_rating = df['hours_transformed'].max()\n",
    "pairs_hours['prediction'] = [max(min_rating, min(max_rating, p)) for p in final_preds]\n",
    "\n",
    "print(pairs_hours.head())\n",
    "# pairs_hours.to_csv('predictions_Hours_Ensemble.csv', index=False, columns=['userID', 'gameID', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80c6f09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Sparse Features...\n",
      "Sparse Matrix Shape: (175000, 12147)\n",
      "Generating OOF Ridge Features...\n",
      "  Processing Fold 1/5...\n",
      "  Processing Fold 2/5...\n",
      "  Processing Fold 3/5...\n",
      "  Processing Fold 4/5...\n",
      "  Processing Fold 5/5...\n",
      "Training Final Ridge on full dataset...\n",
      "Preparing XGBoost Data...\n",
      "Training XGBoost Stack...\n",
      "\n",
      "Final Train MSE: 2.8259\n",
      "Final Val MSE:   2.8601\n",
      "Generating predictions for Test Set...\n",
      "      userID     gameID  prediction\n",
      "0  u04763917  g51093074    3.253604\n",
      "1  u10668484  g42523222    1.368647\n",
      "2  u82502949  g39422502    4.491645\n",
      "3  u14336188  g83517324    2.587467\n",
      "4  u10096161  g10962300    2.587467\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import gc \n",
    "\n",
    "# --- 1. Setup & Data Loading ---\n",
    "df = pd.DataFrame(train_data)\n",
    "df['hours_transformed'] = np.log2(df['hours'] + 1)\n",
    "\n",
    "# --- 2. Sparse Feature Engineering (The Scikit-Learn Way) ---\n",
    "print(\"Constructing Sparse Features...\")\n",
    "\n",
    "# A. One-Hot Encoding Users and Items\n",
    "# handle_unknown='ignore' is CRITICAL. It ensures that if a user appears \n",
    "# in the test set but not the train set, their row is all 0s (Global Average).\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "# We fit on the full dataset ID space to establish the vocabulary\n",
    "ohe.fit(df[['userID', 'gameID']])\n",
    "sparse_ids = ohe.transform(df[['userID', 'gameID']])\n",
    "\n",
    "# B. Text Features (TF-IDF)\n",
    "tfidf = TfidfVectorizer(max_features=3000, stop_words='english')\n",
    "sparse_text = tfidf.fit_transform(df['text'].fillna(''))\n",
    "\n",
    "# C. Combine into one massive sparse matrix\n",
    "# This matrix has: [User_1...User_N, Game_1...Game_M, Word_1...Word_K]\n",
    "X_sparse = hstack([sparse_ids, sparse_text])\n",
    "y_target = df['hours_transformed'].values\n",
    "\n",
    "print(f\"Sparse Matrix Shape: {X_sparse.shape}\")\n",
    "\n",
    "# --- 3. Out-of-Fold (OOF) Ridge Predictions ---\n",
    "print(\"Generating OOF Ridge Features...\")\n",
    "# We replace SVD with Ridge Regression. \n",
    "# Ridge is essentially solving: y = b_u + b_i + w_text\n",
    "ridge_preds = np.zeros(len(df))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold_i, (train_idx, val_idx) in enumerate(kf.split(X_sparse)):\n",
    "    print(f\"  Processing Fold {fold_i + 1}/5...\")\n",
    "    \n",
    "    # Slice the sparse matrix\n",
    "    X_fold_train = X_sparse[train_idx]\n",
    "    y_fold_train = y_target[train_idx]\n",
    "    \n",
    "    X_fold_val = X_sparse[val_idx]\n",
    "    \n",
    "    # Train Ridge (Alpha controls regularization, similar to 'reg_all' in SVD)\n",
    "    # solver='sag' is fast for large sparse matrices\n",
    "    model = Ridge(alpha=1.0, solver='sag', random_state=42)\n",
    "    model.fit(X_fold_train, y_fold_train)\n",
    "    \n",
    "    # Predict\n",
    "    ridge_preds[val_idx] = model.predict(X_fold_val)\n",
    "\n",
    "# Add the OOF prediction to the dataframe\n",
    "df['ridge_feature'] = ridge_preds\n",
    "\n",
    "# --- 4. Train Final Ridge (For Test Set) ---\n",
    "print(\"Training Final Ridge on full dataset...\")\n",
    "final_ridge = Ridge(alpha=1.0, solver='sag', random_state=42)\n",
    "final_ridge.fit(X_sparse, y_target)\n",
    "\n",
    "# --- 5. Stacking with XGBoost ---\n",
    "print(\"Preparing XGBoost Data...\")\n",
    "\n",
    "# We no longer need separate \"Bias\" features (u_bias, i_bias) because \n",
    "# Ridge Regression ALREADY captured them perfectly in 'ridge_feature'.\n",
    "# XGBoost just needs to correct the Ridge residuals.\n",
    "\n",
    "def build_stacking_features(dataframe, ridge_scores):\n",
    "    return pd.DataFrame({\n",
    "        'ridge_score': ridge_scores,\n",
    "        'text_len': dataframe['text'].str.len().fillna(0)\n",
    "        # We can add other metadata here if available\n",
    "    })\n",
    "\n",
    "# Split for XGBoost Training\n",
    "train_idx, val_idx = train_test_split(range(len(df)), test_size=0.2, random_state=42)\n",
    "\n",
    "X_stack_train = build_stacking_features(df.iloc[train_idx], df.iloc[train_idx]['ridge_feature'])\n",
    "y_stack_train = df.iloc[train_idx]['hours_transformed']\n",
    "\n",
    "X_stack_val = build_stacking_features(df.iloc[val_idx], df.iloc[val_idx]['ridge_feature'])\n",
    "y_stack_val = df.iloc[val_idx]['hours_transformed']\n",
    "\n",
    "print(\"Training XGBoost Stack...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,      # Ridge does the heavy lifting, XGB just polishes\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_stack_train, y_stack_train, eval_set=[(X_stack_val, y_stack_val)], verbose=False)\n",
    "\n",
    "train_p = xgb_model.predict(X_stack_train)\n",
    "val_p = xgb_model.predict(X_stack_val)\n",
    "\n",
    "print(f\"\\nFinal Train MSE: {mean_squared_error(y_stack_train, train_p):.4f}\")\n",
    "print(f\"Final Val MSE:   {mean_squared_error(y_stack_val, val_p):.4f}\")\n",
    "\n",
    "# --- 6. Prediction on Test Set ---\n",
    "pairs_hours = pd.read_csv('pairs_Hours.csv')\n",
    "print(\"Generating predictions for Test Set...\")\n",
    "\n",
    "# A. Prepare Test Features for Ridge\n",
    "# We must apply the SAME OneHotEncoder and Tfidf transform\n",
    "# Note: pairs_Hours likely lacks 'text', so we pass empty strings for TFIDF part\n",
    "# Or, better yet, we just rely on IDs.\n",
    "# Limitation: If pairs_Hours doesn't have review text, we assume empty string.\n",
    "test_ids = ohe.transform(pairs_hours[['userID', 'gameID']])\n",
    "# Create dummy text matrix (all zeros) for the test set since we don't have reviews at test time\n",
    "test_text = csr_matrix((len(pairs_hours), 3000), dtype=np.float64) \n",
    "\n",
    "X_test_sparse = hstack([test_ids, test_text])\n",
    "\n",
    "# B. Predict Ridge\n",
    "test_ridge_scores = final_ridge.predict(X_test_sparse)\n",
    "\n",
    "# C. Predict XGBoost\n",
    "X_test_stack = pd.DataFrame({\n",
    "    'ridge_score': test_ridge_scores,\n",
    "    'text_len': [0] * len(pairs_hours) # No text len at test time\n",
    "})\n",
    "\n",
    "raw_predictions = xgb_model.predict(X_test_stack)\n",
    "\n",
    "# D. Clip and Save\n",
    "min_rating = 0\n",
    "max_rating = df['hours_transformed'].max()\n",
    "pairs_hours['prediction'] = [max(min_rating, min(max_rating, p)) for p in raw_predictions]\n",
    "\n",
    "print(pairs_hours.head())\n",
    "# pairs_hours.to_csv('predictions_Hours_RidgeStack.csv', index=False, columns=['userID', 'gameID', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62df08e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing Text...\n",
      "Generating OOF SVD Features...\n",
      "  Processing Fold 1/5...\n",
      "  Processing Fold 2/5...\n",
      "  Processing Fold 3/5...\n",
      "  Processing Fold 4/5...\n",
      "  Processing Fold 5/5...\n",
      "Training Final SVD on full dataset...\n",
      "Preparing XGBoost Data...\n",
      "Training XGBoost...\n",
      "\n",
      "Corrected Train MSE: 2.8833\n",
      "Corrected Val MSE:   2.8970\n",
      "Generating predictions for Test Set...\n",
      "      userID     gameID  prediction\n",
      "0  u04763917  g51093074    3.799462\n",
      "1  u10668484  g42523222    1.337331\n",
      "2  u82502949  g39422502    4.715211\n",
      "3  u14336188  g83517324    3.223988\n",
      "4  u10096161  g10962300    3.296594\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- 1. Setup & Data Loading ---\n",
    "# (Assuming 'train_data' is your list of dictionaries loaded from json)\n",
    "df = pd.DataFrame(train_data)\n",
    "df['hours_transformed'] = np.log2(df['hours'] + 1)\n",
    "\n",
    "# --- 2. NLP Pipeline (Semantic Profiles) ---\n",
    "print(\"Vectorizing Text...\")\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['text'].fillna(''))\n",
    "\n",
    "# Reduce dimensions\n",
    "svd_text = TruncatedSVD(n_components=15, random_state=42)\n",
    "text_latents = svd_text.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Map Latents to User/Item Profiles\n",
    "user_text_sum = defaultdict(lambda: np.zeros(15))\n",
    "user_text_cnt = defaultdict(int)\n",
    "item_text_sum = defaultdict(lambda: np.zeros(15))\n",
    "item_text_cnt = defaultdict(int)\n",
    "\n",
    "for idx, (u, i) in enumerate(zip(df['userID'], df['gameID'])):\n",
    "    vec = text_latents[idx]\n",
    "    user_text_sum[u] += vec\n",
    "    user_text_cnt[u] += 1\n",
    "    item_text_sum[i] += vec\n",
    "    item_text_cnt[i] += 1\n",
    "\n",
    "# Average the vectors\n",
    "user_profile = {u: user_text_sum[u] / c for u, c in user_text_cnt.items()}\n",
    "item_profile = {i: item_text_sum[i] / c for i, c in item_text_cnt.items()}\n",
    "\n",
    "def get_semantic_affinity(uid, iid):\n",
    "    # Dot product of User Profile and Item Profile\n",
    "    if uid not in user_profile or iid not in item_profile: return 0.0\n",
    "    return np.dot(user_profile[uid], item_profile[iid])\n",
    "\n",
    "# --- 3. Out-of-Fold (OOF) SVD Feature Generation ---\n",
    "print(\"Generating OOF SVD Features...\")\n",
    "df['svd_feature'] = 0.0 # Placeholder\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "reader = Reader(rating_scale=(0, df['hours_transformed'].max()))\n",
    "\n",
    "for fold_i, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "    print(f\"  Processing Fold {fold_i + 1}/5...\")\n",
    "    \n",
    "    fold_train = df.iloc[train_idx]\n",
    "    fold_val = df.iloc[val_idx]\n",
    "    \n",
    "    # Train SVD only on this fold's training data\n",
    "    data_train = Dataset.load_from_df(fold_train[['userID', 'gameID', 'hours_transformed']], reader)\n",
    "    trainset = data_train.build_full_trainset()\n",
    "    \n",
    "    model = SVD(n_factors=20, n_epochs=20, lr_all=0.005, reg_all=0.05)\n",
    "    model.fit(trainset)\n",
    "    \n",
    "    # Predict on the hold-out fold\n",
    "    preds = [model.predict(row['userID'], row['gameID']).est for _, row in fold_val.iterrows()]\n",
    "    \n",
    "    # Update the main dataframe with these \"clean\" predictions\n",
    "    df.loc[val_idx, 'svd_feature'] = preds\n",
    "\n",
    "# --- 4. Train Final SVD (For Test Set Only) ---\n",
    "print(\"Training Final SVD on full dataset...\")\n",
    "full_data = Dataset.load_from_df(df[['userID', 'gameID', 'hours_transformed']], reader)\n",
    "full_trainset = full_data.build_full_trainset()\n",
    "final_svd = SVD(n_factors=20, n_epochs=20, lr_all=0.005, reg_all=0.05)\n",
    "final_svd.fit(full_trainset)\n",
    "\n",
    "# --- 5. Feature Assembly & XGBoost Training ---\n",
    "print(\"Preparing XGBoost Data...\")\n",
    "\n",
    "# Pre-compute Global/User/Item Averages\n",
    "global_mean = df['hours_transformed'].mean()\n",
    "user_means = df.groupby('userID')['hours_transformed'].mean().to_dict()\n",
    "item_means = df.groupby('gameID')['hours_transformed'].mean().to_dict()\n",
    "\n",
    "def build_features(dataframe, mode='internal'):\n",
    "    \"\"\"\n",
    "    mode='internal': Use the pre-calculated OOF 'svd_feature' column. \n",
    "                     (For Train and Validation sets)\n",
    "    mode='external': Use 'final_svd' to predict. \n",
    "                     (For the Test set / Submission)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Semantic Affinity\n",
    "    text_scores = [get_semantic_affinity(u, i) for u, i in zip(dataframe['userID'], dataframe['gameID'])]\n",
    "    \n",
    "    # 2. SVD Score (The Critical Logic Fix)\n",
    "    if mode == 'internal':\n",
    "        # Use the column we generated in Step 3\n",
    "        svd_scores = dataframe['svd_feature'].values\n",
    "    else:\n",
    "        # Use the model trained in Step 4\n",
    "        svd_scores = [final_svd.predict(u, i).est for u, i in zip(dataframe['userID'], dataframe['gameID'])]\n",
    "\n",
    "    # 3. Bias Features\n",
    "    u_bias = [user_means.get(u, global_mean) for u in dataframe['userID']]\n",
    "    i_bias = [item_means.get(i, global_mean) for i in dataframe['gameID']]\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'svd_rating': svd_scores,\n",
    "        'semantic_affinity': text_scores,\n",
    "        'user_avg': u_bias,\n",
    "        'item_avg': i_bias\n",
    "    })\n",
    "\n",
    "# Split the dataframe that ALREADY has OOF features\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Both use mode='internal' because they come from the training file\n",
    "X_train = build_features(train_df, mode='internal')\n",
    "y_train = train_df['hours_transformed']\n",
    "\n",
    "X_val = build_features(val_df, mode='internal')\n",
    "y_val = val_df['hours_transformed']\n",
    "\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=4,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_lambda=1.0,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "train_preds = xgb_model.predict(X_train)\n",
    "val_preds = xgb_model.predict(X_val)\n",
    "\n",
    "print(f\"\\nCorrected Train MSE: {mean_squared_error(y_train, train_preds):.4f}\")\n",
    "print(f\"Corrected Val MSE:   {mean_squared_error(y_val, val_preds):.4f}\")\n",
    "\n",
    "# --- 6. Prediction on Test Set ---\n",
    "pairs_hours = pd.read_csv('pairs_Hours.csv')\n",
    "\n",
    "print(\"Generating predictions for Test Set...\")\n",
    "\n",
    "# We use mode='external' here because these pairs are new\n",
    "X_test = build_features(pairs_hours, mode='external')\n",
    "\n",
    "raw_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# Safeguards\n",
    "min_rating = 0\n",
    "max_rating = df['hours_transformed'].max()\n",
    "pairs_hours['prediction'] = [max(min_rating, min(max_rating, p)) for p in raw_predictions]\n",
    "\n",
    "print(pairs_hours.head())\n",
    "pairs_hours.to_csv('predictions_Hours.csv', index=False, columns=['userID', 'gameID', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9e22624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ChainRec...\n",
      "Epoch 1: Loss 149.3796\n",
      "Epoch 2: Loss 81.9664\n",
      "Epoch 3: Loss 63.4963\n",
      "Epoch 4: Loss 55.9711\n",
      "Epoch 5: Loss 51.1110\n",
      "Generating Predictions...\n",
      "      userID     gameID  prediction  u_idx  i_idx  raw_score\n",
      "0  u04836696  g41031307           1    264    322   0.986283\n",
      "1  u32377855  g62450068           0    250   2131   0.968902\n",
      "2  u58289072  g71021765           1   2659   1924   0.998017\n",
      "3  u74685029  g26732871           1   1459    420   0.999361\n",
      "4  u06266052  g69433247           1   3995   1923   0.993517\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import itertools\n",
    "\n",
    "# --- 1. Data Prep & Monotonic Chain ---\n",
    "# Assuming 'train_data' is a list of dicts loaded from json\n",
    "if 'train_data' not in locals():\n",
    "    # Placeholder for the clinic context; ensure this is loaded!\n",
    "    train_data = [] \n",
    "\n",
    "df = pd.DataFrame(train_data)\n",
    "\n",
    "# Transformations\n",
    "df['hours_transformed'] = np.log2(df['hours'] + 1)\n",
    "df['text_len_norm'] = df['text'].str.len().fillna(0)\n",
    "df['text_len_norm'] = df['text_len_norm'] / df['text_len_norm'].max()\n",
    "\n",
    "# Mappings (Handling Unknowns for later)\n",
    "# We reserve index 0 for \"Unknown\" users/items\n",
    "unique_users = df['userID'].unique()\n",
    "unique_items = df['gameID'].unique()\n",
    "\n",
    "user_map = {u: i+1 for i, u in enumerate(unique_users)} # Start at 1\n",
    "item_map = {i: x+1 for x, i in enumerate(unique_items)} # Start at 1\n",
    "\n",
    "df['user_idx'] = df['userID'].map(user_map)\n",
    "df['item_idx'] = df['gameID'].map(item_map)\n",
    "\n",
    "num_users = len(user_map) + 1 # +1 for the padding/unknown index\n",
    "num_items = len(item_map) + 1\n",
    "\n",
    "# Thresholds\n",
    "hours_median = df['hours_transformed'].median()\n",
    "hours_75th = df['hours_transformed'].quantile(0.75)\n",
    "text_threshold = df['text_len_norm'].mean()\n",
    "\n",
    "def create_monotonic_chain(row):\n",
    "    # Stage 1: Played (Always 1 for training data)\n",
    "    y1 = 1\n",
    "    # Stage 2: Engaged (Above Median)\n",
    "    y2 = 1 if row['hours_transformed'] > hours_median else 0\n",
    "    # Stage 3: Deeply Engaged (Above 75th)\n",
    "    y3 = 1 if row['hours_transformed'] > hours_75th else 0\n",
    "    # Stage 4: Reviewed (Has significant text)\n",
    "    y4 = 1 if row['text_len_norm'] > text_threshold else 0\n",
    "    \n",
    "    # Enforce Monotonicity Constraints\n",
    "    if y4 == 1: y3 = 1\n",
    "    if y3 == 1: y2 = 1\n",
    "    \n",
    "    return [y1, y2, y3, y4]\n",
    "\n",
    "# Create Edge Indices (The \"Target\" class for the network)\n",
    "# If chain is [1, 1, 0, 0], sum is 2. The edge is stage 2.\n",
    "df['chain_labels'] = df.apply(create_monotonic_chain, axis=1)\n",
    "df['edge_index'] = df['chain_labels'].apply(sum)\n",
    "\n",
    "# Split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['edge_index'])\n",
    "\n",
    "# Tensors\n",
    "train_u = torch.LongTensor(train_df['user_idx'].values)\n",
    "train_i = torch.LongTensor(train_df['item_idx'].values)\n",
    "train_edge = torch.LongTensor(train_df['edge_index'].values)\n",
    "\n",
    "val_u = torch.LongTensor(val_df['user_idx'].values)\n",
    "val_i = torch.LongTensor(val_df['item_idx'].values)\n",
    "val_edge = torch.LongTensor(val_df['edge_index'].values)\n",
    "\n",
    "# --- 2. The ChainRec Model ---\n",
    "class ChainRec(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_stages=4, embed_dim=16):\n",
    "        super().__init__()\n",
    "        self.num_stages = num_stages\n",
    "        \n",
    "        # Embeddings\n",
    "        self.user_emb = nn.Embedding(num_users, embed_dim, padding_idx=0)\n",
    "        self.item_emb = nn.Embedding(num_items, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Stage-specific projections\n",
    "        # We project the interaction vector into 'num_stages' outputs\n",
    "        self.stage_proj = nn.Linear(embed_dim, num_stages)\n",
    "        \n",
    "        # Biases\n",
    "        self.user_bias = nn.Embedding(num_users, 1, padding_idx=0)\n",
    "        self.item_bias = nn.Embedding(num_items, 1, padding_idx=0)\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # Init\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "\n",
    "    def forward(self, u_idx, i_idx):\n",
    "        u = self.user_emb(u_idx)\n",
    "        i = self.item_emb(i_idx)\n",
    "        \n",
    "        # Element-wise product interaction\n",
    "        interaction = u * i \n",
    "        \n",
    "        # Calculate raw deltas for each stage\n",
    "        # Shape: [batch_size, num_stages]\n",
    "        deltas = self.stage_proj(interaction)\n",
    "        \n",
    "        # Softplus to ensure positive contributions (Monotonicity constraint)\n",
    "        deltas_plus = F.softplus(deltas)\n",
    "        \n",
    "        # Cumulative sum implies: If you passed stage 4, you effectively passed 1, 2, and 3.\n",
    "        # We flip, cumsum, and flip back so stage 1 accumulates all subsequent probabilities\n",
    "        scores = torch.flip(torch.cumsum(torch.flip(deltas_plus, [1]), dim=1), [1])\n",
    "        \n",
    "        # Add biases\n",
    "        b = self.global_bias + self.user_bias(u_idx).squeeze() + self.item_bias(i_idx).squeeze()\n",
    "        \n",
    "        # Broadcast bias across stages\n",
    "        return scores + b.unsqueeze(1)\n",
    "\n",
    "def edge_loss(model, u_idx, i_idx_pos, edge_targets, i_idx_neg):\n",
    "    \"\"\"\n",
    "    Maximizes the likelihood of the specific 'edge' (highest stage reached)\n",
    "    while minimizing likelihood of negative samples.\n",
    "    \"\"\"\n",
    "    # Positive Scores\n",
    "    pos_scores = model(u_idx, i_idx_pos)\n",
    "    \n",
    "    # We want to select the score corresponding to the actual edge reached\n",
    "    # edge_targets range from 1 to 4. We map to indices 0 to 3.\n",
    "    # Note: If edge is 0 (shouldn't happen in train), we clamp.\n",
    "    gather_indices = (edge_targets - 1).clamp(min=0).unsqueeze(1)\n",
    "    s_edge_pos = pos_scores.gather(1, gather_indices).squeeze()\n",
    "    \n",
    "    # Negative Scores (Standard BPR approach)\n",
    "    # We only care about Stage 1 for negatives (Did they play it at all?)\n",
    "    neg_scores = model(u_idx, i_idx_neg)\n",
    "    s_neg = neg_scores[:, 0] \n",
    "    \n",
    "    # BPR-like Log Sigmoid Loss\n",
    "    loss = -torch.mean(F.logsigmoid(s_edge_pos - s_neg))\n",
    "    return loss\n",
    "\n",
    "# --- 3. Training Loop ---\n",
    "print(\"Training ChainRec...\")\n",
    "# Fixed params for demonstration (Use your Grid Search values here)\n",
    "embed_dim = 16\n",
    "batch_size = 1024\n",
    "epochs = 5\n",
    "lr = 0.01\n",
    "\n",
    "model = ChainRec(num_users, num_items, num_stages=4, embed_dim=embed_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "dataset = TensorDataset(train_u, train_i, train_edge)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for b_u, b_i, b_edge in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # Random Negative Sampling\n",
    "        b_neg_i = torch.randint(1, num_items, (len(b_u),))\n",
    "        \n",
    "        loss = edge_loss(model, b_u, b_i, b_edge, b_neg_i)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}: Loss {total_loss:.4f}\")\n",
    "\n",
    "# --- 4. Final Prediction Strategy ---\n",
    "print(\"Generating Predictions...\")\n",
    "model.eval()\n",
    "\n",
    "# A. Predict PLAYED (Classification)\n",
    "pairs_played = pd.read_csv('pairs_Played.csv')\n",
    "# Map Users/Items (Handle unknowns by mapping to 0)\n",
    "pairs_played['u_idx'] = pairs_played['userID'].map(lambda x: user_map.get(x, 0))\n",
    "pairs_played['i_idx'] = pairs_played['gameID'].map(lambda x: item_map.get(x, 0))\n",
    "\n",
    "u_test = torch.LongTensor(pairs_played['u_idx'].values)\n",
    "i_test = torch.LongTensor(pairs_played['i_idx'].values)\n",
    "\n",
    "with torch.no_grad():\n",
    "    scores = model(u_test, i_test)\n",
    "    # We only care about Stage 1 (Played) probability\n",
    "    probs = torch.sigmoid(scores[:, 0]).numpy()\n",
    "\n",
    "# THE RANKING HACK (Crucial for balanced test sets)\n",
    "pairs_played['raw_score'] = probs\n",
    "median_score = pairs_played['raw_score'].median()\n",
    "pairs_played['prediction'] = (pairs_played['raw_score'] > median_score).astype(int)\n",
    "\n",
    "print(pairs_played.head())\n",
    "pairs_played.to_csv('predictions_Played.csv', columns=['userID', 'gameID', 'prediction'], index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee9d2679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Balanced Validation Set...\n",
      "\n",
      "--- Evaluation Metrics ---\n",
      "ROC AUC Score: 0.6977\n",
      "Accuracy (Raw Threshold): 0.5097\n",
      "\n",
      "Confusion Matrix:\n",
      "True Negatives: 885 | False Positives: 34115\n",
      "False Negatives: 208 | True Positives: 34792\n",
      "\n",
      "--- Re-evaluating with Median Threshold ---\n",
      "New Threshold (Median): 0.9856\n",
      "Calibrated Accuracy: 0.6465\n",
      "\n",
      "New Confusion Matrix:\n",
      "True Negatives: 22629 | False Positives: 12371\n",
      "False Negatives: 12371 | True Positives: 22629\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_classification(model, val_df, user_map, item_map, num_items):\n",
    "    print(\"Building Balanced Validation Set...\")\n",
    "    \n",
    "    # 1. Positives (The real data)\n",
    "    # We take the validation split we already made\n",
    "    pos_u = [user_map.get(u, 0) for u in val_df['userID']]\n",
    "    pos_i = [item_map.get(i, 0) for i in val_df['gameID']]\n",
    "    labels = [1] * len(pos_u)\n",
    "    \n",
    "    # 2. Negatives (The fake data)\n",
    "    # We generate an equal number of random negative samples\n",
    "    neg_u = pos_u # Same users\n",
    "    neg_i = []\n",
    "    \n",
    "    # Simple negative sampling: pick a random item. \n",
    "    # In a perfect world, we verify the user hasn't actually played it, \n",
    "    # but for fast validation, random selection is usually sufficient.\n",
    "    for _ in range(len(pos_u)):\n",
    "        neg_i.append(np.random.randint(1, num_items))\n",
    "        \n",
    "    labels.extend([0] * len(neg_u))\n",
    "    \n",
    "    # Combine\n",
    "    all_u = torch.LongTensor(pos_u + neg_u)\n",
    "    all_i = torch.LongTensor(pos_i + neg_i)\n",
    "    y_true = np.array(labels)\n",
    "    \n",
    "    # 3. Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        scores = model(all_u, all_i)\n",
    "        # Get probability of Stage 1 (Played)\n",
    "        y_probs = torch.sigmoid(scores[:, 0]).numpy()\n",
    "        \n",
    "    # 4. Metrics\n",
    "    print(\"\\n--- Evaluation Metrics ---\")\n",
    "    \n",
    "    # AUC (Area Under Curve) - The best metric for ranking ability\n",
    "    # 0.5 = Random Guessing, 1.0 = Perfect\n",
    "    auc = roc_auc_score(y_true, y_probs)\n",
    "    print(f\"ROC AUC Score: {auc:.4f}\")\n",
    "    \n",
    "    # Accuracy (at 0.5 threshold)\n",
    "    y_pred = (y_probs > 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy (Raw Threshold): {acc:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Negatives: {cm[0][0]} | False Positives: {cm[0][1]}\")\n",
    "    print(f\"False Negatives: {cm[1][0]} | True Positives: {cm[1][1]}\")\n",
    "    \n",
    "    return y_true, y_probs\n",
    "\n",
    "# Run the evaluation\n",
    "y_true, y_probs = evaluate_classification(model, val_df, user_map, item_map, num_items)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "print(\"\\n--- Re-evaluating with Median Threshold ---\")\n",
    "\n",
    "# 1. Find the median score of your predictions\n",
    "# This effectively forces the model to predict exactly 50% \"Played\" and 50% \"Not Played\"\n",
    "median_thresh = np.median(y_probs)\n",
    "print(f\"New Threshold (Median): {median_thresh:.4f}\")\n",
    "\n",
    "# 2. Apply the new threshold\n",
    "y_pred_calibrated = (y_probs > median_thresh).astype(int)\n",
    "\n",
    "# 3. Recalculate Accuracy\n",
    "new_acc = accuracy_score(y_true, y_pred_calibrated)\n",
    "print(f\"Calibrated Accuracy: {new_acc:.4f}\")\n",
    "\n",
    "# 4. New Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred_calibrated)\n",
    "print(\"\\nNew Confusion Matrix:\")\n",
    "print(f\"True Negatives: {cm[0][0]} | False Positives: {cm[0][1]}\")\n",
    "print(f\"False Negatives: {cm[1][0]} | True Positives: {cm[1][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25944086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVD for Feature Extraction...\n",
      "Constructing Negative Samples...\n",
      "Building Features...\n",
      "Training XGBoost Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scotty/venvs/ucsd/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [01:33:25] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation ROC AUC: 0.7646\n",
      "Validation Accuracy: 0.7019\n",
      "\n",
      "Generating Predictions for pairs_Played.csv...\n",
      "      userID     gameID  prediction  svd_score  item_pop  user_act\n",
      "0  u04836696  g41031307           1   5.138286       144      59.0\n",
      "1  u32377855  g62450068           0   2.755522        22     173.0\n",
      "2  u58289072  g71021765           1   5.127018        81      17.0\n",
      "3  u74685029  g26732871           1   3.842400       422      26.0\n",
      "4  u06266052  g69433247           0   4.001235        35      48.0\n",
      "Test Set Threshold used: 0.4673\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import random\n",
    "\n",
    "# --- 1. Load and Prep Data ---\n",
    "# Assuming train_data is loaded\n",
    "df = pd.DataFrame(train_data)\n",
    "df['hours_transformed'] = np.log2(df['hours'] + 1)\n",
    "\n",
    "# A. Calculate Statistics (The \"Heuristic\" Features)\n",
    "item_popularity = df['gameID'].value_counts().to_dict()\n",
    "user_activity = df['userID'].value_counts().to_dict()\n",
    "global_pop_median = df['gameID'].value_counts().median()\n",
    "global_act_median = df['userID'].value_counts().median()\n",
    "\n",
    "# --- 2. Train SVD (Latent Feature Extractor) ---\n",
    "# We use SVD not to predict the final output, but to generate a \"Compatibility Score\"\n",
    "print(\"Training SVD for Feature Extraction...\")\n",
    "reader = Reader(rating_scale=(0, df['hours_transformed'].max()))\n",
    "data = Dataset.load_from_df(df[['userID', 'gameID', 'hours_transformed']], reader)\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# We use fewer factors to capture broad strokes, not noise\n",
    "svd = SVD(n_factors=10, n_epochs=10, lr_all=0.005, reg_all=0.02)\n",
    "svd.fit(trainset)\n",
    "\n",
    "def get_svd_score(uid, iid):\n",
    "    return svd.predict(uid, iid).est\n",
    "\n",
    "# --- 3. Build the Classification Dataset (Positives + Negatives) ---\n",
    "print(\"Constructing Negative Samples...\")\n",
    "\n",
    "# Positives: The actual interactions\n",
    "pos_df = df[['userID', 'gameID']].copy()\n",
    "pos_df['label'] = 1\n",
    "\n",
    "# Negatives: We need to generate roughly the same amount of 0s\n",
    "all_games = list(item_popularity.keys())\n",
    "n_negatives = len(pos_df)\n",
    "\n",
    "# Optimized Negative Sampling\n",
    "# We randomly sample users from the existing data, and assign them random games\n",
    "neg_users = df['userID'].sample(n_negatives, replace=True).values\n",
    "neg_games = random.choices(all_games, k=n_negatives)\n",
    "\n",
    "neg_df = pd.DataFrame({'userID': neg_users, 'gameID': neg_games})\n",
    "neg_df['label'] = 0\n",
    "\n",
    "# Concatenate and Shuffle\n",
    "train_class_df = pd.concat([pos_df, neg_df]).sample(frac=1.0, random_state=42)\n",
    "\n",
    "# --- 4. Feature Engineering ---\n",
    "print(\"Building Features...\")\n",
    "\n",
    "def extract_features(dataframe):\n",
    "    # 1. Compatibility (SVD Score)\n",
    "    # This captures: \"Does this user usually like this KIND of game?\"\n",
    "    dataframe['svd_score'] = [get_svd_score(u, i) for u, i in zip(dataframe['userID'], dataframe['gameID'])]\n",
    "    \n",
    "    # 2. Popularity (Global Bias)\n",
    "    # This captures: \"Is this game just generally popular?\"\n",
    "    dataframe['item_pop'] = dataframe['gameID'].map(item_popularity).fillna(0)\n",
    "    \n",
    "    # 3. Activity (User Bias)\n",
    "    # This captures: \"Does this user click 'play' on everything?\"\n",
    "    dataframe['user_act'] = dataframe['userID'].map(user_activity).fillna(0)\n",
    "    \n",
    "    return dataframe[['svd_score', 'item_pop', 'user_act']]\n",
    "\n",
    "X = extract_features(train_class_df)\n",
    "y = train_class_df['label']\n",
    "\n",
    "# --- 5. Train XGBoost Classifier ---\n",
    "print(\"Training XGBoost Classifier...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Note: We use XGBClassifier now, not Regressor\n",
    "clf = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,         # Keep it shallow to prevent overfitting\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# --- 6. Validate ---\n",
    "val_probs = clf.predict_proba(X_val)[:, 1] # Probability of Class 1\n",
    "roc = roc_auc_score(y_val, val_probs)\n",
    "print(f\"\\nValidation ROC AUC: {roc:.4f}\")\n",
    "\n",
    "# Check Accuracy at 0.5 threshold\n",
    "val_preds = (val_probs > 0.5).astype(int)\n",
    "acc = accuracy_score(y_val, val_preds)\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "\n",
    "# --- 7. Prediction on Test Set ---\n",
    "print(\"\\nGenerating Predictions for pairs_Played.csv...\")\n",
    "pairs_played = pd.read_csv('pairs_Played.csv')\n",
    "\n",
    "# Generate features for test data\n",
    "X_test = extract_features(pairs_played)\n",
    "\n",
    "# Predict Probabilities\n",
    "test_probs = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply the Median Ranking Strategy\n",
    "# Since we know the test set is 50/50, we take the top 50% as played\n",
    "threshold = np.median(test_probs)\n",
    "pairs_played['prediction'] = (test_probs > threshold).astype(int)\n",
    "\n",
    "print(pairs_played.head())\n",
    "print(f\"Test Set Threshold used: {threshold:.4f}\")\n",
    "\n",
    "# Save\n",
    "# pairs_played.to_csv('predictions_Played_XGB.csv', index=False, columns=['userID', 'gameID', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7017194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Implicit Latent Factors...\n",
      "Building Co-occurrence Dictionary...\n",
      "Constructing Negative Samples...\n",
      "Building Similarity Features...\n",
      "Training XGBoost...\n",
      "\n",
      "Validation ROC AUC: 0.7601\n",
      "Validation Accuracy (Median Thresh): 0.6989\n",
      "\n",
      "Generating Predictions...\n",
      "      userID     gameID  prediction  item_pop  user_act    gravity  raw_score\n",
      "0  u04836696  g41031307           1       144      59.0  20.376463   0.670874\n",
      "1  u32377855  g62450068           0        22     173.0  16.176188   0.326026\n",
      "2  u58289072  g71021765           1        81      17.0  12.737057   0.505203\n",
      "3  u74685029  g26732871           1       422      26.0  19.931152   0.844870\n",
      "4  u06266052  g69433247           0        35      48.0  13.946412   0.318320\n",
      "Test Threshold: 0.4683\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import random\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "df = pd.DataFrame(train_data)\n",
    "\n",
    "# --- 2. Create Implicit Features (The Fix) ---\n",
    "print(\"Generating Implicit Latent Factors...\")\n",
    "\n",
    "# A. Build the User-Item Binary Matrix\n",
    "# We want a matrix where rows=users, cols=games, value=1 (Played)\n",
    "# Since the matrix is too big for RAM, we use a trick:\n",
    "# We learn embeddings for Users and Items separately using the known pairs.\n",
    "\n",
    "# Map IDs to integers 0..N\n",
    "user_ids = df['userID'].unique()\n",
    "item_ids = df['gameID'].unique()\n",
    "u_map = {u: i for i, u in enumerate(user_ids)}\n",
    "i_map = {i: i for i, i in enumerate(item_ids)}\n",
    "\n",
    "# Create a sparse matrix-like structure using pivoting is expensive.\n",
    "# Instead, we will effectively do \"Content-Based\" filtering using User History.\n",
    "\n",
    "# B. Jaccard-Based \"Popularity Overlap\" (Very strong feature)\n",
    "# Logic: \"People who played Game X also played Game Y\"\n",
    "print(\"Building Co-occurrence Dictionary...\")\n",
    "game_users = df.groupby('gameID')['userID'].apply(set).to_dict()\n",
    "user_games = df.groupby('userID')['gameID'].apply(set).to_dict()\n",
    "\n",
    "# Statistics\n",
    "item_popularity = df['gameID'].value_counts().to_dict()\n",
    "user_activity = df['userID'].value_counts().to_dict()\n",
    "\n",
    "# --- 3. Construct Training Data (Positives + Negatives) ---\n",
    "print(\"Constructing Negative Samples...\")\n",
    "pos_df = df[['userID', 'gameID']].copy()\n",
    "pos_df['label'] = 1\n",
    "\n",
    "# Negative Sampling\n",
    "all_games = list(item_popularity.keys())\n",
    "n_negatives = len(pos_df)\n",
    "\n",
    "neg_users = df['userID'].sample(n_negatives, replace=True).values\n",
    "neg_games = random.choices(all_games, k=n_negatives)\n",
    "neg_df = pd.DataFrame({'userID': neg_users, 'gameID': neg_games})\n",
    "neg_df['label'] = 0\n",
    "\n",
    "train_class_df = pd.concat([pos_df, neg_df]).sample(frac=1.0, random_state=42)\n",
    "\n",
    "# --- 4. Feature Engineering (The \"Jaccard\" Pivot) ---\n",
    "print(\"Building Similarity Features...\")\n",
    "\n",
    "def calculate_overlap_score(row):\n",
    "    \"\"\"\n",
    "    Approximates Item-Item Similarity without the full matrix.\n",
    "    Measures: How popular is this game among the user's specific history?\n",
    "    \"\"\"\n",
    "    u = row['userID']\n",
    "    target_g = row['gameID']\n",
    "    \n",
    "    # If this is a cold-start user or game\n",
    "    if u not in user_games or target_g not in game_users:\n",
    "        return 0.0\n",
    "    \n",
    "    # Get the set of users who played the target game\n",
    "    target_g_players = game_users[target_g]\n",
    "    \n",
    "    # Get the games the user has played\n",
    "    u_history = user_games[u]\n",
    "    \n",
    "    # METRIC: Co-occurrence Sum\n",
    "    # \"Of the games the user played, how many times did they appear \n",
    "    # in the histories of people who played the target game?\"\n",
    "    # This is effectively: sum(freq(g') for g' in user_history if g' is related to target)\n",
    "    # But that's slow. \n",
    "    \n",
    "    # FASTER METRIC: Jaccard Proxy\n",
    "    # Does the user play \"popular\" games? We already have that.\n",
    "    # We want: Does the user play games that share users with the target?\n",
    "    \n",
    "    # Let's stick to pure popularity + activity stats which are O(1) lookups\n",
    "    # and add a simplified \"Jaccard\" if possible, but for speed, let's rely on\n",
    "    # the interaction of User Activity * Item Popularity in the Tree.\n",
    "    return 0.0 \n",
    "\n",
    "# REVISED FEATURE STRATEGY: \n",
    "# XGBoost is great at learning non-linear combos of basic stats.\n",
    "# Let's give it the raw ingredients to learn the \"Gravity\" model: \n",
    "# P(play) ~ (Activity * Popularity) / Distance\n",
    "\n",
    "def extract_features(dataframe):\n",
    "    # 1. Item Popularity (Global)\n",
    "    dataframe['item_pop'] = dataframe['gameID'].map(item_popularity).fillna(0)\n",
    "    \n",
    "    # 2. User Activity (Global)\n",
    "    dataframe['user_act'] = dataframe['userID'].map(user_activity).fillna(0)\n",
    "    \n",
    "    # 3. Interaction Term (Gravity)\n",
    "    dataframe['gravity'] = np.log1p(dataframe['item_pop']) * np.log1p(dataframe['user_act'])\n",
    "    \n",
    "    # 4. Target Encoding (Optional but risky - skip for now to avoid leakage)\n",
    "    \n",
    "    return dataframe[['item_pop', 'user_act', 'gravity']]\n",
    "\n",
    "X = extract_features(train_class_df)\n",
    "y = train_class_df['label']\n",
    "\n",
    "# --- 5. Train XGBoost ---\n",
    "print(\"Training XGBoost...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "    n_estimators=200,        # More trees\n",
    "    max_depth=5,             # Slightly deeper\n",
    "    learning_rate=0.05,      # Slower learning\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# --- 6. Validate ---\n",
    "val_probs = clf.predict_proba(X_val)[:, 1]\n",
    "roc = roc_auc_score(y_val, val_probs)\n",
    "print(f\"\\nValidation ROC AUC: {roc:.4f}\")\n",
    "\n",
    "# Check Median Accuracy\n",
    "threshold = np.median(val_probs)\n",
    "val_preds = (val_probs > threshold).astype(int)\n",
    "acc = accuracy_score(y_val, val_preds)\n",
    "print(f\"Validation Accuracy (Median Thresh): {acc:.4f}\")\n",
    "\n",
    "# --- 7. Prediction ---\n",
    "print(\"\\nGenerating Predictions...\")\n",
    "pairs_played = pd.read_csv('pairs_Played.csv')\n",
    "\n",
    "# Features\n",
    "X_test = extract_features(pairs_played)\n",
    "\n",
    "# Predict\n",
    "test_probs = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# RANKING STRATEGY (Crucial)\n",
    "# 1. Sort predictions\n",
    "# 2. Top 50% = 1, Bottom 50% = 0\n",
    "pairs_played['raw_score'] = test_probs\n",
    "median_thresh = pairs_played['raw_score'].median()\n",
    "pairs_played['prediction'] = (pairs_played['raw_score'] > median_thresh).astype(int)\n",
    "\n",
    "print(pairs_played.head())\n",
    "print(f\"Test Threshold: {median_thresh:.4f}\")\n",
    "\n",
    "# Save\n",
    "# pairs_played.to_csv('predictions_Played_Ranking.csv', index=False, columns=['userID', 'gameID', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a28f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Implicit Matrix...\n",
      "Extracting Latent Factors (SVD)...\n",
      "Constructing Negative Samples...\n",
      "Training Hybrid XGBoost...\n",
      "\n",
      "Validation ROC AUC: 0.8547\n",
      "Validation Accuracy (Median Thresh): 0.7686\n",
      "\n",
      "Generating Predictions for pairs_Played.csv...\n",
      "      userID     gameID  prediction  latent_score  item_pop  user_act  \\\n",
      "0  u04836696  g41031307           1      0.073228       144      59.0   \n",
      "1  u32377855  g62450068           0      0.004881        22     173.0   \n",
      "2  u58289072  g71021765           1      0.017212        81      17.0   \n",
      "3  u74685029  g26732871           1      0.181650       422      26.0   \n",
      "4  u06266052  g69433247           0      0.007638        35      48.0   \n",
      "\n",
      "     gravity  raw_score  \n",
      "0  20.376463   0.703755  \n",
      "1  16.176188   0.005892  \n",
      "2  12.737057   0.630762  \n",
      "3  19.931152   0.803112  \n",
      "4  13.946412   0.182023  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from scipy.sparse import csr_matrix\n",
    "import random\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "df = pd.DataFrame(train_data)\n",
    "\n",
    "# --- 2. Build the Implicit Interaction Matrix ---\n",
    "print(\"Building Implicit Matrix...\")\n",
    "# Map IDs to contiguous integers for matrix operations\n",
    "unique_users = df['userID'].unique()\n",
    "unique_items = df['gameID'].unique()\n",
    "\n",
    "u_map = {u: i for i, u in enumerate(unique_users)}\n",
    "i_map = {j: i for i, j in enumerate(unique_items)}\n",
    "\n",
    "row_ind = [u_map[u] for u in df['userID']]\n",
    "col_ind = [i_map[i] for i in df['gameID']]\n",
    "data_vals = np.ones(len(row_ind))\n",
    "\n",
    "# Create Sparse Matrix (Rows=Users, Cols=Games)\n",
    "# This is the \"Implicit\" matrix (1 = Played, 0 = Not Played)\n",
    "interaction_matrix = csr_matrix((data_vals, (row_ind, col_ind)), \n",
    "                                shape=(len(unique_users), len(unique_items)))\n",
    "\n",
    "# --- 3. Implicit SVD (Latent Features) ---\n",
    "print(\"Extracting Latent Factors (SVD)...\")\n",
    "# We reduce the matrix to 32 dimensions. \n",
    "# This captures \"Genre Affinity\" and \"User Taste\" without needing text.\n",
    "n_components = 32\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "\n",
    "# Fit on the sparse matrix\n",
    "user_factors = svd.fit_transform(interaction_matrix)\n",
    "item_factors = svd.components_.T\n",
    "\n",
    "# Create fast lookup dictionaries\n",
    "user_vec_dict = {u_id: user_factors[i] for u_id, i in u_map.items()}\n",
    "item_vec_dict = {i_id: item_factors[i] for i_id, i in i_map.items()}\n",
    "\n",
    "# Global Averages for Cold Start\n",
    "avg_user_vec = np.mean(user_factors, axis=0)\n",
    "avg_item_vec = np.mean(item_factors, axis=0)\n",
    "\n",
    "# --- 4. Feature Engineering Function ---\n",
    "item_popularity = df['gameID'].value_counts().to_dict()\n",
    "user_activity = df['userID'].value_counts().to_dict()\n",
    "\n",
    "def extract_features(dataframe):\n",
    "    # 1. Latent Compatibility (Dot Product of SVD vectors)\n",
    "    dots = []\n",
    "    for u, i in zip(dataframe['userID'], dataframe['gameID']):\n",
    "        u_vec = user_vec_dict.get(u, avg_user_vec)\n",
    "        i_vec = item_vec_dict.get(i, avg_item_vec)\n",
    "        dots.append(np.dot(u_vec, i_vec))\n",
    "    \n",
    "    dataframe['latent_score'] = dots\n",
    "    \n",
    "    # 2. Gravity Features (Pop/Act)\n",
    "    dataframe['item_pop'] = dataframe['gameID'].map(item_popularity).fillna(0)\n",
    "    dataframe['user_act'] = dataframe['userID'].map(user_activity).fillna(0)\n",
    "    dataframe['gravity'] = np.log1p(dataframe['item_pop']) * np.log1p(dataframe['user_act'])\n",
    "    \n",
    "    return dataframe[['latent_score', 'item_pop', 'user_act', 'gravity']]\n",
    "\n",
    "# --- 5. Construct Training Set (Positives + Negatives) ---\n",
    "print(\"Constructing Negative Samples...\")\n",
    "pos_df = df[['userID', 'gameID']].copy()\n",
    "pos_df['label'] = 1\n",
    "\n",
    "# Negative Sampling\n",
    "all_games = list(item_popularity.keys())\n",
    "n_negatives = len(pos_df)\n",
    "\n",
    "neg_users = df['userID'].sample(n_negatives, replace=True).values\n",
    "neg_games = random.choices(all_games, k=n_negatives)\n",
    "neg_df = pd.DataFrame({'userID': neg_users, 'gameID': neg_games})\n",
    "neg_df['label'] = 0\n",
    "\n",
    "train_class_df = pd.concat([pos_df, neg_df]).sample(frac=1.0, random_state=42)\n",
    "\n",
    "# --- 6. Train XGBoost ---\n",
    "print(\"Training Hybrid XGBoost...\")\n",
    "X = extract_features(train_class_df)\n",
    "y = train_class_df['label']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "    n_estimators=300,        # More trees to leverage the new latent features\n",
    "    max_depth=6,             # Deeper trees to capture interaction between Latent & Gravity\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# --- 7. Validate ---\n",
    "val_probs = clf.predict_proba(X_val)[:, 1]\n",
    "roc = roc_auc_score(y_val, val_probs)\n",
    "print(f\"\\nValidation ROC AUC: {roc:.4f}\")\n",
    "\n",
    "# Median Accuracy\n",
    "threshold = np.median(val_probs)\n",
    "val_preds = (val_probs > threshold).astype(int)\n",
    "acc = accuracy_score(y_val, val_preds)\n",
    "print(f\"Validation Accuracy (Median Thresh): {acc:.4f}\")\n",
    "\n",
    "# --- 8. Final Prediction ---\n",
    "print(\"\\nGenerating Predictions for pairs_Played.csv...\")\n",
    "pairs_played = pd.read_csv('pairs_Played.csv')\n",
    "\n",
    "# Extract Hybrid Features\n",
    "X_test = extract_features(pairs_played)\n",
    "\n",
    "# Predict\n",
    "test_probs = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Ranking Strategy\n",
    "pairs_played['raw_score'] = test_probs\n",
    "median_thresh = pairs_played['raw_score'].median()\n",
    "pairs_played['prediction'] = (pairs_played['raw_score'] > median_thresh).astype(int)\n",
    "\n",
    "print(pairs_played.head())\n",
    "pairs_played.to_csv('predictions_Played.csv', index=False, columns=['userID', 'gameID', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab4d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- 1. Setup & Text Processing (Same as before) ---\n",
    "df = pd.DataFrame(train_data)\n",
    "df['hours_transformed'] = np.log2(df['hours'] + 1)\n",
    "\n",
    "# NLP Pipeline (Keep this, it was good)\n",
    "print(\"Vectorizing Text...\")\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['text'].fillna(''))\n",
    "svd_text = TruncatedSVD(n_components=15, random_state=42)\n",
    "text_latents = svd_text.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Map Latents (Keep existing logic here)\n",
    "user_text_sum = defaultdict(lambda: np.zeros(15))\n",
    "user_text_cnt = defaultdict(int)\n",
    "item_text_sum = defaultdict(lambda: np.zeros(15))\n",
    "item_text_cnt = defaultdict(int)\n",
    "\n",
    "for idx, (u, i) in enumerate(zip(df['userID'], df['gameID'])):\n",
    "    vec = text_latents[idx]\n",
    "    user_text_sum[u] += vec\n",
    "    user_text_cnt[u] += 1\n",
    "    item_text_sum[i] += vec\n",
    "    item_text_cnt[i] += 1\n",
    "\n",
    "user_profile = {u: user_text_sum[u] / c for u, c in user_text_cnt.items()}\n",
    "item_profile = {i: item_text_sum[i] / c for i, c in item_text_cnt.items()}\n",
    "\n",
    "def get_semantic_affinity(uid, iid):\n",
    "    if uid not in user_profile or iid not in item_profile: return 0.0\n",
    "    return np.dot(user_profile[uid], item_profile[iid])\n",
    "\n",
    "# --- 2. Out-of-Fold SVD Feature Generation ---\n",
    "print(\"Generating OOF SVD Features...\")\n",
    "\n",
    "# We need a column to store the SVD predictions for the training data\n",
    "df['svd_feature'] = 0.0\n",
    "\n",
    "# 5-Fold Cross Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "reader = Reader(rating_scale=(0, df['hours_transformed'].max()))\n",
    "\n",
    "for fold_i, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "    print(f\"  Processing Fold {fold_i + 1}/5...\")\n",
    "    \n",
    "    # Create temporary Train/Val sets for this fold\n",
    "    fold_train = df.iloc[train_idx]\n",
    "    fold_val = df.iloc[val_idx]\n",
    "    \n",
    "    # Train SVD on the Train part\n",
    "    data_train = Dataset.load_from_df(fold_train[['userID', 'gameID', 'hours_transformed']], reader)\n",
    "    trainset = data_train.build_full_trainset()\n",
    "    \n",
    "    # Regularization is key here\n",
    "    model = SVD(n_factors=20, n_epochs=20, lr_all=0.005, reg_all=0.05)\n",
    "    model.fit(trainset)\n",
    "    \n",
    "    # Predict on the Val part (This mimics test time!)\n",
    "    preds = [model.predict(row['userID'], row['gameID']).est for _, row in fold_val.iterrows()]\n",
    "    \n",
    "    # Store these \"clean\" predictions in the main dataframe\n",
    "    df.loc[val_idx, 'svd_feature'] = preds\n",
    "\n",
    "# --- 3. Train Final SVD for the Actual Test/Validation Set ---\n",
    "# Now we need a model trained on ALL data to use for the final validation split\n",
    "print(\"Training Final SVD on all data...\")\n",
    "full_data = Dataset.load_from_df(df[['userID', 'gameID', 'hours_transformed']], reader)\n",
    "full_trainset = full_data.build_full_trainset()\n",
    "final_svd = SVD(n_factors=20, n_epochs=20, lr_all=0.005, reg_all=0.05)\n",
    "final_svd.fit(full_trainset)\n",
    "\n",
    "# --- 4. Feature Assembly & XGBoost ---\n",
    "\n",
    "# Now we do the Train/Val split for XGBoost\n",
    "# Note: We use the 'svd_feature' column we generated via OOF for training\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Global Averages for Fallback\n",
    "global_mean = df['hours_transformed'].mean()\n",
    "user_means = df.groupby('userID')['hours_transformed'].mean().to_dict()\n",
    "item_means = df.groupby('gameID')['hours_transformed'].mean().to_dict()\n",
    "\n",
    "def build_features(dataframe, is_training=True):\n",
    "    # 1. Semantic Affinity\n",
    "    text_scores = [get_semantic_affinity(u, i) for u, i in zip(dataframe['userID'], dataframe['gameID'])]\n",
    "    \n",
    "    # 2. SVD Score\n",
    "    if is_training:\n",
    "        # Use the OOF predictions we calculated earlier\n",
    "        svd_scores = dataframe['svd_feature'].values\n",
    "    else:\n",
    "        # For validation/test, use the Final SVD model\n",
    "        svd_scores = [final_svd.predict(u, i).est for u, i in zip(dataframe['userID'], dataframe['gameID'])]\n",
    "\n",
    "    # 3. Explicit Bias Features (Helps XGBoost correct SVD mistakes)\n",
    "    u_bias = [user_means.get(u, global_mean) for u in dataframe['userID']]\n",
    "    i_bias = [item_means.get(i, global_mean) for i in dataframe['gameID']]\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'svd_rating': svd_scores,\n",
    "        'semantic_affinity': text_scores,\n",
    "        'user_avg': u_bias,\n",
    "        'item_avg': i_bias\n",
    "    })\n",
    "\n",
    "X_train = build_features(train_df, is_training=True)\n",
    "y_train = train_df['hours_transformed']\n",
    "\n",
    "X_val = build_features(val_df, is_training=False)\n",
    "y_val = val_df['hours_transformed']\n",
    "\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.02, # Slow learner\n",
    "    max_depth=4,        # Shallow trees to prevent overfitting\n",
    "    subsample=0.7,      # Random sampling\n",
    "    colsample_bytree=0.7,\n",
    "    reg_lambda=1.0,     # L2 Regularization\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "train_preds = xgb_model.predict(X_train)\n",
    "val_preds = xgb_model.predict(X_val)\n",
    "\n",
    "print(f\"\\nFinal Train MSE: {mean_squared_error(y_train, train_preds):.4f}\")\n",
    "print(f\"Final Val MSE:   {mean_squared_error(y_val, val_preds):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92064cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.DataFrame(user_dict,columns=['userID','games_u'])\n",
    "df_users['games_u'] = df_users['games_u'].apply(set)\n",
    "df_games = pd.DataFrame(user_dict,columns=['gameID','users_g'])\n",
    "df_games['user_g'] = df_users['user_g'].apply(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c808338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking on 175000 interactions...\n",
      "Evaluating SVD...\n",
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.8199  1.7818  1.8231  1.8263  1.8050  1.8112  0.0164  \n",
      "MAE (testset)     1.3710  1.3468  1.3712  1.3725  1.3607  1.3644  0.0098  \n",
      "Fit time          2.40    2.93    2.89    2.78    2.20    2.64    0.29    \n",
      "Test time         0.44    0.34    0.42    0.34    0.25    0.36    0.07    \n",
      "  Mean RMSE: 1.8112\n",
      "  Mean MAE:  1.3644\n",
      "\n",
      "Evaluating SVDpp...\n",
      "Evaluating RMSE, MAE of algorithm SVDpp on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.9215  1.9313  1.9080  1.9048  1.9351  1.9201  0.0121  \n",
      "MAE (testset)     1.4201  1.4248  1.4090  1.4057  1.4321  1.4183  0.0098  \n",
      "Fit time          9.70    11.62   9.81    11.56   11.85   10.91   0.95    \n",
      "Test time         1.94    1.73    2.05    1.95    2.18    1.97    0.15    \n",
      "  Mean RMSE: 1.9201\n",
      "  Mean MAE:  1.4183\n",
      "\n",
      "Evaluating NMF...\n",
      "Evaluating RMSE, MAE of algorithm NMF on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.9815  1.9773  1.9783  1.9727  1.9770  1.9774  0.0028  \n",
      "MAE (testset)     1.4693  1.4652  1.4720  1.4661  1.4679  1.4681  0.0024  \n",
      "Fit time          4.72    4.52    4.52    5.50    6.40    5.13    0.73    \n",
      "Test time         0.39    0.21    0.20    0.37    0.26    0.28    0.08    \n",
      "  Mean RMSE: 1.9774\n",
      "  Mean MAE:  1.4681\n",
      "\n",
      "Evaluating KNNBaseline...\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE, MAE of algorithm KNNBaseline on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.9669  1.9581  1.9789  1.9586  1.9730  1.9671  0.0081  \n",
      "MAE (testset)     1.5001  1.4919  1.5008  1.4907  1.5032  1.4973  0.0050  \n",
      "Fit time          1.05    1.11    1.07    1.04    1.05    1.06    0.03    \n",
      "Test time         1.92    1.78    1.65    1.76    1.63    1.75    0.10    \n",
      "  Mean RMSE: 1.9671\n",
      "  Mean MAE:  1.4973\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from surprise import Reader, Dataset, SVD, SVDpp, NMF, KNNBaseline\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# 1. Prepare your DataFrame (Assume train_data is loaded)\n",
    "df = pd.DataFrame(train_data)\n",
    "# Use the transformed hours as the \"Rating\" to predict\n",
    "# We drop columns we don't need for Surprise (it only wants User, Item, Rating)\n",
    "df_surprise = df[['userID', 'gameID', 'hours_transformed']]\n",
    "\n",
    "# 2. Define the Reader\n",
    "# IMPORTANT: Define the scale. Min is 0, Max is the max transformed hour in your set.\n",
    "min_rating = df['hours_transformed'].min()\n",
    "max_rating = df['hours_transformed'].max()\n",
    "reader = Reader(rating_scale=(min_rating, max_rating))\n",
    "\n",
    "# 3. Create the Surprise Dataset\n",
    "data = Dataset.load_from_df(df_surprise[['userID', 'gameID', 'hours_transformed']], reader)\n",
    "\n",
    "# 4. Your Benchmarking Code\n",
    "# (I added SVDpp which models implicit interactions too, potentially good here)\n",
    "algo_svd = SVD()\n",
    "algo_svdpp = SVDpp() \n",
    "algo_nmf = NMF()\n",
    "algo_knn = KNNBaseline(sim_options={'name': 'pearson_baseline', 'user_based': False})\n",
    "\n",
    "algorithms = [algo_svd, algo_svdpp, algo_nmf, algo_knn]\n",
    "\n",
    "print(f\"Benchmarking on {len(df)} interactions...\")\n",
    "\n",
    "for algo in algorithms:\n",
    "    print(f\"Evaluating {algo.__class__.__name__}...\")\n",
    "    # metrics: RMSE (Root Mean Squared Error) is the standard for Hours prediction\n",
    "    results = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "    print(f\"  Mean RMSE: {results['test_rmse'].mean():.4f}\")\n",
    "    print(f\"  Mean MAE:  {results['test_mae'].mean():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c45db455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Grid Search... (This may take a while)\n",
      "\n",
      "--- Grid Search Results ---\n",
      "Best RMSE Score: 1.7671\n",
      "Best Parameters: {'n_factors': 20, 'n_epochs': 20, 'lr_all': 0.005, 'reg_all': 0.1}\n",
      "\n",
      "Retraining SVD on full dataset with best parameters...\n",
      "\n",
      "Final Predictions Sample:\n",
      "      userID     gameID  prediction\n",
      "0  u04763917  g51093074    4.125587\n",
      "1  u10668484  g42523222    1.342928\n",
      "2  u82502949  g39422502    5.397711\n",
      "3  u14336188  g83517324    3.086567\n",
      "4  u10096161  g10962300    3.164058\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "# (Assuming you have your dataframe 'df' from previous steps)\n",
    "# If loading from scratch:\n",
    "# df = pd.DataFrame(train_data) \n",
    "# Ensure you are using the TRANSFORMED hours\n",
    "df_surprise = df[['userID', 'gameID', 'hours_transformed']]\n",
    "\n",
    "# --- 2. Setup Surprise Data Objects ---\n",
    "# Define the scale (Min to Max of your transformed target)\n",
    "min_rating = df['hours_transformed'].min()\n",
    "max_rating = df['hours_transformed'].max()\n",
    "reader = Reader(rating_scale=(min_rating, max_rating))\n",
    "\n",
    "data = Dataset.load_from_df(df_surprise, reader)\n",
    "\n",
    "# --- 3. Define Hyperparameter Grid ---\n",
    "# These are the most critical parameters for SVD\n",
    "param_grid = {\n",
    "    # Number of latent factors (Dimensions of the user/item vectors)\n",
    "    # 20 is standard, 50/100 allows more complex patterns but risks overfitting\n",
    "    'n_factors': [20, 50, 100],\n",
    "    \n",
    "    # Number of epochs (iterations)\n",
    "    # Too few = underfit, Too many = waste of time (SVD converges fast)\n",
    "    'n_epochs': [20, 30,50],\n",
    "    \n",
    "    # Learning Rate\n",
    "    # 0.005 is safe, 0.01 is aggressive\n",
    "    'lr_all': [0.005, 0.01,0.0005],\n",
    "    \n",
    "    # Regularization (The most important one for you!)\n",
    "    # Higher reg (0.1) prevents the model from memorizing outliers (1000+ hour players)\n",
    "    'reg_all': [0.02, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "# --- 4. Run Grid Search ---\n",
    "print(\"Starting Grid Search... (This may take a while)\")\n",
    "# measures=['rmse'] optimizes for Root Mean Squared Error (Standard for regression)\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=-1)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# --- 5. Analyze Results ---\n",
    "print(\"\\n--- Grid Search Results ---\")\n",
    "print(f\"Best RMSE Score: {gs.best_score['rmse']:.4f}\")\n",
    "print(f\"Best Parameters: {gs.best_params['rmse']}\")\n",
    "\n",
    "# --- 6. Train Final Model with Best Parameters ---\n",
    "print(\"\\nRetraining SVD on full dataset with best parameters...\")\n",
    "\n",
    "# Extract best params\n",
    "best_params = gs.best_params['rmse']\n",
    "final_algo = SVD(\n",
    "    n_factors=best_params['n_factors'],\n",
    "    n_epochs=best_params['n_epochs'],\n",
    "    lr_all=best_params['lr_all'],\n",
    "    reg_all=best_params['reg_all']\n",
    ")\n",
    "\n",
    "# Fit on full data\n",
    "trainset = data.build_full_trainset()\n",
    "final_algo.fit(trainset)\n",
    "\n",
    "# --- 7. Generate Predictions for 'Hours' ---\n",
    "# Load Test Data\n",
    "pairs_hours = pd.read_csv('pairs_Hours.csv')\n",
    "\n",
    "def predict_hours(row):\n",
    "    # .predict returns a Prediction object, .est is the value\n",
    "    # Clip predictions to min/max to avoid crazy outliers\n",
    "    pred = final_algo.predict(row['userID'], row['gameID']).est\n",
    "    return max(min_rating, min(max_rating, pred))\n",
    "\n",
    "pairs_hours['prediction'] = pairs_hours.apply(predict_hours, axis=1)\n",
    "\n",
    "print(\"\\nFinal Predictions Sample:\")\n",
    "print(pairs_hours.head())\n",
    "\n",
    "# Save\n",
    "# pairs_hours.to_csv('prediction_Hours_SVD_Optimized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edc495e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Data...\n",
      "Training SVD (Layer 1)...\n",
      "Calculating Jaccard Similarities...\n",
      "Training XGBoost (Layer 2)...\n",
      "\n",
      "Results:\n",
      "Train MSE: 1.4400\n",
      "Val MSE:   3.4097\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.DataFrame(train_data)\n",
    "df['text_len'] = df['text'].str.len().fillna(0)\n",
    "\n",
    "# 1. Create Train/Val Split using SKLearn\n",
    "# We split the DATAFRAME first so we don't leak information\n",
    "print(\"Splitting Data...\")\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 2. Train SVD (Layer 1) ---\n",
    "print(\"Training SVD (Layer 1)...\")\n",
    "# Prepare data for Surprise\n",
    "reader = Reader(rating_scale=(df['hours_transformed'].min(), df['hours_transformed'].max()))\n",
    "train_data_surprise = Dataset.load_from_df(\n",
    "    train_df[['userID', 'gameID', 'hours_transformed']], \n",
    "    reader\n",
    ")\n",
    "trainset = train_data_surprise.build_full_trainset()\n",
    "\n",
    "# Train SVD\n",
    "svd_model = SVD(n_factors=20, n_epochs=20, lr_all=0.005, reg_all=0.01)\n",
    "svd_model.fit(trainset)\n",
    "\n",
    "# Generate SVD Feature for Train and Val\n",
    "def get_svd_score(uid, iid):\n",
    "    # .est returns the predicted rating\n",
    "    return svd_model.predict(uid, iid).est\n",
    "\n",
    "train_df['svd_feature'] = train_df.apply(lambda x: get_svd_score(x['userID'], x['gameID']), axis=1)\n",
    "val_df['svd_feature'] = val_df.apply(lambda x: get_svd_score(x['userID'], x['gameID']), axis=1)\n",
    "\n",
    "# --- 3. Feature Engineering (Similarity Scores) ---\n",
    "print(\"Calculating Jaccard Similarities...\")\n",
    "\n",
    "# A. Build Lookups (Adjacency Lists) strictly from TRAINING data\n",
    "# User -> Set of Games\n",
    "user_history = train_df.groupby('userID')['gameID'].apply(set).to_dict()\n",
    "# Game -> Set of Users\n",
    "game_history = train_df.groupby('gameID')['userID'].apply(set).to_dict()\n",
    "\n",
    "# Helper Jaccard Function\n",
    "def calculate_jaccard(set_a, set_b):\n",
    "    if not set_a or not set_b: return 0.0\n",
    "    intersection = len(set_a.intersection(set_b))\n",
    "    union = len(set_a.union(set_b))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "# Feature 1: Item-Based Jaccard\n",
    "# \"How similar is this game to other games this user has played?\"\n",
    "def get_mean_item_jaccard(row):\n",
    "    user, target_game = row['userID'], row['gameID']\n",
    "    \n",
    "    # Games the user played (from history)\n",
    "    history_games = user_history.get(user, set())\n",
    "    \n",
    "    # Target game's users\n",
    "    target_game_users = game_history.get(target_game, set())\n",
    "    \n",
    "    similarities = []\n",
    "    for other_game in history_games:\n",
    "        if other_game == target_game: continue\n",
    "        \n",
    "        # Calculate Jaccard between Target Game and Other Game\n",
    "        # (Based on the users who played them)\n",
    "        other_game_users = game_history.get(other_game, set())\n",
    "        sim = calculate_jaccard(target_game_users, other_game_users)\n",
    "        similarities.append(sim)\n",
    "        \n",
    "    return np.mean(similarities) if similarities else 0.0\n",
    "\n",
    "# Feature 2: User-Based Jaccard\n",
    "# \"How similar is this user to other users who played this game?\"\n",
    "def get_mean_user_jaccard(row):\n",
    "    target_user, game = row['userID'], row['gameID']\n",
    "    \n",
    "    # Users who played this game\n",
    "    game_users = game_history.get(game, set())\n",
    "    \n",
    "    # Target user's games\n",
    "    target_user_games = user_history.get(target_user, set())\n",
    "    \n",
    "    similarities = []\n",
    "    for other_user in game_users:\n",
    "        if other_user == target_user: continue\n",
    "        \n",
    "        # Calculate Jaccard between Target User and Other User\n",
    "        # (Based on the games they played)\n",
    "        other_user_games = user_history.get(other_user, set())\n",
    "        sim = calculate_jaccard(target_user_games, other_user_games)\n",
    "        similarities.append(sim)\n",
    "        \n",
    "    return np.mean(similarities) if similarities else 0.0\n",
    "\n",
    "# Apply Calculations (This can be slow on large data!)\n",
    "train_df['item_jaccard'] = train_df.apply(get_mean_item_jaccard, axis=1)\n",
    "train_df['user_jaccard'] = train_df.apply(get_mean_user_jaccard, axis=1)\n",
    "\n",
    "# Apply to Validation (Using Train History lookups to avoid leakage)\n",
    "val_df['item_jaccard'] = val_df.apply(get_mean_item_jaccard, axis=1)\n",
    "val_df['user_jaccard'] = val_df.apply(get_mean_user_jaccard, axis=1)\n",
    "\n",
    "# --- 4. Train XGBoost (Layer 2) ---\n",
    "print(\"Training XGBoost (Layer 2)...\")\n",
    "\n",
    "features = ['svd_feature', 'item_jaccard', 'user_jaccard', 'text_len']\n",
    "target = 'hours_transformed'\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(train_df[features], train_df[target])\n",
    "\n",
    "# --- 5. Evaluation ---\n",
    "train_preds = xgb_model.predict(train_df[features])\n",
    "val_preds = xgb_model.predict(val_df[features])\n",
    "\n",
    "train_mse = mean_squared_error(train_df[target], train_preds)\n",
    "val_mse = mean_squared_error(val_df[target], val_preds)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Train MSE: {train_mse:.4f}\")\n",
    "print(f\"Val MSE:   {val_mse:.4f}\")\n",
    "\n",
    "# --- 6. Prediction on Test Set (Optional) ---\n",
    "# When you run on the actual test set, remember:\n",
    "# 1. Calculate 'svd_feature' using the SVD model trained on FULL train data\n",
    "# 2. Calculate Jaccard features using histories from FULL train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "963096de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df['userID'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7155e0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Data...\n",
      "Vectorizing Text...\n",
      "Training Rating SVD...\n",
      "Assembling Features...\n",
      "Training XGBoost...\n",
      "\n",
      "Final Train MSE: 1.5840\n",
      "Final Val MSE:   3.2767\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load Data\n",
    "df = pd.DataFrame(train_data)\n",
    "# Ensure transformations exist\n",
    "df['hours_transformed'] = np.log2(df['hours'] + 1)\n",
    "df['text_len'] = df['text'].str.len().fillna(0)\n",
    "\n",
    "# --- 1. Split Data ---\n",
    "print(\"Splitting Data...\")\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 2. NLP Pipeline: Creating Semantic Profiles ---\n",
    "print(\"Vectorizing Text...\")\n",
    "\n",
    "# A. TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(train_df['text'].fillna(''))\n",
    "\n",
    "# B. Dimensionality Reduction (LSA)\n",
    "# We shrink 5000 words down to 15 dense features\n",
    "n_text_factors = 15\n",
    "svd_text = TruncatedSVD(n_components=n_text_factors, random_state=42)\n",
    "text_latents = svd_text.fit_transform(tfidf_matrix)\n",
    "\n",
    "# C. Map Latent Vectors to Users and Items\n",
    "# Create dictionaries to store the sum of vectors and count\n",
    "user_text_sum = defaultdict(lambda: np.zeros(n_text_factors))\n",
    "user_text_cnt = defaultdict(int)\n",
    "item_text_sum = defaultdict(lambda: np.zeros(n_text_factors))\n",
    "item_text_cnt = defaultdict(int)\n",
    "\n",
    "# Zip indices to iterate fast (avoiding .apply)\n",
    "u_ids = train_df['userID'].values\n",
    "i_ids = train_df['gameID'].values\n",
    "\n",
    "for idx, (u, i) in enumerate(zip(u_ids, i_ids)):\n",
    "    vec = text_latents[idx]\n",
    "    user_text_sum[u] += vec\n",
    "    user_text_cnt[u] += 1\n",
    "    item_text_sum[i] += vec\n",
    "    item_text_cnt[i] += 1\n",
    "\n",
    "# D. Create Average Profiles\n",
    "user_profile = {u: user_text_sum[u] / c for u, c in user_text_cnt.items()}\n",
    "item_profile = {i: item_text_sum[i] / c for i, c in item_text_cnt.items()}\n",
    "\n",
    "# E. Feature Extraction Function\n",
    "def get_semantic_affinity(uid, iid):\n",
    "    # If user or item is new (cold start), return 0 affinity\n",
    "    if uid not in user_profile or iid not in item_profile:\n",
    "        return 0.0\n",
    "    \n",
    "    u_vec = user_profile[uid]\n",
    "    i_vec = item_profile[iid]\n",
    "    \n",
    "    # Dot product as similarity score\n",
    "    return np.dot(u_vec, i_vec)\n",
    "\n",
    "# --- 3. Train Collaborative Filtering (Surprise SVD) ---\n",
    "print(\"Training Rating SVD...\")\n",
    "reader = Reader(rating_scale=(0, df['hours_transformed'].max()))\n",
    "train_data_surprise = Dataset.load_from_df(\n",
    "    train_df[['userID', 'gameID', 'hours_transformed']], \n",
    "    reader\n",
    ")\n",
    "trainset = train_data_surprise.build_full_trainset()\n",
    "\n",
    "# Increased regularization to prevent overfitting on residuals\n",
    "rating_svd = SVD(n_factors=20, n_epochs=20, lr_all=0.005, reg_all=0.02)\n",
    "rating_svd.fit(trainset)\n",
    "\n",
    "def get_rating_svd_score(uid, iid):\n",
    "    return rating_svd.predict(uid, iid).est\n",
    "\n",
    "# --- 4. Assemble Features for XGBoost ---\n",
    "print(\"Assembling Features...\")\n",
    "\n",
    "def build_features(dataframe):\n",
    "    # Vectorized operations are preferred, but for lookups, list comprehension is acceptable\n",
    "    # 1. Collaborative Filtering Score\n",
    "    cf_scores = [get_rating_svd_score(u, i) for u, i in zip(dataframe['userID'], dataframe['gameID'])]\n",
    "    \n",
    "    # 2. Semantic Affinity (Text Match)\n",
    "    text_scores = [get_semantic_affinity(u, i) for u, i in zip(dataframe['userID'], dataframe['gameID'])]\n",
    "    \n",
    "    # 3. Simple Global/Bias Stats (Much faster than Jaccard)\n",
    "    # (Optional: Add specific user/item bias lookups here if SVD misses them)\n",
    "    \n",
    "    X = pd.DataFrame({\n",
    "        'svd_rating': cf_scores,\n",
    "        'semantic_affinity': text_scores,\n",
    "        # 'text_len': dataframe['text_len'] # Be careful: Test set does NOT have this!\n",
    "    })\n",
    "    return X\n",
    "\n",
    "X_train = build_features(train_df)\n",
    "y_train = train_df['hours_transformed']\n",
    "\n",
    "X_val = build_features(val_df)\n",
    "y_val = val_df['hours_transformed']\n",
    "\n",
    "# --- 5. Train XGBoost ---\n",
    "print(\"Training XGBoost...\")\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.003, # Lower LR for better generalization\n",
    "    max_depth=5,\n",
    "    subsample=0.8,      # Reduce overfitting\n",
    "    colsample_bytree=0.8,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "# --- 6. Results ---\n",
    "train_preds = xgb_model.predict(X_train)\n",
    "val_preds = xgb_model.predict(X_val)\n",
    "\n",
    "print(f\"\\nFinal Train MSE: {mean_squared_error(y_train, train_preds):.4f}\")\n",
    "print(f\"Final Val MSE:   {mean_squared_error(y_val, val_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfdd5f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVD Layer...\n",
      "Calculating Jaccard Features...\n",
      "Training XGBoost Layer...\n",
      "\n",
      "Validation MSE: 3.5627\n",
      "\n",
      "--- Diagnostic Report ---\n",
      "\n",
      "Confusion Matrix (Rows=Actual, Cols=Predicted):\n",
      "           Low        Med        High      \n",
      "Low        6739       3721       1105      \n",
      "Medium     2937       6519       2101      \n",
      "High       676        4580       6622      \n",
      "\n",
      "Classification Report (Accuracy per Bin):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.67      0.56      0.61     11878\n",
      "         Low       0.65      0.58      0.61     11565\n",
      "      Medium       0.44      0.56      0.49     11557\n",
      "\n",
      "    accuracy                           0.57     35000\n",
      "   macro avg       0.59      0.57      0.57     35000\n",
      "weighted avg       0.59      0.57      0.57     35000\n",
      "\n",
      "\n",
      "Feature Importance:\n",
      "  svd_feature: 0.9897\n",
      "  item_jaccard: 0.0036\n",
      "  user_jaccard: 0.0024\n",
      "  text_len_norm: 0.0043\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, classification_report\n",
    "\n",
    "# --- 1. Load & Split Data ---\n",
    "if 'train_data' not in locals():\n",
    "    # Placeholder for safety\n",
    "    print(\"Warning: Loading dummy data.\")\n",
    "    train_data = [{'hours': 12.0, 'text': \"A\", 'gameID': 'g1', 'hours_transformed': 3.6, 'userID': 'u1'}] # ... (rest of dummy data)\n",
    "\n",
    "df = pd.DataFrame(train_data)\n",
    "df['text_len_norm'] = df['text'].str.len().fillna(0) / df['text'].str.len().max()\n",
    "\n",
    "# Split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 2. Layer 1: SVD Feature ---\n",
    "print(\"Training SVD Layer...\")\n",
    "reader = Reader(rating_scale=(df['hours_transformed'].min(), df['hours_transformed'].max()))\n",
    "train_data_surprise = Dataset.load_from_df(train_df[['userID', 'gameID', 'hours_transformed']], reader)\n",
    "trainset = train_data_surprise.build_full_trainset()\n",
    "\n",
    "# Using your best parameters from earlier\n",
    "svd_model = SVD(n_factors=20, n_epochs=30, lr_all=0.005, reg_all=0.05)\n",
    "svd_model.fit(trainset)\n",
    "\n",
    "def get_svd_est(uid, iid):\n",
    "    return svd_model.predict(uid, iid).est\n",
    "\n",
    "# Generate Feature\n",
    "train_df['svd_feature'] = train_df.apply(lambda x: get_svd_est(x['userID'], x['gameID']), axis=1)\n",
    "val_df['svd_feature'] = val_df.apply(lambda x: get_svd_est(x['userID'], x['gameID']), axis=1)\n",
    "\n",
    "# --- 3. Feature Engineering: Jaccard Similarity ---\n",
    "print(\"Calculating Jaccard Features...\")\n",
    "# Adjacency Lists (From Training Data ONLY)\n",
    "user_history = train_df.groupby('userID')['gameID'].apply(set).to_dict()\n",
    "game_history = train_df.groupby('gameID')['userID'].apply(set).to_dict()\n",
    "\n",
    "def calculate_jaccard(set_a, set_b):\n",
    "    if not set_a or not set_b: return 0.0\n",
    "    intersection = len(set_a.intersection(set_b))\n",
    "    union = len(set_a.union(set_b))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def get_jaccard_feats(row):\n",
    "    u, g = row['userID'], row['gameID']\n",
    "    \n",
    "    # Item-Based\n",
    "    u_games = user_history.get(u, set())\n",
    "    target_users = game_history.get(g, set())\n",
    "    \n",
    "    item_sims = []\n",
    "    for other_g in u_games:\n",
    "        if other_g == g: continue\n",
    "        other_users = game_history.get(other_g, set())\n",
    "        item_sims.append(calculate_jaccard(target_users, other_users))\n",
    "    mean_item = np.mean(item_sims) if item_sims else 0.0\n",
    "    \n",
    "    # User-Based\n",
    "    g_users = game_history.get(g, set())\n",
    "    target_games = user_history.get(u, set())\n",
    "    \n",
    "    user_sims = []\n",
    "    for other_u in g_users:\n",
    "        if other_u == u: continue\n",
    "        other_games = user_history.get(other_u, set())\n",
    "        user_sims.append(calculate_jaccard(target_games, other_games))\n",
    "    mean_user = np.mean(user_sims) if user_sims else 0.0\n",
    "    \n",
    "    return mean_item, mean_user\n",
    "\n",
    "# Apply\n",
    "train_df[['item_jaccard', 'user_jaccard']] = train_df.apply(get_jaccard_feats, axis=1, result_type='expand')\n",
    "val_df[['item_jaccard', 'user_jaccard']] = val_df.apply(get_jaccard_feats, axis=1, result_type='expand')\n",
    "\n",
    "# --- 4. Layer 2: XGBoost ---\n",
    "print(\"Training XGBoost Layer...\")\n",
    "features = ['svd_feature', 'item_jaccard', 'user_jaccard', 'text_len_norm']\n",
    "target = 'hours_transformed'\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100, learning_rate=0.05, max_depth=6, \n",
    "    objective='reg:squarederror', random_state=42\n",
    ")\n",
    "xgb_model.fit(train_df[features], train_df[target])\n",
    "\n",
    "# Predict\n",
    "val_preds = xgb_model.predict(val_df[features])\n",
    "mse = mean_squared_error(val_df[target], val_preds)\n",
    "print(f\"\\nValidation MSE: {mse:.4f}\")\n",
    "\n",
    "# --- 5. THE DIAGNOSTIC: Binned Confusion Matrix ---\n",
    "print(\"\\n--- Diagnostic Report ---\")\n",
    "\n",
    "# A. Create Bins based on Quantiles (Low / Medium / High)\n",
    "# We define \"Low\" as bottom 33%, \"High\" as top 33%\n",
    "bins = [-1, np.percentile(df[target], 33), np.percentile(df[target], 66), float('inf')]\n",
    "labels = ['Low', 'Medium', 'High']\n",
    "\n",
    "# B. Bin the Actuals and Predictions\n",
    "val_df['bin_actual'] = pd.cut(val_df[target], bins=bins, labels=labels)\n",
    "val_df['bin_pred'] = pd.cut(val_preds, bins=bins, labels=labels)\n",
    "\n",
    "# C. Generate Matrix\n",
    "cm = confusion_matrix(val_df['bin_actual'], val_df['bin_pred'], labels=labels)\n",
    "\n",
    "# Visualizing (Text format)\n",
    "print(\"\\nConfusion Matrix (Rows=Actual, Cols=Predicted):\")\n",
    "print(f\"{'':<10} {'Low':<10} {'Med':<10} {'High':<10}\")\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"{label:<10} {cm[i][0]:<10} {cm[i][1]:<10} {cm[i][2]:<10}\")\n",
    "\n",
    "print(\"\\nClassification Report (Accuracy per Bin):\")\n",
    "print(classification_report(val_df['bin_actual'], val_df['bin_pred']))\n",
    "\n",
    "# D. Feature Importance (Why did it make those decisions?)\n",
    "print(\"\\nFeature Importance:\")\n",
    "for name, score in zip(features, xgb_model.feature_importances_):\n",
    "    print(f\"  {name}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1374d01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing Text...\n",
      "Generating OOF SVD Features...\n",
      "  Processing Fold 1/5...\n",
      "  Processing Fold 2/5...\n",
      "  Processing Fold 3/5...\n",
      "  Processing Fold 4/5...\n",
      "  Processing Fold 5/5...\n",
      "Training Final SVD on all data...\n",
      "Training XGBoost...\n",
      "\n",
      "Final Train MSE: 2.8835\n",
      "Final Val MSE:   2.4813\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- 1. Setup & Text Processing (Same as before) ---\n",
    "df = pd.DataFrame(train_data)\n",
    "df['hours_transformed'] = np.log2(df['hours'] + 1)\n",
    "\n",
    "# NLP Pipeline (Keep this, it was good)\n",
    "print(\"Vectorizing Text...\")\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['text'].fillna(''))\n",
    "svd_text = TruncatedSVD(n_components=15, random_state=42)\n",
    "text_latents = svd_text.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Map Latents (Keep existing logic here)\n",
    "user_text_sum = defaultdict(lambda: np.zeros(15))\n",
    "user_text_cnt = defaultdict(int)\n",
    "item_text_sum = defaultdict(lambda: np.zeros(15))\n",
    "item_text_cnt = defaultdict(int)\n",
    "\n",
    "for idx, (u, i) in enumerate(zip(df['userID'], df['gameID'])):\n",
    "    vec = text_latents[idx]\n",
    "    user_text_sum[u] += vec\n",
    "    user_text_cnt[u] += 1\n",
    "    item_text_sum[i] += vec\n",
    "    item_text_cnt[i] += 1\n",
    "\n",
    "user_profile = {u: user_text_sum[u] / c for u, c in user_text_cnt.items()}\n",
    "item_profile = {i: item_text_sum[i] / c for i, c in item_text_cnt.items()}\n",
    "\n",
    "def get_semantic_affinity(uid, iid):\n",
    "    if uid not in user_profile or iid not in item_profile: return 0.0\n",
    "    return np.dot(user_profile[uid], item_profile[iid])\n",
    "\n",
    "# --- 2. Out-of-Fold SVD Feature Generation ---\n",
    "print(\"Generating OOF SVD Features...\")\n",
    "\n",
    "# We need a column to store the SVD predictions for the training data\n",
    "df['svd_feature'] = 0.0\n",
    "\n",
    "# 5-Fold Cross Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "reader = Reader(rating_scale=(0, df['hours_transformed'].max()))\n",
    "\n",
    "for fold_i, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "    print(f\"  Processing Fold {fold_i + 1}/5...\")\n",
    "    \n",
    "    # Create temporary Train/Val sets for this fold\n",
    "    fold_train = df.iloc[train_idx]\n",
    "    fold_val = df.iloc[val_idx]\n",
    "    \n",
    "    # Train SVD on the Train part\n",
    "    data_train = Dataset.load_from_df(fold_train[['userID', 'gameID', 'hours_transformed']], reader)\n",
    "    trainset = data_train.build_full_trainset()\n",
    "    \n",
    "    # Regularization is key here\n",
    "    model = SVD(n_factors=20, n_epochs=20, lr_all=0.005, reg_all=0.05)\n",
    "    model.fit(trainset)\n",
    "    \n",
    "    # Predict on the Val part (This mimics test time!)\n",
    "    preds = [model.predict(row['userID'], row['gameID']).est for _, row in fold_val.iterrows()]\n",
    "    \n",
    "    # Store these \"clean\" predictions in the main dataframe\n",
    "    df.loc[val_idx, 'svd_feature'] = preds\n",
    "\n",
    "# --- 3. Train Final SVD for the Actual Test/Validation Set ---\n",
    "# Now we need a model trained on ALL data to use for the final validation split\n",
    "print(\"Training Final SVD on all data...\")\n",
    "full_data = Dataset.load_from_df(df[['userID', 'gameID', 'hours_transformed']], reader)\n",
    "full_trainset = full_data.build_full_trainset()\n",
    "final_svd = SVD(n_factors=20, n_epochs=20, lr_all=0.005, reg_all=0.05)\n",
    "final_svd.fit(full_trainset)\n",
    "\n",
    "# --- 4. Feature Assembly & XGBoost ---\n",
    "\n",
    "# Now we do the Train/Val split for XGBoost\n",
    "# Note: We use the 'svd_feature' column we generated via OOF for training\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Global Averages for Fallback\n",
    "global_mean = df['hours_transformed'].mean()\n",
    "user_means = df.groupby('userID')['hours_transformed'].mean().to_dict()\n",
    "item_means = df.groupby('gameID')['hours_transformed'].mean().to_dict()\n",
    "\n",
    "def build_features(dataframe, is_training=True):\n",
    "    # 1. Semantic Affinity\n",
    "    text_scores = [get_semantic_affinity(u, i) for u, i in zip(dataframe['userID'], dataframe['gameID'])]\n",
    "    \n",
    "    # 2. SVD Score\n",
    "    if is_training:\n",
    "        # Use the OOF predictions we calculated earlier\n",
    "        svd_scores = dataframe['svd_feature'].values\n",
    "    else:\n",
    "        # For validation/test, use the Final SVD model\n",
    "        svd_scores = [final_svd.predict(u, i).est for u, i in zip(dataframe['userID'], dataframe['gameID'])]\n",
    "\n",
    "    # 3. Explicit Bias Features (Helps XGBoost correct SVD mistakes)\n",
    "    u_bias = [user_means.get(u, global_mean) for u in dataframe['userID']]\n",
    "    i_bias = [item_means.get(i, global_mean) for i in dataframe['gameID']]\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'svd_rating': svd_scores,\n",
    "        'semantic_affinity': text_scores,\n",
    "        'user_avg': u_bias,\n",
    "        'item_avg': i_bias\n",
    "    })\n",
    "\n",
    "X_train = build_features(train_df, is_training=True)\n",
    "y_train = train_df['hours_transformed']\n",
    "\n",
    "X_val = build_features(val_df, is_training=False)\n",
    "y_val = val_df['hours_transformed']\n",
    "\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.02, # Slow learner\n",
    "    max_depth=4,        # Shallow trees to prevent overfitting\n",
    "    subsample=0.7,      # Random sampling\n",
    "    colsample_bytree=0.7,\n",
    "    reg_lambda=1.0,     # L2 Regularization\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "train_preds = xgb_model.predict(X_train)\n",
    "val_preds = xgb_model.predict(X_val)\n",
    "\n",
    "print(f\"\\nFinal Train MSE: {mean_squared_error(y_train, train_preds):.4f}\")\n",
    "print(f\"Final Val MSE:   {mean_squared_error(y_val, val_preds):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
