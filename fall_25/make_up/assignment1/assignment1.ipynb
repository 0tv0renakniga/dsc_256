{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6db190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "import datetime\n",
    "\n",
    "def readJSON(path):\n",
    "  for l in gzip.open(path, 'rt'):\n",
    "    d = eval(l)\n",
    "    u = d['userID']\n",
    "    try:\n",
    "      g = d['gameID']\n",
    "    except Exception as e:\n",
    "      g = None\n",
    "    yield u,g,d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4189d84c",
   "metadata": {},
   "source": [
    "### read train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "591b53bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "user_games = defaultdict(list)\n",
    "game_users = defaultdict(list)\n",
    "\n",
    "for u,g,d in readJSON(\"train.json.gz\"):\n",
    "    user_games[u].append(g)\n",
    "    game_users[g].append(u)\n",
    "    train_data.append(d)\n",
    "\n",
    "df_train_data = pd.DataFrame(train_data)\n",
    "df_train_data.drop('user_id',axis=1,inplace=True)\n",
    "df_train_data['found_funny'] = df_train_data['found_funny'].fillna(0)\n",
    "df_train_data['compensation'] = df_train_data['compensation'].fillna(0)\n",
    "df_train_data.loc[df_train_data['compensation'] != 0,'compensation'] = 1\n",
    "df_test_hours_data = pd.read_csv(\"pairs_Hours.csv\")\n",
    "df_test_play_data = pd.read_csv(\"pairs_Played.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0229aac3",
   "metadata": {},
   "source": [
    "### define biased matrix factorization (regularized SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed7c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasedMF:\n",
    "    def __init__(self, n_factors=10, learning_rate=0.01, \n",
    "                 reg_bu=0.0, reg_bi=0.0, reg_pu=0.0, reg_qi=0.0, n_epochs=10):\n",
    "        \"\"\"\n",
    "        Biased Matrix Factorization (Regularized SVD) optimized via SGD.\n",
    "        \n",
    "        Model: r_hat = mu + b_u + b_i + dot(p_u, q_i)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_factors : int\n",
    "            Number of latent factors (dimension of vectors p_u and q_i)\n",
    "        learning_rate : float\n",
    "            Step size for Gradient Descent (eta)\n",
    "        reg_bu, reg_bi, reg_pu, reg_qi : float\n",
    "            Regularization parameters (lambda) to prevent overfitting\n",
    "        n_epochs : int\n",
    "            Number of passes over the training data\n",
    "        \"\"\"\n",
    "        self.n_factors = n_factors\n",
    "        self.lr = learning_rate\n",
    "        self.reg_bu = reg_bu\n",
    "        self.reg_bi = reg_bi\n",
    "        self.reg_pu = reg_pu\n",
    "        self.reg_qi = reg_qi\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "        # Internal state\n",
    "        self.mu = 0.0\n",
    "        self.user_bias = None  # numpy array\n",
    "        self.item_bias = None  # numpy array\n",
    "        self.P = None          # User factors (n_users x n_factors)\n",
    "        self.Q = None          # Item factors (n_items x n_factors)\n",
    "        \n",
    "        # Mappings\n",
    "        self.user2idx = {}\n",
    "        self.idx2user = {}\n",
    "        self.game2idx = {}\n",
    "        self.idx2game = {}\n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Trains the model using Stochastic Gradient Descent.\n",
    "        df columns: ['userID', 'gameID', 'hours_transformed']\n",
    "        \"\"\"\n",
    "        print(\"Initializing BiasedMF...\")\n",
    "        \n",
    "        # 1. Create Mappings\n",
    "        unique_users = df['userID'].unique()\n",
    "        unique_games = df['gameID'].unique()\n",
    "        \n",
    "        self.n_users = len(unique_users)\n",
    "        self.n_games = len(unique_games)\n",
    "        \n",
    "        self.user2idx = {u: i for i, u in enumerate(unique_users)}\n",
    "        self.game2idx = {g: i for i, g in enumerate(unique_games)}\n",
    "        \n",
    "        # 2. Initialize Parameters\n",
    "        self.mu = df['hours_transformed'].mean()\n",
    "        \n",
    "        # Biases initialized to zero\n",
    "        self.user_bias = np.zeros(self.n_users)\n",
    "        self.item_bias = np.zeros(self.n_games)\n",
    "        \n",
    "        # Latent Factors initialized with small random noise (Normal Dist)\n",
    "        # Scaling by 0.1 helps convergence\n",
    "        self.P = np.random.normal(0, 0.1, (self.n_users, self.n_factors))\n",
    "        self.Q = np.random.normal(0, 0.1, (self.n_games, self.n_factors))\n",
    "        \n",
    "        # Convert dataframe to numpy arrays for faster iteration\n",
    "        # We map string IDs to integers here for the training loop\n",
    "        users_idx = df['userID'].map(self.user2idx).values\n",
    "        games_idx = df['gameID'].map(self.game2idx).values\n",
    "        ratings = df['hours_transformed'].values\n",
    "        \n",
    "        n_samples = len(df)\n",
    "        \n",
    "        # 3. Training Loop (SGD)\n",
    "        print(f\"Starting training on {n_samples} samples for {self.n_epochs} epochs.\")\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            # Calculate MSE at start of epoch (optional, but good for monitoring)\n",
    "            # Keeping it simple here to focus on speed\n",
    "            \n",
    "            # Shuffle indices for true SGD behavior\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            \n",
    "            total_error = 0\n",
    "            \n",
    "            for i in indices:\n",
    "                u = users_idx[i]\n",
    "                g = games_idx[i]\n",
    "                r = ratings[i]\n",
    "                \n",
    "                # --- Prediction Step ---\n",
    "                # r_hat = mu + b_u + b_i + P_u . Q_i\n",
    "                dot_prod = np.dot(self.P[u], self.Q[g])\n",
    "                pred = self.mu + self.user_bias[u] + self.item_bias[g] + dot_prod\n",
    "                \n",
    "                # --- Error Calculation ---\n",
    "                err = r - pred\n",
    "                total_error += err**2\n",
    "                \n",
    "                # --- Update Rules (Gradient Descent) ---\n",
    "                # Update Biases\n",
    "                # b_u <- b_u + lr * (err - reg * b_u)\n",
    "                self.user_bias[u] += self.lr * (err - self.reg_bu * self.user_bias[u])\n",
    "                self.item_bias[g] += self.lr * (err - self.reg_bi * self.item_bias[g])\n",
    "                \n",
    "                # Update Latent Factors\n",
    "                # Note: We need to copy P[u] before updating it to update Q[g] correctly \n",
    "                # (though simultaneous update approximation is standard in code)\n",
    "                p_u_current = self.P[u].copy()\n",
    "                \n",
    "                # P_u <- P_u + lr * (err * Q_i - reg * P_u)\n",
    "                self.P[u] += self.lr * (err * self.Q[g] - self.reg_pu * self.P[u])\n",
    "                \n",
    "                # Q_i <- Q_i + lr * (err * P_u - reg * Q_i)\n",
    "                self.Q[g] += self.lr * (err * p_u_current - self.reg_qi * self.Q[g])\n",
    "            \n",
    "            mse = total_error / n_samples\n",
    "            print(f\"Epoch {epoch+1}/{self.n_epochs} - MSE: {mse:.4f}\")\n",
    "\n",
    "    def predict(self, user_id, game_id):\n",
    "        \"\"\"\n",
    "        Predict rating for user item pair.\n",
    "        Handles Cold Start by falling back to global mean.\n",
    "        \"\"\"\n",
    "        # Cold Start Check\n",
    "        if user_id not in self.user2idx or game_id not in self.game2idx:\n",
    "            # If we haven't seen the user or game, return global mean\n",
    "            # (A more advanced approach would use just user_bias or item_bias if available)\n",
    "            return self.mu\n",
    "        \n",
    "        u = self.user2idx[user_id]\n",
    "        g = self.game2idx[game_id]\n",
    "        \n",
    "        pred = self.mu + self.user_bias[u] + self.item_bias[g] + np.dot(self.P[u], self.Q[g])\n",
    "        return pred\n",
    "\n",
    "    def get_components(self, user_id, game_id):\n",
    "        \"\"\"\n",
    "        Show breakdown of prediction components for analysis.\n",
    "        \"\"\"\n",
    "        if user_id not in self.user2idx or game_id not in self.game2idx:\n",
    "            return {\"error\": \"User or Game not found in training data\"}\n",
    "            \n",
    "        u = self.user2idx[user_id]\n",
    "        g = self.game2idx[game_id]\n",
    "        \n",
    "        components = {\n",
    "            \"Global Mean\": self.mu,\n",
    "            \"User Bias\": self.user_bias[u],\n",
    "            \"Game Bias\": self.item_bias[g],\n",
    "            \"Latent Interaction\": np.dot(self.P[u], self.Q[g]),\n",
    "            \"Total Prediction\": self.predict(user_id, game_id)\n",
    "        }\n",
    "        return components\n",
    "\n",
    "    def predict_batch(self, df):\n",
    "        \"\"\"\n",
    "        Batch prediction for efficiency.\n",
    "        df: dataframe containing 'userID' and 'gameID' columns\n",
    "        Returns: list of predictions\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        # Iterating is acceptable here as we need to handle string IDs and potential cold starts\n",
    "        # Vectorization is difficult with mixed cold-start scenarios without complex masking\n",
    "        for _, row in df.iterrows():\n",
    "            pred = self.predict(row['userID'], row['gameID'])\n",
    "            predictions.append(pred)\n",
    "        return predictions\n",
    "\n",
    "# --- Example Usage (Commented out) ---\n",
    "# model = BiasedMF(n_factors=5, learning_rate=0.005, reg_bu=0.02, reg_bi=0.02, n_epochs=10)\n",
    "# model.fit(train_df)\n",
    "# preds = model.predict_batch(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72b237",
   "metadata": {},
   "source": [
    "### define hybrid model\n",
    "1. biased matrix factorization (regularized SVD)\n",
    "2. gradient boosting machine on features\n",
    "\n",
    "A \"Hybrid\" approach, specifically a Residual Learning pipeline, is often the \"secret sauce\" in winning machine learning competitions. By letting the Matrix Factorization model handle the broad patterns (latent factors) and using a Gradient Boosting Machine (GBM) to fix the errors using specific content features (like review length or release date), you get the best of both world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f5e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridMF:\n",
    "    \"\"\"\n",
    "    Two-stage hybrid:\n",
    "    1. BiasedMF learns from user/game IDs (Latent Factors)\n",
    "    2. Residual model (GBM/Ridge) learns from content features to correct MF errors\n",
    "    \"\"\"\n",
    "    def __init__(self, mf_model, residual_model_type='gbm'):\n",
    "        self.mf_model = mf_model\n",
    "        \n",
    "        if residual_model_type == 'gbm':\n",
    "            self.residual_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "        elif residual_model_type == 'ridge':\n",
    "            self.residual_model = Ridge(alpha=1.0)\n",
    "        else:\n",
    "            raise ValueError(\"residual_model must be 'gbm' or 'ridge'\")\n",
    "            \n",
    "        self.is_fitted = False\n",
    "\n",
    "    def fit(self, train_df, feature_columns):\n",
    "        \"\"\"\n",
    "        Stage 1: Train MF model\n",
    "        Stage 2: Train residual model on (True - MF_Pred) using features\n",
    "        \"\"\"\n",
    "        print(\"--- Stage 1: Training BiasedMF ---\")\n",
    "        self.mf_model.fit(train_df)\n",
    "        \n",
    "        # Get MF predictions for training set\n",
    "        print(\"--- Calculating Residuals ---\")\n",
    "        mf_preds = np.array(self.mf_model.predict_batch(train_df))\n",
    "        y_true = train_df['hours_transformed'].values\n",
    "        \n",
    "        # Residual = Actual - Predicted\n",
    "        residuals = y_true - mf_preds\n",
    "        \n",
    "        # Check for NaN features\n",
    "        X = train_df[feature_columns].fillna(0)\n",
    "        \n",
    "        print(f\"--- Stage 2: Training Residual Model ({type(self.residual_model).__name__}) ---\")\n",
    "        self.residual_model.fit(X, residuals)\n",
    "        self.is_fitted = True\n",
    "        print(\"Hybrid Training Complete.\")\n",
    "\n",
    "        # Optional: Print Feature Importance if GBM\n",
    "        if hasattr(self.residual_model, 'feature_importances_'):\n",
    "            importances = self.residual_model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            print(\"\\nTop 5 Feature Importances (Residual Model):\")\n",
    "            for f in range(min(5, len(feature_columns))):\n",
    "                print(f\"{feature_columns[indices[f]]}: {importances[indices[f]]:.4f}\")\n",
    "\n",
    "    def predict_batch(self, df, feature_columns):\n",
    "        \"\"\"\n",
    "        Hybrid prediction: MF Prediction + Residual Correction\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise Exception(\"Model not fitted yet.\")\n",
    "            \n",
    "        # 1. MF Prediction (Base)\n",
    "        mf_preds = np.array(self.mf_model.predict_batch(df))\n",
    "        \n",
    "        # 2. Residual Prediction (Correction)\n",
    "        X = df[feature_columns].fillna(0)\n",
    "        res_preds = self.residual_model.predict(X)\n",
    "        \n",
    "        # 3. Combine\n",
    "        final_preds = mf_preds + res_preds\n",
    "        \n",
    "        # Clip to valid range if known (e.g., hours >= 0)\n",
    "        # hours_transformed = log2(hours+1), so it must be >= 0\n",
    "        final_preds = np.maximum(final_preds, 0)\n",
    "        \n",
    "        return final_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b3728",
   "metadata": {},
   "source": [
    "### create new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ad0ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_features(df):\n",
    "    \"\"\"\n",
    "    Create powerful features for predicting 'hours_transformed'.\n",
    "    \n",
    "    Note: For a rigorous production pipeline, aggregate features (means) \n",
    "    should be computed on the Train set and mapped to Test. \n",
    "    Here, we compute them on the provided dataframe for simplicity/demonstration.\n",
    "    \"\"\"\n",
    "    df_feat = df.copy()\n",
    "    \n",
    "    print(\"Generating features...\")\n",
    "    \n",
    "    # --- 1. User Features ---\n",
    "    # User Activity Level\n",
    "    if 'userID' in df_feat.columns:\n",
    "        # (Self-join aggregation)\n",
    "        user_counts = df_feat['userID'].value_counts()\n",
    "        df_feat['user_game_count'] = df_feat['userID'].map(user_counts)\n",
    "        \n",
    "        # User Mean Hours (Target Leakage warning if used on Train without CV, but useful signal)\n",
    "        if 'hours_transformed' in df_feat.columns:\n",
    "            user_means = df_feat.groupby('userID')['hours_transformed'].transform('mean')\n",
    "            df_feat['user_mean_hours'] = user_means\n",
    "        else:\n",
    "            df_feat['user_mean_hours'] = 0 # Placeholder for Test set if not mapped\n",
    "\n",
    "    # --- 2. Game Features ---\n",
    "    # Game Popularity\n",
    "    if 'gameID' in df_feat.columns:\n",
    "        game_counts = df_feat['gameID'].value_counts()\n",
    "        df_feat['game_user_count'] = df_feat['gameID'].map(game_counts)\n",
    "        \n",
    "        if 'hours_transformed' in df_feat.columns:\n",
    "            game_means = df_feat.groupby('gameID')['hours_transformed'].transform('mean')\n",
    "            df_feat['game_mean_hours'] = game_means\n",
    "        else:\n",
    "            df_feat['game_mean_hours'] = 0\n",
    "\n",
    "    # --- 3. Text Features ---\n",
    "    if 'text' in df_feat.columns:\n",
    "        # Length of review (longer reviews might indicate more passion/playtime)\n",
    "        df_feat['review_length'] = df_feat['text'].fillna('').apply(len)\n",
    "    else:\n",
    "        df_feat['review_length'] = 0\n",
    "\n",
    "    # --- 4. Time Features ---\n",
    "    if 'date' in df_feat.columns:\n",
    "        # Parse date\n",
    "        # Assuming format \"%Y-%m-%d\" based on prompt\n",
    "        df_feat['date_dt'] = pd.to_datetime(df_feat['date'], errors='coerce')\n",
    "        \n",
    "        # Day of week (0=Monday, 6=Sunday)\n",
    "        df_feat['day_of_week'] = df_feat['date_dt'].dt.dayofweek\n",
    "        \n",
    "        # Is Weekend?\n",
    "        df_feat['is_weekend'] = df_feat['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "        \n",
    "        # Month (Seasonality?)\n",
    "        df_feat['month'] = df_feat['date_dt'].dt.month\n",
    "        \n",
    "        # Years since review (Vintage)\n",
    "        current_year = 2024 # Or max year in data\n",
    "        df_feat['years_since_review'] = current_year - df_feat['date_dt'].dt.year\n",
    "    else:\n",
    "        df_feat['day_of_week'] = 0\n",
    "        df_feat['is_weekend'] = 0\n",
    "        df_feat['years_since_review'] = 0\n",
    "\n",
    "    # --- 5. Engagement/Interaction Features ---\n",
    "    if 'found_funny' in df_feat.columns:\n",
    "        # Funny votes might correlate with popularity or meme-games\n",
    "        df_feat['found_funny'] = df_feat['found_funny'].fillna(0)\n",
    "    \n",
    "    if 'compensation' in df_feat.columns:\n",
    "        # Convert bool/str to int\n",
    "        df_feat['compensation_flag'] = (df_feat['compensation'] == 'Recorded Free').astype(int)\n",
    "\n",
    "    # --- 6. Quartile/Distribution Feature ---\n",
    "    # This helps separate \"hardcore\" vs \"casual\" patterns\n",
    "    if 'hours_transformed' in df_feat.columns:\n",
    "        # Create quantiles for the target to help analysis, \n",
    "        # (Note: You can't use this column as a feature for prediction directly!)\n",
    "        # Instead, let's create a feature: \"Is this user usually in the top 10% of players?\"\n",
    "        \n",
    "        # Global 90th percentile of playtime\n",
    "        global_90_pct = df_feat['hours_transformed'].quantile(0.90)\n",
    "        global_10_pct = df_feat['hours_transformed'].quantile(0.10)\n",
    "        \n",
    "        # Feature: User's deviation from global mean\n",
    "        global_mean = df_feat['hours_transformed'].mean()\n",
    "        df_feat['user_deviation'] = df_feat['user_mean_hours'] - global_mean\n",
    "        \n",
    "    else:\n",
    "        df_feat['user_deviation'] = 0\n",
    "\n",
    "    # Clean up NaNs created by mapping (Cold start handling for features)\n",
    "    df_feat = df_feat.fillna(0)\n",
    "    \n",
    "    return df_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c06f4f",
   "metadata": {},
   "source": [
    "### split train data into train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db9f96a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_data(df, test_size=0.2, min_user_games=2, min_game_users=2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data ensuring all users and items appear in BOTH train and validation to mimic test data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame \n",
    "        pandas DataFramewith columns ['userID','gameID', 'hours_transformed']\n",
    "    test_size : float\n",
    "        proportion of data for validation\n",
    "    min_user_games : int\n",
    "        min number of games per user to keep in training\n",
    "    min_game_users : int\n",
    "        min number of users per game to keep in training\n",
    "    random_state : int\n",
    "        for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    train_df, val_df : DataFrames\n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Step 1: Filter users and games with enough interactions (Data Cleaning)\n",
    "    # We iteratively filter until convergence to ensure constraints are met\n",
    "    print(\"Filtering data for minimum interactions...\")\n",
    "    temp_df = df.copy()\n",
    "    \n",
    "    # Simple one-pass filter (iterative is better but this is usually sufficient for large sparse data)\n",
    "    user_counts = temp_df['userID'].value_counts()\n",
    "    valid_users = user_counts[user_counts >= min_user_games].index\n",
    "    temp_df = temp_df[temp_df['userID'].isin(valid_users)]\n",
    "    \n",
    "    game_counts = temp_df['gameID'].value_counts()\n",
    "    valid_games = game_counts[game_counts >= min_game_users].index\n",
    "    temp_df = temp_df[temp_df['gameID'].isin(valid_games)]\n",
    "    \n",
    "    print(f\"Data filtered. Original: {len(df)}, Filtered: {len(temp_df)}\")\n",
    "    \n",
    "    # Step 2: Group by user to ensure user split\n",
    "    # We will build a boolean mask for the training set\n",
    "    # True = Train, False = Validation\n",
    "    train_mask = np.ones(len(temp_df), dtype=bool)\n",
    "    \n",
    "    # Reset index to allow easy indexing\n",
    "    temp_df = temp_df.reset_index(drop=True)\n",
    "    \n",
    "    grouped = temp_df.groupby('userID')\n",
    "    \n",
    "    val_indices_list = []\n",
    "    \n",
    "    print(\"Splitting data by user...\")\n",
    "    for user, group in grouped:\n",
    "        n_samples = len(group)\n",
    "        # Calculate how many to validate\n",
    "        n_val = int(n_samples * test_size)\n",
    "        \n",
    "        # Constraint: Ensure min_user_games stay in training\n",
    "        if n_samples - n_val < min_user_games:\n",
    "            n_val = n_samples - min_user_games\n",
    "            \n",
    "        if n_val > 0:\n",
    "            # Randomly select indices for validation\n",
    "            val_idx = np.random.choice(group.index, n_val, replace=False)\n",
    "            val_indices_list.extend(val_idx)\n",
    "            \n",
    "    # Apply split mask\n",
    "    train_mask[val_indices_list] = False\n",
    "    \n",
    "    train_df = temp_df[train_mask].copy()\n",
    "    val_df = temp_df[~train_mask].copy()\n",
    "    \n",
    "    # Step 3 & 4: Check for Item Cold-Start in Validation\n",
    "    # (Items that ended up ONLY in validation)\n",
    "    train_items = set(train_df['gameID'].unique())\n",
    "    val_items = set(val_df['gameID'].unique())\n",
    "    \n",
    "    # Items present in validation but MISSING from training\n",
    "    cold_start_items = val_items - train_items\n",
    "    \n",
    "    if len(cold_start_items) > 0:\n",
    "        print(f\"Warning: {len(cold_start_items)} games appear in Validation but not Training. Moving them back to Train.\")\n",
    "        \n",
    "        # Identify rows in validation that contain these cold-start items\n",
    "        rows_to_move_mask = val_df['gameID'].isin(cold_start_items)\n",
    "        rows_to_move = val_df[rows_to_move_mask]\n",
    "        \n",
    "        # Move them to training\n",
    "        train_df = pd.concat([train_df, rows_to_move], ignore_index=True)\n",
    "        val_df = val_df[~rows_to_move_mask].copy()\n",
    "        \n",
    "    # Step 5: Final Verification\n",
    "    final_train_users = set(train_df['userID'].unique())\n",
    "    final_val_users = set(val_df['userID'].unique())\n",
    "    final_train_items = set(train_df['gameID'].unique())\n",
    "    final_val_items = set(val_df['gameID'].unique())\n",
    "    \n",
    "    # Assertions\n",
    "    if not final_val_users.issubset(final_train_users):\n",
    "        print(\"Warning: Validation has users not in Train (Should not happen with user-group split).\")\n",
    "        \n",
    "    if not final_val_items.issubset(final_train_items):\n",
    "        print(\"Warning: Validation has items not in Train (Logic error in Step 4).\")\n",
    "        \n",
    "    # Check for pair overlap\n",
    "    train_pairs = set(zip(train_df['userID'], train_df['gameID']))\n",
    "    val_pairs = set(zip(val_df['userID'], val_df['gameID']))\n",
    "    overlap = train_pairs.intersection(val_pairs)\n",
    "    \n",
    "    if len(overlap) > 0:\n",
    "        print(f\"Critical Error: {len(overlap)} overlapping pairs found between Train and Val.\")\n",
    "    else:\n",
    "        print(\"Success: No pair overlap between Train and Val.\")\n",
    "        \n",
    "    print(f\"Final Split -- Train: {len(train_df)}, Val: {len(val_df)}\")\n",
    "    \n",
    "    return train_df, val_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d771deb",
   "metadata": {},
   "source": [
    "### error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e98183c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_error_analysis(model, val_df, train_df, feature_columns=None, top_n_errors=50):\n",
    "    \"\"\"\n",
    "    Deep dive into model performance to identify weaknesses.\n",
    "    Suggests hyperparameters or feature engineering based on error patterns.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPREHENSIVE ERROR ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Generate Predictions\n",
    "    is_hybrid = isinstance(model, HybridMF)\n",
    "    \n",
    "    if is_hybrid:\n",
    "        if feature_columns is None:\n",
    "            print(\"Warning: feature_columns not provided for HybridMF. Using inferred numeric columns.\")\n",
    "            feature_columns = [c for c in val_df.columns if pd.api.types.is_numeric_dtype(val_df[c]) \n",
    "                               and c not in ['hours_transformed', 'prediction', 'user_idx', 'game_idx']]\n",
    "        preds = model.predict_batch(val_df, feature_columns)\n",
    "    else:\n",
    "        preds = np.array(model.predict_batch(val_df))\n",
    "        \n",
    "    y_true = val_df['hours_transformed'].values\n",
    "    residuals = y_true - preds\n",
    "    abs_residuals = np.abs(residuals)\n",
    "    \n",
    "    # 2. Overall Metrics (Validation)\n",
    "    mse = np.mean(residuals**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(abs_residuals)\n",
    "    \n",
    "    # Estimate Training Error for Overfitting Check (using sample for speed)\n",
    "    train_sample = train_df.sample(min(len(train_df), 10000), random_state=42)\n",
    "    if is_hybrid:\n",
    "        train_preds = model.predict_batch(train_sample, feature_columns)\n",
    "    else:\n",
    "        train_preds = np.array(model.predict_batch(train_sample))\n",
    "    train_rmse = np.sqrt(np.mean((train_sample['hours_transformed'].values - train_preds)**2))\n",
    "    \n",
    "    print(f\"\\n--- PERFORMANCE METRICS ---\")\n",
    "    print(f\"{'Metric':<10} | {'Validation':<10} | {'Train (Est)':<10}\")\n",
    "    print(f\"{'-'*36}\")\n",
    "    print(f\"{'RMSE':<10} | {rmse:<10.4f} | {train_rmse:<10.4f}\")\n",
    "    print(f\"{'MAE':<10} | {mae:<10.4f} | {'-':<10}\")\n",
    "    \n",
    "    # 3. Error Distribution Statistics\n",
    "    print(f\"\\n--- ERROR DISTRIBUTION ---\")\n",
    "    print(f\"Bias (Mean Residual): {np.mean(residuals):.4f} (Pos=Underpredict, Neg=Overpredict)\")\n",
    "    print(f\"Std Deviation:        {np.std(residuals):.4f}\")\n",
    "    print(f\"Quantiles [5, 25, 50, 75, 95]: {np.percentile(residuals, [5, 25, 50, 75, 95]).round(2)}\")\n",
    "    \n",
    "    # 4. Cold Start Analysis\n",
    "    train_users = set(train_df['userID'].unique())\n",
    "    train_games = set(train_df['gameID'].unique())\n",
    "    \n",
    "    val_analysis = val_df.copy()\n",
    "    val_analysis['prediction'] = preds\n",
    "    val_analysis['error'] = residuals\n",
    "    val_analysis['abs_error'] = abs_residuals\n",
    "    val_analysis['is_new_user'] = ~val_analysis['userID'].isin(train_users)\n",
    "    val_analysis['is_new_game'] = ~val_analysis['gameID'].isin(train_games)\n",
    "    \n",
    "    print(f\"\\n--- COLD START METRICS (MAE) ---\")\n",
    "    print(f\"Known Users: {val_analysis[~val_analysis['is_new_user']]['abs_error'].mean():.4f}\")\n",
    "    print(f\"New Users:   {val_analysis[val_analysis['is_new_user']]['abs_error'].mean():.4f} (Count: {val_analysis['is_new_user'].sum()})\")\n",
    "    \n",
    "    # 5. Prediction Range Analysis (Binning)\n",
    "    print(f\"\\n--- BIAS BY TARGET VALUE (Playtime) ---\")\n",
    "    bins = [0, 2, 4, 6, 8, 10, 20]\n",
    "    labels = ['0-2', '2-4', '4-6', '6-8', '8-10', '10+']\n",
    "    val_analysis['bin'] = pd.cut(val_analysis['hours_transformed'], bins=bins, labels=labels)\n",
    "    grouped = val_analysis.groupby('bin')['error'].agg(['mean', 'count'])\n",
    "    print(grouped)\n",
    "\n",
    "    # 6. Worst Errors & Component Analysis\n",
    "    print(f\"\\n--- TOP {top_n_errors} WORST PREDICTIONS ---\")\n",
    "    worst = val_analysis.sort_values('abs_error', ascending=False).head(top_n_errors)\n",
    "    \n",
    "    # Iterate to show components\n",
    "    print(f\"{'UserID':<12} {'GameID':<12} {'True':<6} {'Pred':<6} {'Error':<6} {'Breakdown'}\")\n",
    "    for _, row in worst.iterrows():\n",
    "        uid, gid = row['userID'], row['gameID']\n",
    "        \n",
    "        # Get components from base MF model\n",
    "        base_model = model.mf_model if is_hybrid else model\n",
    "        comps = base_model.get_components(uid, gid)\n",
    "        \n",
    "        # Simple string representation of breakdown\n",
    "        if \"error\" in comps:\n",
    "            breakdown = \"Cold Start\"\n",
    "        else:\n",
    "            breakdown = f\"GAvg:{comps['Global Mean']:.1f} UB:{comps['User Bias']:.1f} GB:{comps['Game Bias']:.1f} Int:{comps['Latent Interaction']:.1f}\"\n",
    "            \n",
    "        print(f\"{uid:<12} {gid:<12} {row['hours_transformed']:<6.2f} {row['prediction']:<6.2f} {row['error']:<6.2f} {breakdown}\")\n",
    "\n",
    "    # 7. Actionable Suggestions (Heuristics)\n",
    "    print(f\"\\n\" + \"=\"*30)\n",
    "    print(\"ACTIONABLE SUGGESTIONS\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # A. Regularization Check\n",
    "    if train_rmse < rmse * 0.85:\n",
    "        print(\"[!] OVERFITTING DETECTED\")\n",
    "        print(\"    -> Increase regularization terms (reg_bu, reg_bi, reg_pu, reg_qi).\")\n",
    "        print(\"    -> Decrease n_factors or n_epochs.\")\n",
    "        if is_hybrid:\n",
    "            print(\"    -> Reduce GradientBoosting max_depth or n_estimators.\")\n",
    "    elif train_rmse > rmse:\n",
    "         print(\"[!] UNDERFITTING DETECTED (or Val set is easier)\")\n",
    "         print(\"    -> Increase n_factors.\")\n",
    "         print(\"    -> Increase n_epochs.\")\n",
    "    \n",
    "    # B. Bias Check\n",
    "    mean_res = np.mean(residuals)\n",
    "    if abs(mean_res) > 0.5:\n",
    "        direction = \"Underpredicting\" if mean_res > 0 else \"Overpredicting\"\n",
    "        print(f\"[!] SYSTEMATIC BIAS: {direction}\")\n",
    "        print(\"    -> Check global mean calculation.\")\n",
    "        print(\"    -> Hybrid: Add a feature for 'global_trend' or adjust residual model intercept.\")\n",
    "\n",
    "    # C. High Value Underprediction\n",
    "    high_bin_bias = grouped.loc['10+', 'mean'] if '10+' in grouped.index else 0\n",
    "    if high_bin_bias > 1.0:\n",
    "        print(\"[!] DIFFICULTY PREDICTING HARDCORE GAMERS (High hours)\")\n",
    "        print(\"    -> Feature Idea: Add 'is_hardcore_user' (top 10% playtime) as feature.\")\n",
    "        print(\"    -> Feature Idea: Add 'game_completion_time' (if external data available).\")\n",
    "\n",
    "    print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e41b2",
   "metadata": {},
   "source": [
    "### everybody all togther now.. full work flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd51c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Add powerful features FIRST\n",
    "# (Do this before splitting so features like 'user_game_count' exist in both sets)\n",
    "df_train_data_with_features = create_new_features(df_train_data)\n",
    "\n",
    "# 2. Split into Train and Validation\n",
    "# This ensures no cold-start items end up in validation\n",
    "df_train, df_val = split_train_data(\n",
    "    df_train_data_with_features, \n",
    "    test_size=0.2, \n",
    "    min_user_games=2, \n",
    "    min_game_users=2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Define numeric features for the Hybrid model\n",
    "# We exclude ID columns and the target variable\n",
    "feature_cols = [\n",
    "    'review_length', 'user_game_count', 'game_user_count', \n",
    "    'user_mean_hours', 'game_mean_hours', 'day_of_week', \n",
    "    'is_weekend', 'user_deviation'\n",
    "]\n",
    "\n",
    "# 4. Initialize and Train BiasedMF (Base Model)\n",
    "svd_reg_model = BiasedMF(\n",
    "    n_factors=10, \n",
    "    learning_rate=0.01, \n",
    "    reg_bu=0.02, reg_bi=0.2, reg_pu=0.01, reg_qi=0.1, \n",
    "    n_epochs=20\n",
    ")\n",
    "svd_reg_model.fit(df_train)\n",
    "\n",
    "# 5. Error Analysis for Base Model\n",
    "# Note: I changed val_df to df_val here. Using df_train (as in your snippet) \n",
    "# gives training error; df_val gives the actual generalization error.\n",
    "print(\"--- BASE MODEL ANALYSIS ---\")\n",
    "comprehensive_error_analysis(svd_reg_model, val_df=df_val, train_df=df_train, top_n_errors=50)\n",
    "\n",
    "# 6. Initialize and Train HybridMF\n",
    "hybrid_model = HybridMF(svd_reg_model, residual_model_type='gbm')\n",
    "\n",
    "# !CORRECTION HERE!: You must pass feature_columns\n",
    "hybrid_model.fit(df_train, feature_columns=feature_cols)\n",
    "\n",
    "# 7. Error Analysis for Hybrid Model\n",
    "print(\"--- HYBRID MODEL ANALYSIS ---\")\n",
    "comprehensive_error_analysis(hybrid_model, val_df=df_val, train_df=df_train, feature_columns=feature_cols, top_n_errors=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42483b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "class HybridMF:\n",
    "    \"\"\"\n",
    "    Two-stage hybrid:\n",
    "    1. BiasedMF learns from user/game IDs\n",
    "    2. Residual model learns from features\n",
    "    \"\"\"\n",
    "    def __init__(self,mf_model, residual_model='gbm'):\n",
    "        self.mf_model = mf_model\n",
    "        self.residual_model_type = residual_model\n",
    "\n",
    "    def fit(self, train_df, feature_columns):\n",
    "        \"\"\"\n",
    "        Stage 1: train MF model\n",
    "        Stage 2: train residual model on MF errors using features\n",
    "        \"\"\"\n",
    "        # get MF predictions\n",
    "\n",
    "        # calculate residuals\n",
    "\n",
    "        # train residual model\n",
    "\n",
    "        #feature imporance for GBM\n",
    "\n",
    "    def predict_batch(self,df):\n",
    "        \"\"\"\n",
    "        Hybrid prediction: MF + residual corrcection\n",
    "        \"\"\"\n",
    "        # MF prediction\n",
    "\n",
    "        # residual prediction\n",
    "\n",
    "        # combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f27e2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hours</th>\n",
       "      <th>hours_transformed</th>\n",
       "      <th>found_funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>175000.000000</td>\n",
       "      <td>175000.000000</td>\n",
       "      <td>175000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>66.408189</td>\n",
       "      <td>3.717845</td>\n",
       "      <td>1.356497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>275.203113</td>\n",
       "      <td>2.297882</td>\n",
       "      <td>24.119107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.100000</td>\n",
       "      <td>3.472488</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>33.500000</td>\n",
       "      <td>5.108524</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16539.900000</td>\n",
       "      <td>14.013750</td>\n",
       "      <td>4013.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               hours  hours_transformed    found_funny\n",
       "count  175000.000000      175000.000000  175000.000000\n",
       "mean       66.408189           3.717845       1.356497\n",
       "std       275.203113           2.297882      24.119107\n",
       "min         0.000000           0.000000       0.000000\n",
       "25%         3.000000           2.000000       0.000000\n",
       "50%        10.100000           3.472488       0.000000\n",
       "75%        33.500000           5.108524       0.000000\n",
       "max     16539.900000          14.013750    4013.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_new_features(df):\n",
    "    \"\"\"\n",
    "    create powerful features for predicting 'hours_transformed' for (userID,gameID) pairs\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : Dataframe\n",
    "        orginal pandas DataFrame for training data with following columns:\n",
    "        'userID' : str\n",
    "        'gameID' : str \n",
    "        'hours_transformed' : float \n",
    "        'early_access' : bool {0,1} \n",
    "        'date' : str (format=\"%Y-%m-%d\"), \n",
    "        'text' : str\n",
    "        'found_funny' : int\n",
    "        'compensation' : bool {0,1}\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df_with_features : Dataframe\n",
    "        orginal pandas DataFrame with additional powerful features\n",
    "    \"\"\"\n",
    "    # create new user features\n",
    "    # get game count for each user in user_games dict\n",
    "    user_games_count = pd.Series({u:len(g) for u,g in user_games.items()})\n",
    "    df['user_games_count_log'] = np.log1p(df['userID'].map(user_games_count).fillna(0))\n",
    "    # get avg hours played for each user\n",
    "    user_avg_hours = df.groupby('userID')['hours_transformed'].mean()\n",
    "    df['user_avg_hours'] = df['userID'].map(user_avg_hours)\n",
    "    user_avg_text_len = df.groupby('userID')['text'].apply(lambda x: x.str.len().mean())\n",
    "    df['user_avg_text_len'] = df['userID'].map(user_avg_text_len).fillna(0)\n",
    "\n",
    "    ## create new game features\n",
    "# get user count for each game in game_users dict\n",
    "game_users_count = pd.Series({g:len(u) for g,u in game_users.items()})\n",
    "\n",
    "df_train_data.head()\n",
    "df_train_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0afd48d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering data for minimum interactions...\n",
      "Data filtered. Original: 175000, Filtered: 174993\n",
      "Splitting data by user...\n",
      "Success: No pair overlap between Train and Val.\n",
      "Final Split -- Train: 142675, Val: 32318\n"
     ]
    }
   ],
   "source": [
    "def split_train_data(df, test_size=0.2, min_user_games=2, min_game_users=2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data ensuring all users and items appear in BOTH train and validation to mimic test data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame \n",
    "        pandas DataFramewith columns ['userID','gameID', 'hours_transformed']\n",
    "    test_size : float\n",
    "        proportion of data for validation\n",
    "    min_user_games : int\n",
    "        min number of games per user to keep in training\n",
    "    min_game_users : int\n",
    "        min number of users per game to keep in training\n",
    "    random_state : int\n",
    "        for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    train_df, val_df : DataFrames\n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Step 1: Filter users and games with enough interactions (Data Cleaning)\n",
    "    # We iteratively filter until convergence to ensure constraints are met\n",
    "    print(\"Filtering data for minimum interactions...\")\n",
    "    temp_df = df.copy()\n",
    "    \n",
    "    # Simple one-pass filter (iterative is better but this is usually sufficient for large sparse data)\n",
    "    user_counts = temp_df['userID'].value_counts()\n",
    "    valid_users = user_counts[user_counts >= min_user_games].index\n",
    "    temp_df = temp_df[temp_df['userID'].isin(valid_users)]\n",
    "    \n",
    "    game_counts = temp_df['gameID'].value_counts()\n",
    "    valid_games = game_counts[game_counts >= min_game_users].index\n",
    "    temp_df = temp_df[temp_df['gameID'].isin(valid_games)]\n",
    "    \n",
    "    print(f\"Data filtered. Original: {len(df)}, Filtered: {len(temp_df)}\")\n",
    "    \n",
    "    # Step 2: Group by user to ensure user split\n",
    "    # We will build a boolean mask for the training set\n",
    "    # True = Train, False = Validation\n",
    "    train_mask = np.ones(len(temp_df), dtype=bool)\n",
    "    \n",
    "    # Reset index to allow easy indexing\n",
    "    temp_df = temp_df.reset_index(drop=True)\n",
    "    \n",
    "    grouped = temp_df.groupby('userID')\n",
    "    \n",
    "    val_indices_list = []\n",
    "    \n",
    "    print(\"Splitting data by user...\")\n",
    "    for user, group in grouped:\n",
    "        n_samples = len(group)\n",
    "        # Calculate how many to validate\n",
    "        n_val = int(n_samples * test_size)\n",
    "        \n",
    "        # Constraint: Ensure min_user_games stay in training\n",
    "        if n_samples - n_val < min_user_games:\n",
    "            n_val = n_samples - min_user_games\n",
    "            \n",
    "        if n_val > 0:\n",
    "            # Randomly select indices for validation\n",
    "            val_idx = np.random.choice(group.index, n_val, replace=False)\n",
    "            val_indices_list.extend(val_idx)\n",
    "            \n",
    "    # Apply split mask\n",
    "    train_mask[val_indices_list] = False\n",
    "    \n",
    "    train_df = temp_df[train_mask].copy()\n",
    "    val_df = temp_df[~train_mask].copy()\n",
    "    \n",
    "    # Step 3 & 4: Check for Item Cold-Start in Validation\n",
    "    # (Items that ended up ONLY in validation)\n",
    "    train_items = set(train_df['gameID'].unique())\n",
    "    val_items = set(val_df['gameID'].unique())\n",
    "    \n",
    "    # Items present in validation but MISSING from training\n",
    "    cold_start_items = val_items - train_items\n",
    "    \n",
    "    if len(cold_start_items) > 0:\n",
    "        print(f\"Warning: {len(cold_start_items)} games appear in Validation but not Training. Moving them back to Train.\")\n",
    "        \n",
    "        # Identify rows in validation that contain these cold-start items\n",
    "        rows_to_move_mask = val_df['gameID'].isin(cold_start_items)\n",
    "        rows_to_move = val_df[rows_to_move_mask]\n",
    "        \n",
    "        # Move them to training\n",
    "        train_df = pd.concat([train_df, rows_to_move], ignore_index=True)\n",
    "        val_df = val_df[~rows_to_move_mask].copy()\n",
    "        \n",
    "    # Step 5: Final Verification\n",
    "    final_train_users = set(train_df['userID'].unique())\n",
    "    final_val_users = set(val_df['userID'].unique())\n",
    "    final_train_items = set(train_df['gameID'].unique())\n",
    "    final_val_items = set(val_df['gameID'].unique())\n",
    "    \n",
    "    # Assertions\n",
    "    if not final_val_users.issubset(final_train_users):\n",
    "        print(\"Warning: Validation has users not in Train (Should not happen with user-group split).\")\n",
    "        \n",
    "    if not final_val_items.issubset(final_train_items):\n",
    "        print(\"Warning: Validation has items not in Train (Logic error in Step 4).\")\n",
    "        \n",
    "    # Check for pair overlap\n",
    "    train_pairs = set(zip(train_df['userID'], train_df['gameID']))\n",
    "    val_pairs = set(zip(val_df['userID'], val_df['gameID']))\n",
    "    overlap = train_pairs.intersection(val_pairs)\n",
    "    \n",
    "    if len(overlap) > 0:\n",
    "        print(f\"Critical Error: {len(overlap)} overlapping pairs found between Train and Val.\")\n",
    "    else:\n",
    "        print(\"Success: No pair overlap between Train and Val.\")\n",
    "        \n",
    "    print(f\"Final Split -- Train: {len(train_df)}, Val: {len(val_df)}\")\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "train_df,val_df = split_train_data(df_train_data, test_size=0.2, min_user_games=2, min_game_users=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a8b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class BiasedMF:\n",
    "    def __init__(self, n_factors=10, learning_rate=0.01, \n",
    "                 reg_bu=0.0, reg_bi=0.0, reg_pu=0.0, reg_qi=0.0, n_epochs=10):\n",
    "        \"\"\"\n",
    "        Biased Matrix Factorization (Regularized SVD) optimized via SGD.\n",
    "        \n",
    "        Model: r_hat = mu + b_u + b_i + dot(p_u, q_i)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_factors : int\n",
    "            Number of latent factors (dimension of vectors p_u and q_i)\n",
    "        learning_rate : float\n",
    "            Step size for Gradient Descent (eta)\n",
    "        reg_bu, reg_bi, reg_pu, reg_qi : float\n",
    "            Regularization parameters (lambda) to prevent overfitting\n",
    "        n_epochs : int\n",
    "            Number of passes over the training data\n",
    "        \"\"\"\n",
    "        self.n_factors = n_factors\n",
    "        self.lr = learning_rate\n",
    "        self.reg_bu = reg_bu\n",
    "        self.reg_bi = reg_bi\n",
    "        self.reg_pu = reg_pu\n",
    "        self.reg_qi = reg_qi\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "        # Internal state\n",
    "        self.mu = 0.0\n",
    "        self.user_bias = None  # numpy array\n",
    "        self.item_bias = None  # numpy array\n",
    "        self.P = None          # User factors (n_users x n_factors)\n",
    "        self.Q = None          # Item factors (n_items x n_factors)\n",
    "        \n",
    "        # Mappings\n",
    "        self.user2idx = {}\n",
    "        self.idx2user = {}\n",
    "        self.game2idx = {}\n",
    "        self.idx2game = {}\n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Trains the model using Stochastic Gradient Descent.\n",
    "        df columns: ['userID', 'gameID', 'hours_transformed']\n",
    "        \"\"\"\n",
    "        print(\"Initializing BiasedMF...\")\n",
    "        \n",
    "        # 1. Create Mappings\n",
    "        unique_users = df['userID'].unique()\n",
    "        unique_games = df['gameID'].unique()\n",
    "        \n",
    "        self.n_users = len(unique_users)\n",
    "        self.n_games = len(unique_games)\n",
    "        \n",
    "        self.user2idx = {u: i for i, u in enumerate(unique_users)}\n",
    "        self.game2idx = {g: i for i, g in enumerate(unique_games)}\n",
    "        \n",
    "        # 2. Initialize Parameters\n",
    "        self.mu = df['hours_transformed'].mean()\n",
    "        \n",
    "        # Biases initialized to zero\n",
    "        self.user_bias = np.zeros(self.n_users)\n",
    "        self.item_bias = np.zeros(self.n_games)\n",
    "        \n",
    "        # Latent Factors initialized with small random noise (Normal Dist)\n",
    "        # Scaling by 0.1 helps convergence\n",
    "        self.P = np.random.normal(0, 0.1, (self.n_users, self.n_factors))\n",
    "        self.Q = np.random.normal(0, 0.1, (self.n_games, self.n_factors))\n",
    "        \n",
    "        # Convert dataframe to numpy arrays for faster iteration\n",
    "        # We map string IDs to integers here for the training loop\n",
    "        users_idx = df['userID'].map(self.user2idx).values\n",
    "        games_idx = df['gameID'].map(self.game2idx).values\n",
    "        ratings = df['hours_transformed'].values\n",
    "        \n",
    "        n_samples = len(df)\n",
    "        \n",
    "        # 3. Training Loop (SGD)\n",
    "        print(f\"Starting training on {n_samples} samples for {self.n_epochs} epochs.\")\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            # Calculate MSE at start of epoch (optional, but good for monitoring)\n",
    "            # Keeping it simple here to focus on speed\n",
    "            \n",
    "            # Shuffle indices for true SGD behavior\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            \n",
    "            total_error = 0\n",
    "            \n",
    "            for i in indices:\n",
    "                u = users_idx[i]\n",
    "                g = games_idx[i]\n",
    "                r = ratings[i]\n",
    "                \n",
    "                # --- Prediction Step ---\n",
    "                # r_hat = mu + b_u + b_i + P_u . Q_i\n",
    "                dot_prod = np.dot(self.P[u], self.Q[g])\n",
    "                pred = self.mu + self.user_bias[u] + self.item_bias[g] + dot_prod\n",
    "                \n",
    "                # --- Error Calculation ---\n",
    "                err = r - pred\n",
    "                total_error += err**2\n",
    "                \n",
    "                # --- Update Rules (Gradient Descent) ---\n",
    "                # Update Biases\n",
    "                # b_u <- b_u + lr * (err - reg * b_u)\n",
    "                self.user_bias[u] += self.lr * (err - self.reg_bu * self.user_bias[u])\n",
    "                self.item_bias[g] += self.lr * (err - self.reg_bi * self.item_bias[g])\n",
    "                \n",
    "                # Update Latent Factors\n",
    "                # Note: We need to copy P[u] before updating it to update Q[g] correctly \n",
    "                # (though simultaneous update approximation is standard in code)\n",
    "                p_u_current = self.P[u].copy()\n",
    "                \n",
    "                # P_u <- P_u + lr * (err * Q_i - reg * P_u)\n",
    "                self.P[u] += self.lr * (err * self.Q[g] - self.reg_pu * self.P[u])\n",
    "                \n",
    "                # Q_i <- Q_i + lr * (err * P_u - reg * Q_i)\n",
    "                self.Q[g] += self.lr * (err * p_u_current - self.reg_qi * self.Q[g])\n",
    "            \n",
    "            mse = total_error / n_samples\n",
    "            print(f\"Epoch {epoch+1}/{self.n_epochs} - MSE: {mse:.4f}\")\n",
    "\n",
    "    def predict(self, user_id, game_id):\n",
    "        \"\"\"\n",
    "        Predict rating for user item pair.\n",
    "        Handles Cold Start by falling back to global mean.\n",
    "        \"\"\"\n",
    "        # Cold Start Check\n",
    "        if user_id not in self.user2idx or game_id not in self.game2idx:\n",
    "            # If we haven't seen the user or game, return global mean\n",
    "            # (A more advanced approach would use just user_bias or item_bias if available)\n",
    "            return self.mu\n",
    "        \n",
    "        u = self.user2idx[user_id]\n",
    "        g = self.game2idx[game_id]\n",
    "        \n",
    "        pred = self.mu + self.user_bias[u] + self.item_bias[g] + np.dot(self.P[u], self.Q[g])\n",
    "        return pred\n",
    "\n",
    "    def get_components(self, user_id, game_id):\n",
    "        \"\"\"\n",
    "        Show breakdown of prediction components for analysis.\n",
    "        \"\"\"\n",
    "        if user_id not in self.user2idx or game_id not in self.game2idx:\n",
    "            return {\"error\": \"User or Game not found in training data\"}\n",
    "            \n",
    "        u = self.user2idx[user_id]\n",
    "        g = self.game2idx[game_id]\n",
    "        \n",
    "        components = {\n",
    "            \"Global Mean\": self.mu,\n",
    "            \"User Bias\": self.user_bias[u],\n",
    "            \"Game Bias\": self.item_bias[g],\n",
    "            \"Latent Interaction\": np.dot(self.P[u], self.Q[g]),\n",
    "            \"Total Prediction\": self.predict(user_id, game_id)\n",
    "        }\n",
    "        return components\n",
    "\n",
    "    def predict_batch(self, df):\n",
    "        \"\"\"\n",
    "        Batch prediction for efficiency.\n",
    "        df: dataframe containing 'userID' and 'gameID' columns\n",
    "        Returns: list of predictions\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        # Iterating is acceptable here as we need to handle string IDs and potential cold starts\n",
    "        # Vectorization is difficult with mixed cold-start scenarios without complex masking\n",
    "        for _, row in df.iterrows():\n",
    "            pred = self.predict(row['userID'], row['gameID'])\n",
    "            predictions.append(pred)\n",
    "        return predictions\n",
    "\n",
    "# --- Example Usage (Commented out) ---\n",
    "# model = BiasedMF(n_factors=5, learning_rate=0.005, reg_bu=0.02, reg_bi=0.02, n_epochs=10)\n",
    "# model.fit(train_df)\n",
    "# preds = model.predict_batch(test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
