{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cc156ce",
   "metadata": {},
   "source": [
    "## BB example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf541703",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c4faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "\n",
    "def readJSON(path):\n",
    "  for l in gzip.open(path, 'rt'):\n",
    "    d = eval(l)\n",
    "    u = d['userID']\n",
    "    try:\n",
    "      g = d['gameID']\n",
    "    except Exception as e:\n",
    "      g = None\n",
    "    yield u,g,d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f861d0",
   "metadata": {},
   "source": [
    "### define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45fd4edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==================================================================================\n",
    "TRAINING DATA (train.json.gz)\n",
    "==================================================================================\n",
    "DESCRIPTION: 175,000 instances to be used for training. This data should be used \n",
    "for both the play prediction and time played prediction tasks. It is not necessary \n",
    "to use all observations for training, for example if doing so proves too \n",
    "computationally intensive.\n",
    "    userID The ID of the user. \n",
    "    gameID The ID of the game. \n",
    "    text Text of the user's review of the game.\n",
    "    date Date when the review was entered.\n",
    "    hours How many hours the user played the game.\n",
    "    hours transformed log_{2}(hours+1). \n",
    "        !Note!: log_{2}(hours+1) = hours_transformed\n",
    "        * This transformed value is the one we are trying to predict\n",
    "__________________________________________________________________________________\n",
    "==================================================================================\n",
    "GOAL 1: predict hours_transformed given userID and gameID\n",
    "==================================================================================\n",
    "DESCRIPTION:Predict how long a person will play a game (transformed as log2\n",
    "(hours + 1), for those (user,game) pairs in pairs Hours.csv. Accuracy will be measured in terms of the mean-squared\n",
    "error (MSE).\n",
    "    !Note!: log_{2}(hours+1) = hours_transformed)\n",
    "subset (pairs_Hours.csv):\n",
    "    userID,gameID,prediction\n",
    "    u04763917,g51093074\n",
    "    u10668484,g42523222\n",
    "    u82502949,g39422502\n",
    "    u14336188,g83517324\n",
    "    u10096161,g10962300\n",
    "    u12864301,g29951417\n",
    "    u74352605,g88258978\n",
    "    u75043948,g25417282\n",
    "    u29296791,g59132435\n",
    "__________________________________________________________________________________\n",
    "==================================================================================    \n",
    "GOAL 2: predict 'played' given userID and gameID\n",
    "==================================================================================\n",
    "\n",
    "DESCRIPTION: Predict given a (user,game) pair from pairs Played.csv whether the \n",
    "user would play the game (0 or 1). Accuracy will be measured in terms of the \n",
    "categorization accuracy (fraction of correct predictions). \n",
    "    !Note!:The test set has been constructed such that exactly 50% of the pairs \n",
    "    correspond to played games and the other 50% do not.\n",
    "subset (pairs_Played.csv):\n",
    "    userID,gameID,prediction\n",
    "    u04836696,g41031307\n",
    "    u32377855,g62450068\n",
    "    u58289072,g71021765\n",
    "    u74685029,g26732871\n",
    "    u06266052,g69433247\n",
    "    u45011836,g57175884\n",
    "    u98484614,g01435414\n",
    "    u34253509,g66197269\n",
    "    u76252009,g10053132\n",
    "__________________________________________________________________________________\n",
    "\"\"\"\n",
    "# define data paths\n",
    "train_json = '/home/scotty/dsc_256/fall_25/make_up/assignment1/train.json.gz'\n",
    "pairs_hours = '/home/scotty/dsc_256/fall_25/make_up/assignment1/pairs_Hours.csv'\n",
    "pairs_played = '/home/scotty/dsc_256/fall_25/make_up/assignment1/pairs_Played.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bd028b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create containers for users and games\n",
    "user_dict = defaultdict(list)\n",
    "game_dict = defaultdict(list)\n",
    "train_data = []\n",
    "\n",
    "# read train.json.gz and populate user_dict and game_dict\n",
    "for u,g,d in readJSON(train_json):\n",
    "    user_dict[u].append(g)\n",
    "    game_dict[g].append(u)\n",
    "    train_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dd7067e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['g05339526', 'g26654626', 'g56751692', 'g88534444', 'g35155801', 'g68132896', 'g70072143', 'g09480895', 'g02141122', 'g46446145', 'g29741733', 'g84367501', 'g04462740', 'g95765068', 'g83425645', 'g47086763', 'g97784906', 'g36222633', 'g34147651', 'g27545941', 'g13121437', 'g30645652', 'g42862258', 'g54979645', 'g34397868', 'g34842774', 'g64509433', 'g14120436', 'g40372626', 'g79333823', 'g68302537', 'g71073909', 'g66258359', 'g12460730', 'g83595955', 'g46430686', 'g43436077', 'g48254339', 'g84227207', 'g01269364', 'g07027969', 'g66296428', 'g53666531', 'g67071738', 'g16087475', 'g15881340', 'g88681493', 'g10773791', 'g03162218', 'g32641915', 'g90153689', 'g87795702', 'g51259924', 'g77859458', 'g24741744', 'g41141819', 'g33400185', 'g16097342', 'g62438733']\n",
      "{'userID': 'u70666506', 'early_access': False, 'hours': 63.5, 'hours_transformed': 6.011227255423254, 'found_funny': 1, 'text': 'If you want to sit in queue for 10-20min and have 140 ping then this game is perfect for you :)', 'gameID': 'g49368897', 'user_id': '76561198030408772', 'date': '2017-05-20'}\n"
     ]
    }
   ],
   "source": [
    "print(user_dict['u04836696'])\n",
    "print(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dfab39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175000 entries, 0 to 174999\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   hours              175000 non-null  float64\n",
      " 1   gameID             175000 non-null  object \n",
      " 2   hours_transformed  175000 non-null  float64\n",
      " 3   early_access       175000 non-null  bool   \n",
      " 4   date               175000 non-null  object \n",
      " 5   text               175000 non-null  object \n",
      " 6   userID             175000 non-null  object \n",
      " 7   found_funny        29938 non-null   float64\n",
      " 8   user_id            54841 non-null   object \n",
      " 9   compensation       2870 non-null    object \n",
      " 10  text_len_norm      175000 non-null  float64\n",
      "dtypes: bool(1), float64(4), object(6)\n",
      "memory usage: 13.5+ MB\n",
      "None\n",
      "               hours  hours_transformed   found_funny  text_len_norm\n",
      "count  175000.000000      175000.000000  29938.000000  175000.000000\n",
      "mean       66.408189           3.717845      7.929287       0.048870\n",
      "std       275.203113           2.297882     57.865664       0.094451\n",
      "min         0.000000           0.000000      1.000000       0.000000\n",
      "25%         3.000000           2.000000      1.000000       0.005250\n",
      "50%        10.100000           3.472488      1.000000       0.016250\n",
      "75%        33.500000           5.108524      3.000000       0.049375\n",
      "max     16539.900000          14.013750   4013.000000       1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "def softplus(x, beta=1.0):\n",
    "    return (1/beta) * np.log(1 + np.exp(beta * x))\n",
    "df = pd.DataFrame(train_data)\n",
    "df['text_len_norm'] = df['text'].str.len()/df['text'].str.len().max()\n",
    "df['user_idx'], user_ids = pd.factorize(df['userID'])\n",
    "df['item_idx'], item_ids = pd.factorize(df['gameID'])\n",
    "\n",
    "num_users = len(user_ids)\n",
    "num_items = len(item_ids)\n",
    "# Calculate thresholds once (more efficient than inside the loop)\n",
    "hours_threshold = df['hours_transformed'].mean()\n",
    "text_threshold = df['text_len_norm'].mean()\n",
    "\n",
    "def create_monotonic_chain(row):\n",
    "    \"\"\"\n",
    "    Creates a ground truth vector y that enforces y_1 >= y_2 >= y_3\n",
    "    \"\"\"\n",
    "    # Stage 1: Played (Always 1 for this dataset)\n",
    "    y1 = 1\n",
    "    \n",
    "    # Stage 2: Engaged (High Hours)\n",
    "    y2 = 1 if row['hours_transformed'] > hours_threshold else 0\n",
    "    \n",
    "    # Stage 3: Reviewed (Long Text)\n",
    "    y3 = 1 if row['text_len_norm'] > text_threshold else 0\n",
    "    \n",
    "    # --- CRITICAL FIX: Enforce Monotonicity ---\n",
    "    # If they reviewed (y3=1), they MUST be considered engaged (y2=1)\n",
    "    # Logic: \"A review action implies... a 'purchase' action\" [cite: 9]\n",
    "    if y3 == 1:\n",
    "        y2 = 1\n",
    "        \n",
    "    return [y1, y2, y3]\n",
    "\n",
    "# Apply the function\n",
    "# Rename 'intention' to 'chain_labels' or 'y_true' to avoid confusion.\n",
    "# \"Intention\" usually refers to the model's *output* score (delta), not the input data.\n",
    "df['chain_labels'] = df.apply(create_monotonic_chain, axis=1)\n",
    "\n",
    "# Quick check: The sum of the chain should tell you the \"Edge\" (l*)\n",
    "# [1, 0, 0] -> Sum 1 -> Edge 1\n",
    "# [1, 1, 0] -> Sum 2 -> Edge 2\n",
    "# [1, 1, 1] -> Sum 3 -> Edge 3\n",
    "df['edge_index'] = df['chain_labels'].apply(sum)\n",
    "\n",
    "print(df[['hours_transformed', 'text_len_norm', 'chain_labels', 'edge_index']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8a1d08",
   "metadata": {},
   "source": [
    "### all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb1bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Prepared. Users: 6710, Items: 2437\n",
      "      userID  edge_index\n",
      "0  u55351001           1\n",
      "1  u70666506           2\n",
      "2  u18612571           1\n",
      "3  u34283088           1\n",
      "4  u16220374           1\n",
      "\n",
      "Starting Training...\n",
      "Epoch 0: Loss 569881.8742\n",
      "Epoch 2: Loss 589084.5074\n",
      "Epoch 4: Loss 586710.0344\n",
      "Epoch 6: Loss 589763.4476\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 186\u001b[39m\n\u001b[32m    184\u001b[39m     loss = edge_loss(model, b_u, b_i, b_edge, b_neg_i)\n\u001b[32m    185\u001b[39m     loss.backward()\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m     total_loss += loss.item()\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m2\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ucsd/lib/python3.12/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ucsd/lib/python3.12/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ucsd/lib/python3.12/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ucsd/lib/python3.12/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ucsd/lib/python3.12/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ucsd/lib/python3.12/site-packages/torch/optim/adam.py:456\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    454\u001b[39m         exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    459\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "############################################################\n",
    "###########        READ/PREP TRAINING DATA       ###########\n",
    "############################################################\n",
    "# read data\n",
    "df = pd.DataFrame(train_data)\n",
    "\n",
    "# define features: normailized text length, user_idx(userID -> int), item_idx(gameID -> int)\n",
    "df['text_len_norm'] = df['text'].str.len()/df['text'].str.len().max()\n",
    "df['user_idx'], user_ids = pd.factorize(df['userID'])\n",
    "df['item_idx'], item_ids = pd.factorize(df['gameID'])\n",
    "\n",
    "# get user/game counts\n",
    "num_users = len(user_ids)\n",
    "num_items = len(item_ids)\n",
    "# calc threshhold for transfrmed hours and normalized text length\n",
    "hours_threshold = df['hours_transformed'].mean()\n",
    "text_threshold = df['text_len_norm'].mean()\n",
    "\n",
    "# define create_monotonic_chain function\n",
    "def create_monotonic_chain(row):\n",
    "    \"\"\"\n",
    "    Creates a ground truth vector y that enforces y_1 >= y_2 >= y_3\n",
    "    \"\"\"\n",
    "    y1 = 1 # Played\n",
    "    y2 = 1 if row['hours_transformed'] > hours_threshold else 0 # Engaged\n",
    "    y3 = 1 if row['text_len_norm'] > text_threshold else 0      # Reviewed\n",
    "    \n",
    "    # Enforce Monotonicity: Review -> Engaged\n",
    "    if y3 == 1:\n",
    "        y2 = 1\n",
    "        \n",
    "    return [y1, y2, y3]\n",
    "\n",
    "# define features: chain_labels and edge_index\n",
    "df['chain_labels'] = df.apply(create_monotonic_chain, axis=1)\n",
    "df['edge_index'] = df['chain_labels'].apply(sum) # 1, 2, or 3\n",
    "\n",
    "print(f\"Data Prepared. Users: {num_users}, Items: {num_items}\")\n",
    "print(df[['userID', 'edge_index']].head())\n",
    "############################################################\n",
    "\n",
    "\n",
    "############################################################\n",
    "#######    DEFINE CHAINREC MODEL AND EDGE LOSS     #########\n",
    "############################################################\n",
    "class ChainRec(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_stages=3, embed_dim=16, beta=1.0):\n",
    "        super().__init__()\n",
    "        self.num_stages = num_stages\n",
    "        self.beta = beta\n",
    "        \n",
    "        # 1. Embeddings (User & Item)\n",
    "        self.user_emb = nn.Embedding(num_users, embed_dim)\n",
    "        self.item_emb = nn.Embedding(num_items, embed_dim)\n",
    "        \n",
    "        # 2. Biases\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # 3. Stage Embeddings (The \"Lenses\")\n",
    "        # Shape: (num_stages, embed_dim)\n",
    "        self.stage_emb = nn.Embedding(num_stages, embed_dim)\n",
    "        \n",
    "        # Initialize weights (Optional but good practice)\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.stage_emb.weight)\n",
    "\n",
    "    def forward(self, u_idx, i_idx):\n",
    "        \"\"\"\n",
    "        Returns: A tensor of shape (batch_size, num_stages) \n",
    "                 containing scores [s_1, s_2, s_3] for each pair.\n",
    "        \"\"\"\n",
    "        # Look up embeddings\n",
    "        u = self.user_emb(u_idx)      # (batch, dim)\n",
    "        i = self.item_emb(i_idx)      # (batch, dim)\n",
    "        \n",
    "        # Biases\n",
    "        b_u = self.user_bias(u_idx).squeeze()\n",
    "        b_i = self.item_bias(i_idx).squeeze()\n",
    "        base_score = self.global_bias + b_u + b_i\n",
    "        \n",
    "        # Interaction vector (Element-wise product) [cite: 310]\n",
    "        interaction = u * i           # (batch, dim)\n",
    "        \n",
    "        # Calculate Intention (delta) for each stage\n",
    "        # We want delta_l = dot(stage_l, interaction)\n",
    "        # We can do this via matrix multiplication for all stages at once\n",
    "        # stages weight shape: (num_stages, dim)\n",
    "        # interaction shape: (batch, dim)\n",
    "        # result shape: (batch, num_stages)\n",
    "        deltas = torch.matmul(interaction, self.stage_emb.weight.t())\n",
    "        \n",
    "        # Rectify Intentions (Softplus) [cite: 313]\n",
    "        # delta_plus = (1/beta) * log(1 + exp(beta * delta))\n",
    "        deltas_plus = F.softplus(deltas, beta=self.beta)\n",
    "        \n",
    "        # Cumulative Sum (Monotonicity Enforcement) [cite: 335]\n",
    "        # We sum from l to L. \n",
    "        # In pytorch, flip -> cumsum -> flip achieves this \"reverse cumsum\"\n",
    "        scores = torch.flip(torch.cumsum(torch.flip(deltas_plus, [1]), dim=1), [1])\n",
    "        \n",
    "        # Add base biases to every stage's score\n",
    "        # scores shape: (batch, stages)\n",
    "        # base_score shape: (batch) -> unsqueeze to (batch, 1)\n",
    "        final_scores = scores + base_score.unsqueeze(1)\n",
    "        \n",
    "        return final_scores\n",
    "def edge_loss(model, u_idx, i_idx_pos, edge_indices, i_idx_neg):\n",
    "    \"\"\"\n",
    "    u_idx: User Indices\n",
    "    i_idx_pos: The game they actually played\n",
    "    edge_indices: Where they stopped (1, 2, or 3) - derived from your dataframe\n",
    "    i_idx_neg: A random game they didn't play (Stage 0)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Get scores for Positive Items\n",
    "    # Shape: (batch, 3) -> [s1, s2, s3]\n",
    "    pos_scores = model(u_idx, i_idx_pos)\n",
    "    \n",
    "    # Select the score at the specific \"edge\" for each user\n",
    "    # We need to use gather to pick s1 for user A, s3 for user B, etc.\n",
    "    # edge_indices need to be 0-based for array indexing (Stage 1 -> index 0)\n",
    "    # Your df['edge_index'] is likely 1, 2, 3. Subtract 1.\n",
    "    gather_indices = (edge_indices - 1).unsqueeze(1) # Shape (batch, 1)\n",
    "    s_edge_pos = pos_scores.gather(1, gather_indices).squeeze()\n",
    "    \n",
    "    # 2. Get scores for Negative Items\n",
    "    neg_scores = model(u_idx, i_idx_neg)\n",
    "    \n",
    "    # For negative items, we assume the user is at Stage 0 (no interaction).\n",
    "    # So we want to minimize the probability of entering Stage 1.\n",
    "    # We look at the score for Stage 1 (index 0)\n",
    "    s_neg = neg_scores[:, 0] \n",
    "    \n",
    "    # 3. Compute Probabilities (Sigmoid)\n",
    "    p_pos = torch.sigmoid(s_edge_pos)\n",
    "    p_neg = torch.sigmoid(s_neg)\n",
    "    \n",
    "    # 4. Log Likelihood Loss [cite: 380]\n",
    "    # Maximize log(p_pos) + log(1 - p_neg)\n",
    "    # Which is equivalent to minimizing:\n",
    "    loss = -torch.mean(torch.log(p_pos + 1e-10) + torch.log(1 - p_neg + 1e-10))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "############################################################\n",
    "\n",
    "############################################################\n",
    "##############         TRAINING LOOP         ###############\n",
    "############################################################\n",
    "# Prepare Tensors\n",
    "train_u = torch.LongTensor(df['user_idx'].values)\n",
    "train_i = torch.LongTensor(df['item_idx'].values)\n",
    "train_edge = torch.LongTensor(df['edge_index'].values)\n",
    "\n",
    "dataset = TensorDataset(train_u, train_i, train_edge)\n",
    "# Small batch size for this tiny fake dataset\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True) \n",
    "\n",
    "model = ChainRec(num_users, num_items, num_stages=3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005) # High LR for tiny data\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "model.train()\n",
    "for epoch in range(10): # 10 Epochs\n",
    "    total_loss = 0\n",
    "    for b_u, b_i, b_edge in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Negative Sampling: Random item for each user\n",
    "        b_neg_i = torch.randint(0, num_items, (len(b_u),))\n",
    "        \n",
    "        loss = edge_loss(model, b_u, b_i, b_edge, b_neg_i)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss {total_loss:.4f}\")\n",
    "############################################################\n",
    "\n",
    "\n",
    "############################################################\n",
    "########       PREDICTION AND REGRESSION LOGIC       #######\n",
    "############################################################\n",
    "# Create lookup maps for test data\n",
    "u_map = {u: i for i, u in enumerate(user_ids)}\n",
    "i_map = {i: idx for idx, i in enumerate(item_ids)}\n",
    "\n",
    "test_play_df = pd.read_csv('pairs_Played.csv')\n",
    "# 3. Use Maps to Translate Test Data\n",
    "test_play_df['u_idx'] = test_play_df['userID'].map(lambda x: u_map.get(x, 0))\n",
    "test_play_df['i_idx'] = test_play_df['gameID'].map(lambda x: i_map.get(x, 0))\n",
    "\n",
    "# A. Predict \"Played\" (Stage 1 Score)\n",
    "\n",
    "u_test = torch.LongTensor(test_play_df['u_idx'])\n",
    "i_test = torch.LongTensor(test_play_df['i_idx'])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(u_test, i_test)\n",
    "    probs_played = torch.sigmoid(scores[:, 0]).numpy() # Stage 1\n",
    "\n",
    "test_play_df['prediction'] = probs_played\n",
    "print(\"\\nPlayed Predictions:\")\n",
    "print(test_play_df)\n",
    "\n",
    "test_hours_df = pd.read_csv('pairs_Hours.csv')\n",
    "# 3. Use Maps to Translate Test Data\n",
    "test_hours_df['u_idx'] = test_hours_df['userID'].map(lambda x: u_map.get(x, 0))\n",
    "test_hours_df['i_idx'] = test_hours_df['gameID'].map(lambda x: i_map.get(x, 0))\n",
    "\n",
    "# A. Predict \"Played\" (Stage 1 Score)\n",
    "\n",
    "u_test = torch.LongTensor(test_hours_df['u_idx'])\n",
    "i_test = torch.LongTensor(test_hours_df['i_idx'])\n",
    "\n",
    "# B. Predict \"Hours\" (Regression from Scores)\n",
    "# 1. Get scores for training data\n",
    "with torch.no_grad():\n",
    "    train_scores = model(train_u, train_i).numpy()\n",
    "\n",
    "# 2. Train Regressor (Model Scores -> Real Hours)\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(train_scores, df['hours_transformed'])\n",
    "\n",
    "# 3. Predict\n",
    "with torch.no_grad():\n",
    "    test_scores = model(u_test, i_test).numpy()\n",
    "    pred_hours = regressor.predict(test_scores)\n",
    "\n",
    "test_hours_df['prediction'] = pred_hours\n",
    "print(\"\\nHours Predictions:\")\n",
    "print(test_hours_df)\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b3b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "############################################################\n",
    "###########        READ/PREP TRAINING DATA       ###########\n",
    "############################################################\n",
    "# read data\n",
    "df = pd.DataFrame(train_data)\n",
    "\n",
    "# define features: normailized text length, user_idx(userID -> int), item_idx(gameID -> int)\n",
    "df['text_len_norm'] = df['text'].str.len()/df['text'].str.len().max()\n",
    "df['user_idx'], user_ids = pd.factorize(df['userID'])\n",
    "df['item_idx'], item_ids = pd.factorize(df['gameID'])\n",
    "\n",
    "# get user/game counts\n",
    "num_users = len(user_ids)\n",
    "num_items = len(item_ids)\n",
    "# calc threshhold for transfrmed hours and normalized text length\n",
    "hours_threshold = df['hours_transformed'].mean()\n",
    "text_threshold = df['text_len_norm'].mean()\n",
    "\n",
    "# define create_monotonic_chain function\n",
    "def create_monotonic_chain(row):\n",
    "    \"\"\"\n",
    "    Creates a ground truth vector y that enforces y_1 >= y_2 >= y_3\n",
    "    \"\"\"\n",
    "    y1 = 1 # Played\n",
    "    y2 = 1 if row['hours_transformed'] > hours_threshold else 0 # Engaged\n",
    "    y3 = 1 if row['text_len_norm'] > text_threshold else 0      # Reviewed\n",
    "    \n",
    "    # Enforce Monotonicity: Review -> Engaged\n",
    "    if y3 == 1:\n",
    "        y2 = 1\n",
    "        \n",
    "    return [y1, y2, y3]\n",
    "\n",
    "# define features: chain_labels and edge_index\n",
    "df['chain_labels'] = df.apply(create_monotonic_chain, axis=1)\n",
    "df['edge_index'] = df['chain_labels'].apply(sum) # 1, 2, or 3\n",
    "\n",
    "print(f\"Data Prepared. Users: {num_users}, Items: {num_items}\")\n",
    "print(df[['userID', 'edge_index']].head())\n",
    "############################################################\n",
    "\n",
    "\n",
    "############################################################\n",
    "#######    DEFINE CHAINREC MODEL AND EDGE LOSS     #########\n",
    "############################################################\n",
    "class ChainRec(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_stages=3, embed_dim=16, beta=1.0):\n",
    "        super().__init__()\n",
    "        self.num_stages = num_stages\n",
    "        self.beta = beta\n",
    "        \n",
    "        # 1. Embeddings (User & Item)\n",
    "        self.user_emb = nn.Embedding(num_users, embed_dim)\n",
    "        self.item_emb = nn.Embedding(num_items, embed_dim)\n",
    "        \n",
    "        # 2. Biases\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # 3. Stage Embeddings (The \"Lenses\")\n",
    "        # Shape: (num_stages, embed_dim)\n",
    "        self.stage_emb = nn.Embedding(num_stages, embed_dim)\n",
    "        \n",
    "        # Initialize weights (Optional but good practice)\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.stage_emb.weight)\n",
    "\n",
    "    def forward(self, u_idx, i_idx):\n",
    "        \"\"\"\n",
    "        Returns: A tensor of shape (batch_size, num_stages) \n",
    "                 containing scores [s_1, s_2, s_3] for each pair.\n",
    "        \"\"\"\n",
    "        # Look up embeddings\n",
    "        u = self.user_emb(u_idx)      # (batch, dim)\n",
    "        i = self.item_emb(i_idx)      # (batch, dim)\n",
    "        \n",
    "        # Biases\n",
    "        b_u = self.user_bias(u_idx).squeeze()\n",
    "        b_i = self.item_bias(i_idx).squeeze()\n",
    "        base_score = self.global_bias + b_u + b_i\n",
    "        \n",
    "        # Interaction vector (Element-wise product) [cite: 310]\n",
    "        interaction = u * i           # (batch, dim)\n",
    "        \n",
    "        # Calculate Intention (delta) for each stage\n",
    "        # We want delta_l = dot(stage_l, interaction)\n",
    "        # We can do this via matrix multiplication for all stages at once\n",
    "        # stages weight shape: (num_stages, dim)\n",
    "        # interaction shape: (batch, dim)\n",
    "        # result shape: (batch, num_stages)\n",
    "        deltas = torch.matmul(interaction, self.stage_emb.weight.t())\n",
    "        \n",
    "        # Rectify Intentions (Softplus) [cite: 313]\n",
    "        # delta_plus = (1/beta) * log(1 + exp(beta * delta))\n",
    "        deltas_plus = F.softplus(deltas, beta=self.beta)\n",
    "        \n",
    "        # Cumulative Sum (Monotonicity Enforcement) [cite: 335]\n",
    "        # We sum from l to L. \n",
    "        # In pytorch, flip -> cumsum -> flip achieves this \"reverse cumsum\"\n",
    "        scores = torch.flip(torch.cumsum(torch.flip(deltas_plus, [1]), dim=1), [1])\n",
    "        \n",
    "        # Add base biases to every stage's score\n",
    "        # scores shape: (batch, stages)\n",
    "        # base_score shape: (batch) -> unsqueeze to (batch, 1)\n",
    "        final_scores = scores + base_score.unsqueeze(1)\n",
    "        \n",
    "        return final_scores\n",
    "def edge_loss(model, u_idx, i_idx_pos, edge_indices, i_idx_neg):\n",
    "    \"\"\"\n",
    "    u_idx: User Indices\n",
    "    i_idx_pos: The game they actually played\n",
    "    edge_indices: Where they stopped (1, 2, or 3) - derived from your dataframe\n",
    "    i_idx_neg: A random game they didn't play (Stage 0)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Get scores for Positive Items\n",
    "    # Shape: (batch, 3) -> [s1, s2, s3]\n",
    "    pos_scores = model(u_idx, i_idx_pos)\n",
    "    \n",
    "    # Select the score at the specific \"edge\" for each user\n",
    "    # We need to use gather to pick s1 for user A, s3 for user B, etc.\n",
    "    # edge_indices need to be 0-based for array indexing (Stage 1 -> index 0)\n",
    "    # Your df['edge_index'] is likely 1, 2, 3. Subtract 1.\n",
    "    gather_indices = (edge_indices - 1).unsqueeze(1) # Shape (batch, 1)\n",
    "    s_edge_pos = pos_scores.gather(1, gather_indices).squeeze()\n",
    "    \n",
    "    # 2. Get scores for Negative Items\n",
    "    neg_scores = model(u_idx, i_idx_neg)\n",
    "    \n",
    "    # For negative items, we assume the user is at Stage 0 (no interaction).\n",
    "    # So we want to minimize the probability of entering Stage 1.\n",
    "    # We look at the score for Stage 1 (index 0)\n",
    "    s_neg = neg_scores[:, 0] \n",
    "    \n",
    "    # 3. Compute Probabilities (Sigmoid)\n",
    "    p_pos = torch.sigmoid(s_edge_pos)\n",
    "    p_neg = torch.sigmoid(s_neg)\n",
    "    \n",
    "    # 4. Log Likelihood Loss [cite: 380]\n",
    "    # Maximize log(p_pos) + log(1 - p_neg)\n",
    "    # Which is equivalent to minimizing:\n",
    "    loss = -torch.mean(torch.log(p_pos + 1e-10) + torch.log(1 - p_neg + 1e-10))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "############################################################\n",
    "\n",
    "############################################################\n",
    "##############         TRAINING LOOP         ###############\n",
    "############################################################\n",
    "# Prepare Tensors\n",
    "train_u = torch.LongTensor(df['user_idx'].values)\n",
    "train_i = torch.LongTensor(df['item_idx'].values)\n",
    "train_edge = torch.LongTensor(df['edge_index'].values)\n",
    "\n",
    "dataset = TensorDataset(train_u, train_i, train_edge)\n",
    "# Small batch size for this tiny fake dataset\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True) \n",
    "\n",
    "model = ChainRec(num_users, num_items, num_stages=3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005) # High LR for tiny data\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "model.train()\n",
    "for epoch in range(10): # 10 Epochs\n",
    "    total_loss = 0\n",
    "    for b_u, b_i, b_edge in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Negative Sampling: Random item for each user\n",
    "        b_neg_i = torch.randint(0, num_items, (len(b_u),))\n",
    "        \n",
    "        loss = edge_loss(model, b_u, b_i, b_edge, b_neg_i)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss {total_loss:.4f}\")\n",
    "############################################################\n",
    "\n",
    "\n",
    "############################################################\n",
    "########       PREDICTION AND REGRESSION LOGIC       #######\n",
    "############################################################\n",
    "# Create lookup maps for test data\n",
    "u_map = {u: i for i, u in enumerate(user_ids)}\n",
    "i_map = {i: idx for idx, i in enumerate(item_ids)}\n",
    "\n",
    "test_play_df = pd.read_csv('pairs_Played.csv')\n",
    "# 3. Use Maps to Translate Test Data\n",
    "test_play_df['u_idx'] = test_play_df['userID'].map(lambda x: u_map.get(x, 0))\n",
    "test_play_df['i_idx'] = test_play_df['gameID'].map(lambda x: i_map.get(x, 0))\n",
    "\n",
    "# A. Predict \"Played\" (Stage 1 Score)\n",
    "\n",
    "u_test = torch.LongTensor(test_play_df['u_idx'])\n",
    "i_test = torch.LongTensor(test_play_df['i_idx'])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(u_test, i_test)\n",
    "    probs_played = torch.sigmoid(scores[:, 0]).numpy() # Stage 1\n",
    "\n",
    "test_play_df['prediction'] = probs_played\n",
    "print(\"\\nPlayed Predictions:\")\n",
    "print(test_play_df)\n",
    "\n",
    "test_hours_df = pd.read_csv('pairs_Hours.csv')\n",
    "# 3. Use Maps to Translate Test Data\n",
    "test_hours_df['u_idx'] = test_hours_df['userID'].map(lambda x: u_map.get(x, 0))\n",
    "test_hours_df['i_idx'] = test_hours_df['gameID'].map(lambda x: i_map.get(x, 0))\n",
    "\n",
    "# A. Predict \"Played\" (Stage 1 Score)\n",
    "\n",
    "u_test = torch.LongTensor(test_hours_df['u_idx'])\n",
    "i_test = torch.LongTensor(test_hours_df['i_idx'])\n",
    "\n",
    "# B. Predict \"Hours\" (Regression from Scores)\n",
    "# 1. Get scores for training data\n",
    "with torch.no_grad():\n",
    "    train_scores = model(train_u, train_i).numpy()\n",
    "\n",
    "# 2. Train Regressor (Model Scores -> Real Hours)\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(train_scores, df['hours_transformed'])\n",
    "\n",
    "# 3. Predict\n",
    "with torch.no_grad():\n",
    "    test_scores = model(u_test, i_test).numpy()\n",
    "    pred_hours = regressor.predict(test_scores)\n",
    "\n",
    "test_hours_df['prediction'] = pred_hours\n",
    "print(\"\\nHours Predictions:\")\n",
    "print(test_hours_df)\n",
    "############################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
