{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4187aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import dateutil.parser\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d10c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import homework1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e983fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/home/scotty/dsc_256/fall_25/module_01/hw/data/datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9506e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.open(root + \"/fantasy_10000.json.gz\")\n",
    "dataset = []\n",
    "for l in f:\n",
    "    dataset.append(json.loads(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa1c922a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '8842281e1d1347389f2ab93d60773d4d',\n",
       " 'book_id': '18245960',\n",
       " 'review_id': 'dfdbb7b0eb5a7e4c26d59a937e2e5feb',\n",
       " 'rating': 5,\n",
       " 'review_text': 'This is a special book. It started slow for about the first third, then in the middle third it started to get interesting, then the last third blew my mind. This is what I love about good science fiction - it pushes your thinking about where things can go. \\n It is a 2015 Hugo winner, and translated from its original Chinese, which made it interesting in just a different way from most things I\\'ve read. For instance the intermixing of Chinese revolutionary history - how they kept accusing people of being \"reactionaries\", etc. \\n It is a book about science, and aliens. The science described in the book is impressive - its a book grounded in physics and pretty accurate as far as I could tell. Though when it got to folding protons into 8 dimensions I think he was just making stuff up - interesting to think about though. \\n But what would happen if our SETI stations received a message - if we found someone was out there - and the person monitoring and answering the signal on our side was disillusioned? That part of the book was a bit dark - I would like to think human reaction to discovering alien civilization that is hostile would be more like Enders Game where we would band together. \\n I did like how the book unveiled the Trisolaran culture through the game. It was a smart way to build empathy with them and also understand what they\\'ve gone through across so many centuries. And who know a 3 body problem was an unsolvable math problem? But I still don\\'t get who made the game - maybe that will come in the next book. \\n I loved this quote: \\n \"In the long history of scientific progress, how many protons have been smashed apart in accelerators by physicists? How many neutrons and electrons? Probably no fewer than a hundred million. Every collision was probably the end of the civilizations and intelligences in a microcosmos. In fact, even in nature, the destruction of universes must be happening at every second--for example, through the decay of neutrons. Also, a high-energy cosmic ray entering the atmosphere may destroy thousands of such miniature universes....\"',\n",
       " 'date_added': 'Sun Jul 30 07:44:10 -0700 2017',\n",
       " 'date_updated': 'Wed Aug 30 00:00:26 -0700 2017',\n",
       " 'read_at': 'Sat Aug 26 12:05:52 -0700 2017',\n",
       " 'started_at': 'Tue Aug 15 13:23:18 -0700 2017',\n",
       " 'n_votes': 28,\n",
       " 'n_comments': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b00bdbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 {'user_id': 'c84807d576e92c3746180ec064fbd440', 'book_id': '17853851', 'review_id': '83c175b6faf1ffc0b8d0dac0d7d4f3d9', 'rating': 3, 'review_text': \"I was really looking forward to Nick and Shaya's story. I enjoyed it but I did find it lacking in some ways. It wasn't what I expected of these characters, as their story has been built up from the two previous books. It didn't feel like a Phoenix Pack story to me, not bad just not what I expected.\", 'date_added': 'Wed May 29 10:28:27 -0700 2013', 'date_updated': 'Thu Feb 13 12:10:54 -0800 2014', 'read_at': 'Thu Feb 13 12:10:54 -0800 2014', 'started_at': 'Thu Feb 13 00:00:00 -0800 2014', 'n_votes': 2, 'n_comments': 0}\n",
      "(5000, 19)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by LinearRegression.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mprint\u001b[39m(test_mse2)\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m test_mse2, test_mse3\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[43mQ4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mQ4\u001b[39m\u001b[34m(dataset)\u001b[39m\n\u001b[32m     60\u001b[39m model = LinearRegression(fit_intercept=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     61\u001b[39m model.fit(X2_train, y_train)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m y_pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX2_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m#Sum of squared errors (SSE)\u001b[39;00m\n\u001b[32m     66\u001b[39m sse = \u001b[38;5;28msum\u001b[39m([x**\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (y_test - y_pred)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ucsd/lib/python3.12/site-packages/sklearn/linear_model/_base.py:297\u001b[39m, in \u001b[36mLinearModel.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    284\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    Predict using the linear model.\u001b[39;00m\n\u001b[32m    286\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    295\u001b[39m \u001b[33;03m        Returns predicted values.\u001b[39;00m\n\u001b[32m    296\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ucsd/lib/python3.12/site-packages/sklearn/linear_model/_base.py:276\u001b[39m, in \u001b[36mLinearModel._decision_function\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    274\u001b[39m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcoo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m     coef_ = \u001b[38;5;28mself\u001b[39m.coef_\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coef_.ndim == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ucsd/lib/python3.12/site-packages/sklearn/utils/validation.py:2944\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2942\u001b[39m         out = X, y\n\u001b[32m   2943\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ucsd/lib/python3.12/site-packages/sklearn/utils/validation.py:1130\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1128\u001b[39m     n_samples = _num_samples(array)\n\u001b[32m   1129\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_samples < ensure_min_samples:\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m) while a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1133\u001b[39m             % (n_samples, array.shape, ensure_min_samples, context)\n\u001b[32m   1134\u001b[39m         )\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m   1137\u001b[39m     n_features = array.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by LinearRegression."
     ]
    }
   ],
   "source": [
    "## import libraries\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gzip\n",
    "import json\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "def Q4(dataset):\n",
    "    def getMaxLen(dataset):\n",
    "        # Find the longest review (number of characters)\n",
    "        review_lens = [len(i['review_text']) for i in dataset]\n",
    "        maxLen = max(review_lens)\n",
    "        return maxLen\n",
    "    \n",
    "    def featureQ2(datum, maxLen):\n",
    "        # Implement (should be 1, length feature, day feature, month feature)\n",
    "        len_feature = len(datum['review_text'])/maxLen\n",
    "        wd_encoder = OneHotEncoder(drop='first', handle_unknown='error', sparse_output=False)\n",
    "        m_encoder = OneHotEncoder(drop='first', handle_unknown='error', sparse_output=False)\n",
    "\n",
    "        wd_encoder.fit(np.array(['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']).reshape(-1, 1))\n",
    "        m_encoder.fit(np.array(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']).reshape(-1, 1))\n",
    "\n",
    "        day_feature = wd_encoder.transform([[datum['date_added'].split(' ')[0]]]).ravel()\n",
    "        month_feature = m_encoder.transform([[datum['date_added'].split(' ')[1]]]).ravel()\n",
    "        \n",
    "        feature = np.concatenate((np.array([1]),np.array([len_feature]), day_feature, month_feature))\n",
    "        return feature\n",
    "\n",
    "    def featureQ3(datum, maxLen):\n",
    "        # Implement (should be 1, length feature, day feature, month feature)\n",
    "        len_feature = len(datum['review_text'])/maxLen\n",
    "        day_feature = datetime.strptime(datum['date_added'].split(' ')[0],'%a').isoweekday()\n",
    "        month_feature = datetime.strptime(datum['date_added'].split(' ')[1],'%b').month\n",
    "        return [1] + [len_feature] + [day_feature] + [month_feature]\n",
    "    \n",
    "    maxLen = getMaxLen(dataset)\n",
    "    #dataset4 = dataset[:]\n",
    "    random.seed(0)\n",
    "    random.shuffle(dataset)\n",
    "    half_num = len(dataset)//2\n",
    "    dataset_train = dataset[:half_num]\n",
    "    print(len(dataset_train),dataset_train[0])\n",
    "    dataset_test = dataset[len(dataset)+1:1]\n",
    "    X2_train = np.array([featureQ2(d,maxLen) for d in dataset_train])\n",
    "    print(X2_train.shape)\n",
    "    X2_test = [featureQ2(d,maxLen) for d in dataset_test]\n",
    "    X3_train = [featureQ3(d,maxLen) for d in dataset_train]\n",
    "    X3_test = [featureQ3(d,maxLen) for d in dataset_test]\n",
    "    y_train = [d['rating'] for d in dataset_train]\n",
    "    y_test = [d['rating'] for d in dataset_test]\n",
    "    model = LinearRegression(fit_intercept=False)\n",
    "    model.fit(X2_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(np.array(X2_test).reshape(-1,1))\n",
    "    #Sum of squared errors (SSE)\n",
    "\n",
    "    sse = sum([x**2 for x in (y_test - y_pred)])\n",
    "    #Mean squared error (MSE)\n",
    "\n",
    "    test_mse2 = sse / len(y_test)\n",
    "\n",
    "    print(test_mse2)\n",
    "\n",
    "    model = LinearRegression(fit_intercept=False)\n",
    "    model.fit(X3_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X3_test)\n",
    "    #Sum of squared errors (SSE)\n",
    "\n",
    "    sse = sum([x**2 for x in (y_test - y_pred)])\n",
    "    #Mean squared error (MSE)\n",
    "\n",
    "    test_mse3 = sse / len(y_test)\n",
    "    print(test_mse2)\n",
    "\n",
    "    return test_mse2, test_mse3\n",
    "Q4(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65dffb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(1.5266596343007535), np.float64(1.5191618561424114))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import libraries\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gzip\n",
    "import json\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "def Q4(dataset):\n",
    "    def feature_extractors_setup(dataset):\n",
    "        \"\"\"Initializes and fits all necessary components for both models.\"\"\"\n",
    "    \n",
    "        # 1. Calculate max length\n",
    "        maxLen = max(len(d['review_text']) for d in dataset)\n",
    "    \n",
    "        # 2. Setup and Fit Encoders (for Model 2: One-Hot)\n",
    "        wd_encoder = OneHotEncoder(drop='first', handle_unknown='error', sparse_output=False)\n",
    "        m_encoder = OneHotEncoder(drop='first', handle_unknown='error', sparse_output=False)\n",
    "    \n",
    "        # Fit: [Sun, Mon, ...] and [Jan, Feb, ...]\n",
    "        wd_cats = np.array(['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']).reshape(-1, 1)\n",
    "        m_cats = np.array(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']).reshape(-1, 1)\n",
    "    \n",
    "        wd_encoder.fit(wd_cats)\n",
    "        m_encoder.fit(m_cats)\n",
    "    \n",
    "        return maxLen, wd_encoder, m_encoder\n",
    "    def get_features_model2(datum, maxLen, wd_encoder, m_encoder):\n",
    "        \"\"\"Generates One-Hot features (Model 2) using fitted encoders.\"\"\"\n",
    "        len_feature = len(datum['review_text']) / maxLen\n",
    "        date_parts = datum['date_added'].split(' ')\n",
    "        day_abbr = date_parts[0]\n",
    "        month_abbr = date_parts[1]\n",
    "    \n",
    "        # Use .transform() with fitted encoders\n",
    "        day_feature_1d = wd_encoder.transform([[day_abbr]]).ravel()\n",
    "        month_feature_1d = m_encoder.transform([[month_abbr]]).ravel()\n",
    "    \n",
    "        # Concatenate: [Bias (1), Length (1), Day (6), Month (11)]\n",
    "        base_features = np.array([1.0, len_feature])\n",
    "        return np.concatenate((base_features, day_feature_1d, month_feature_1d))\n",
    "\n",
    "    def get_features_model3(datum, maxLen):\n",
    "        \"\"\"Generates Raw Integer features (Model 3).\"\"\"\n",
    "        # Note: Using %a %b %d %Y format codes for a robust parse\n",
    "        full_date_string = datum['date_added'] + ' 01 2000' # Append dummy day/year to parse fully\n",
    "    \n",
    "        len_feature = len(datum['review_text']) / maxLen\n",
    "    \n",
    "        # Extract integer features\n",
    "        try:\n",
    "            t = datetime.strptime(full_date_string, '%a %b %d %Y')\n",
    "            day_feature = t.isoweekday() # 1-7\n",
    "            month_feature = t.month      # 1-12\n",
    "        except ValueError:\n",
    "            # Fallback if date is malformed (shouldn't happen with given data structure)\n",
    "            day_feature = 0\n",
    "            month_feature = 0\n",
    "    \n",
    "        # Return as NumPy array directly\n",
    "        return np.array([1.0, len_feature, day_feature, month_feature])\n",
    "    \n",
    "    maxLen, wd_encoder, m_encoder = feature_extractors_setup(dataset)\n",
    "    \n",
    "    # 2. DATA SPLIT (CRITICAL FIXES HERE)\n",
    "    random.seed(0)\n",
    "    # Important: Create a copy of the dataset before shuffling to avoid side effects\n",
    "    dataset_shuffled = dataset[:] \n",
    "    random.shuffle(dataset_shuffled) \n",
    "    \n",
    "    half_num = len(dataset_shuffled) // 2\n",
    "    \n",
    "    # Correct slicing for 50/50 split\n",
    "    dataset_train = dataset_shuffled[:half_num]\n",
    "    dataset_test = dataset_shuffled[half_num:] \n",
    "    \n",
    "    # 3. FEATURE MATRIX CONSTRUCTION\n",
    "    \n",
    "    # Features for Model 2 (One-Hot)\n",
    "    # The encoders were FIT on the whole dataset (if train/test are subsets of a whole).\n",
    "    # For a strictly correct split, we should refit on dataset_train, but to match the\n",
    "    # Q2 spirit, we use the encoders established by the whole process.\n",
    "    X2_train = np.array([get_features_model2(d, maxLen, wd_encoder, m_encoder) for d in dataset_train])\n",
    "    X2_test = np.array([get_features_model2(d, maxLen, wd_encoder, m_encoder) for d in dataset_test])\n",
    "    \n",
    "    # Features for Model 3 (Raw Integer)\n",
    "    X3_train = np.array([get_features_model3(d, maxLen) for d in dataset_train])\n",
    "    X3_test = np.array([get_features_model3(d, maxLen) for d in dataset_test])\n",
    "    \n",
    "    # Targets\n",
    "    y_train = np.array([d['rating'] for d in dataset_train])\n",
    "    y_test = np.array([d['rating'] for d in dataset_test])\n",
    "    \n",
    "    # 4. TRAIN AND TEST MODEL 2 (One-Hot)\n",
    "    model2 = LinearRegression(fit_intercept=False)\n",
    "    model2.fit(X2_train, y_train)\n",
    "    \n",
    "    y_pred2 = model2.predict(X2_test)\n",
    "    \n",
    "    # MSE Calculation (Robust NumPy method)\n",
    "    test_mse2 = np.mean((y_test - y_pred2)**2)\n",
    "    \n",
    "    # 5. TRAIN AND TEST MODEL 3 (Raw Integer)\n",
    "    model3 = LinearRegression(fit_intercept=False)\n",
    "    model3.fit(X3_train, y_train)\n",
    "    \n",
    "    y_pred3 = model3.predict(X3_test)\n",
    "    \n",
    "    # MSE Calculation (Robust NumPy method)\n",
    "    test_mse3 = np.mean((y_test - y_pred3)**2)\n",
    "    \n",
    "    # 6. RETURN RESULTS\n",
    "    return test_mse2, test_mse3\n",
    "\n",
    "Q4(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dce3b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset:\n",
    "    t = dateutil.parser.parse(d['date_added'])\n",
    "    d['parsed_date'] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038717f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m     MSE2 = MSE[\u001b[32m0\u001b[39m]\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y, MSE2\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[43mQ2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mQ2\u001b[39m\u001b[34m(dataset)\u001b[39m\n\u001b[32m     46\u001b[39m MSE = residuals/\u001b[38;5;28mlen\u001b[39m(y)\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(MSE)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m MSE2 = \u001b[43mMSE\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, MSE2\n",
      "\u001b[31mIndexError\u001b[39m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "dataset[0]\n",
    "def getMaxLen(dataset):\n",
    "    # Find the longest review (number of characters)\n",
    "    review_lens = [len(i['review_text']) for i in dataset]\n",
    "    maxLen = max(review_lens)\n",
    "    return maxLen\n",
    "\n",
    "def featureQ2(datum, maxLen):\n",
    "    # note: should be 1, length feature, day feature (list), month feature (list)\n",
    "    # normailize length feature\n",
    "    len_feature = len(datum['review_text'])/maxLen\n",
    "    date_dt = datum['parsed_date']\n",
    "\n",
    "    # weekday returns Mon: 0,...,Sun:6\n",
    "    day = date_dt.weekday()\n",
    "\n",
    "    # initalize day feature \n",
    "    day_feature = [0,0,0,0,0,0,0]\n",
    "    \n",
    "    # day is index and mark as true  for element reprsenting day of week\n",
    "    day_feature[day] = 1\n",
    "\n",
    "    # month returns Jan:1,...,Dec:12 (must account for index start at zero)\n",
    "    month = date_dt.month\n",
    "\n",
    "    # initalize month feature then insert into list\n",
    "    month_feature = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    # day is index and mark as true  for element reprsenting day of week\n",
    "    day_feature[day] = 1\n",
    "    \n",
    "    # since counting starts at 0 Jan:0,...,Dec:11\n",
    "    month_feature[month-1] = 1\n",
    "\n",
    "    # build Q2 feature \n",
    "    # note: drop first element for month and day feature to prevent multicollinearity\n",
    "    feature = np.concatenate((np.array([1]),np.array([len_feature]), np.array(day_feature[1:]), np.array(month_feature[1:])))\n",
    "\n",
    "    return feature\n",
    "\n",
    "def Q2(dataset):\n",
    "    # get max len of 'review_text'\n",
    "    maxLen = getMaxLen(dataset)\n",
    "\n",
    "    # build features(X) and target(y) lists\n",
    "    X = [featureQ2(d,maxLen) for d in dataset]\n",
    "    y = [d['rating'] for d in dataset]\n",
    "    \n",
    "    # calc least squares\n",
    "    theta,residuals,rank,s = np.linalg.lstsq(X, y)\n",
    "\n",
    "    # calc mean squared error\n",
    "    MSE = residuals/len(y)\n",
    "    print(MSE)\n",
    "    MSE2 = MSE[0]\n",
    "\n",
    "    return X, y, MSE2\n",
    "Q2(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de30310",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset4 = dataset[:]\n",
    "random.seed(0)\n",
    "random.shuffle(dataset4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b3c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gzip\n",
    "import json\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "\n",
    "root = '/home/scotty/dsc_256/fall_25/module_01/hw/data/datasets'\n",
    "\n",
    "f = open(f\"{root}/beer_50000.json\")\n",
    "datasetB = []\n",
    "for l in f:\n",
    "    datasetB.append(eval(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "889735b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance by component: [0.39646459 0.17578565 0.12424039 0.09263147]\n",
      "Total variance explained by 2 components: 0.79\n",
      "\n",
      "PCA Component Loadings:\n",
      "     text len  review/text  appearance    palate     taste     aroma  \\\n",
      "PC1  0.178561     0.134287    0.413668  0.469787  0.482680  0.469809   \n",
      "PC2  0.650991     0.674400   -0.134852 -0.149903 -0.160307 -0.120212   \n",
      "PC3  0.128017     0.101006    0.025232  0.012069  0.014497  0.003619   \n",
      "PC4 -0.047527    -0.251601   -0.221796 -0.143628 -0.138049 -0.043574   \n",
      "\n",
      "         hour       abv  \n",
      "PC1 -0.035841  0.321367  \n",
      "PC2 -0.133582  0.150816  \n",
      "PC3  0.982780 -0.080917  \n",
      "PC4  0.116716  0.911064  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.int64(27777),\n",
       " np.int64(12804),\n",
       " np.int64(3584),\n",
       " np.int64(5835),\n",
       " np.float64(0.19614766100917436))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "def extract_beer_features(datum):\n",
    "    features = [\n",
    "        len(datum['review/text']),\n",
    "        datum['review/text'].count('\\t'),\n",
    "        datum['review/appearance'],\n",
    "        datum['review/palate'],\n",
    "        datum['review/taste'],\n",
    "        datum['review/aroma'],\n",
    "        datum['review/timeStruct']['hour'],\n",
    "        # Use .get() with a default value in case 'beer/ABV' is missing\n",
    "        datum.get('beer/ABV', 0) or 0 # Handles None or missing key\n",
    "    ]\n",
    "    \n",
    "    return features\n",
    "\n",
    "\"\"\"\n",
    "{'review/appearance': 2.5, \n",
    " 'beer/style': 'Hefeweizen', \n",
    " 'review/palate': 1.5, \n",
    " 'review/taste': 1.5, \n",
    " 'beer/name': 'Sausa Weizen', \n",
    " 'review/timeUnix': 1234817823, \n",
    " 'beer/ABV': 5.0, \n",
    " 'beer/beerId': '47986', \n",
    " 'beer/brewerId': '10325', \n",
    " 'review/timeStruct': {'isdst': 0, 'mday': 16, 'hour': 20, 'min': 57, 'sec': 3, 'mon': 2, 'year': 2009, 'yday': 47, 'wday': 0}, \n",
    " 'review/overall': 1.5, \n",
    " , \n",
    " 'user/profileName': 'stcules', \n",
    " 'review/aroma': 2.0}\n",
    "\"\"\"\n",
    "\n",
    "def Q5_with_PCA(dataset, feat_func):\n",
    "    \"\"\"\n",
    "    Applies a StandardScaler -> PCA -> LogisticRegression pipeline.\n",
    "    \"\"\"\n",
    "    # Build feature matrix X and target vector y\n",
    "    X = np.array([feat_func(d) for d in dataset])\n",
    "    y = np.array([d['review/overall'] >= 4 for d in dataset])\n",
    "\n",
    "    # 1. Scale the features\n",
    "    # This standardizes features by removing the mean and scaling to unit variance.\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # 2. Apply PCA for dimensionality reduction\n",
    "    # We'll reduce our 6 features down to 2 principal components.\n",
    "    # You can experiment with different numbers for n_components.\n",
    "    pca = PCA(n_components=4)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    print(f\"Explained variance by component: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"Total variance explained by 2 components: {np.sum(pca.explained_variance_ratio_):.2f}\\n\")\n",
    "\n",
    "    # 4. View the Component Loadings in a DataFrame for easy reading\n",
    "    feature_names = ['text len','review/text','appearance','palate','taste','aroma','hour','abv']\n",
    "    components_df = pd.DataFrame(\n",
    "        pca.components_, \n",
    "        columns=feature_names, \n",
    "        index=['PC1', 'PC2','PC3', 'PC4']\n",
    "    )\n",
    "    print(\"PCA Component Loadings:\")\n",
    "    print(components_df)\n",
    "    \n",
    "    # 3. Train the logistic regression model on the PCA-transformed data\n",
    "    log_reg = LogisticRegression(class_weight='balanced')\n",
    "    log_reg.fit(X_pca, y) # Notice we use X_pca here\n",
    "    \n",
    "    # Make predictions using the PCA-transformed data\n",
    "    y_pred = log_reg.predict(X_pca)\n",
    "\n",
    "    # Calculate balanced error rate (1 - balanced accuracy)\n",
    "    bal_accuracy = balanced_accuracy_score(y, y_pred)\n",
    "    bal_error_rate = 1 - bal_accuracy\n",
    "    \n",
    "    # --- The TP/TN/FP/FN calculations remain the same ---\n",
    "    y_true = y.astype(int)\n",
    "    y_pred_int = y_pred.astype(int)\n",
    "    true_positives = np.sum(y_true * y_pred_int)\n",
    "    true_negatives = np.sum((1 - y_true) * (1 - y_pred_int))\n",
    "    false_positives = np.sum((1 - y_true) * y_pred_int)\n",
    "    false_negatives = np.sum(y_true * (1 - y_pred_int))\n",
    "\n",
    "    return true_positives, true_negatives, false_positives, false_negatives, bal_error_rate\n",
    "\n",
    "Q5_with_PCA(datasetB,extract_beer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67295057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14201 10503 5885 19411 0.4683031525957275\n",
      "24522 11878 4510 9090 0.2728202478681687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.int64(24522),\n",
       " np.int64(11878),\n",
       " np.int64(4510),\n",
       " np.int64(9090),\n",
       " np.float64(0.2728202478681687))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "def featureQ5(datum):\n",
    "    # Feature vector for one data point\n",
    "    feature = len(datum['review/text'])\n",
    "    return [1] + [feature]\n",
    "\n",
    "def featureQ7(datum):\n",
    "    # Implement (any feature vector which improves performance over Q5)\n",
    "    appearance = datum['review/appearance']\n",
    "    palate = datum['review/palate']\n",
    "    taste = datum['review/taste']\n",
    "    aroma = datum['review/aroma']\n",
    "    abv = datum.get('beer/ABV', 0) \n",
    "    \n",
    "    # calc weighted sum using results from PCA\n",
    "    yum_factor = (0.41 * appearance + \n",
    "                     0.47 * palate + \n",
    "                     0.48 * taste + \n",
    "                     0.47 * aroma + \n",
    "                     0.32 * abv)\n",
    "    \n",
    "    return [1] + [yum_factor]\n",
    "\n",
    "def Q5(dataset,feat_func):\n",
    "\n",
    "    # build feature(X) and target(y) lists\n",
    "    X = np.array([feat_func(d) for d in dataset])\n",
    "    y = np.array([d['review/overall'] >=4 for d in dataset])\n",
    "\n",
    "    # initalize sklearn logistic regression model\n",
    "    log_reg = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "    # fit logistic regression model\n",
    "    log_reg.fit(X,y)\n",
    "\n",
    "    # make predictions using log_reg model\n",
    "    y_pred = log_reg.predict(X)\n",
    "\n",
    "    #define true condition True:1 and False:0\n",
    "    y_true = y.astype(int) \n",
    "    y_pred_int = y_pred.astype(int)\n",
    "\n",
    "    # find true positives\n",
    "    true_positives = np.sum(y_true * y_pred_int)\n",
    "\n",
    "    #find true negatives\n",
    "    true_negatives = np.sum((1 - y_true) * (1 - y_pred_int))\n",
    "\n",
    "    # find false positives\n",
    "    false_positives = np.sum((1 - y_true) * y_pred_int)\n",
    "\n",
    "    # find false negatives\n",
    "    false_negatives = np.sum(y_true * (1 - y_pred_int))\n",
    "\n",
    "    # Calculate balanced error rate (1-accuracy)\n",
    "    bal_accuracy = balanced_accuracy_score(y, y_pred)\n",
    "    bal_error_rate = 1 - bal_accuracy\n",
    "    print(true_positives, true_negatives, false_positives, false_negatives, bal_error_rate)\n",
    "    return true_positives, true_negatives, false_positives, false_negatives, bal_error_rate\n",
    "\n",
    "Q5(datasetB,featureQ5)\n",
    "Q5(datasetB,featureQ7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81595ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.int64(14201),\n",
       " np.int64(10503),\n",
       " np.int64(5885),\n",
       " np.int64(19411),\n",
       " np.float64(0.4683031525957275))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = {'review/text': \"A lot of foam. But a lot.\\tIn the smell some banana, and then lactic and tart. Not a good start.\\tQuite dark orange in color, with a lively carbonation (now visible, under the foam).\\tAgain tending to lactic sourness.\\tSame for the taste. With some yeast and banana.\"}\n",
    "count_tabs = a['review/text'].count('\\t')\n",
    "print(count_tabs)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, balanced_accuracy_score\n",
    "import numpy as np\n",
    "def featureQ5(datum):\n",
    "    # Feature vector for one data point\n",
    "    feature = len(datum['review/text'])\n",
    "    return [1] + [feature]\n",
    "\n",
    "def Q5(dataset,feat_func):\n",
    "\n",
    "    # build feature(X) and target(y) lists\n",
    "    X = np.array([feat_func(d) for d in dataset])\n",
    "    y = np.array([d['review/overall'] >=4 for d in dataset])\n",
    "\n",
    "    # initalize sklearn logistic regression model\n",
    "    log_reg = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "    # fit logistic regression model\n",
    "    log_reg.fit(X,y)\n",
    "\n",
    "    # make predictions using log_reg model\n",
    "    y_pred = log_reg.predict(X)\n",
    "\n",
    "    #define true condition True:1 and False:0\n",
    "    y_true = y.astype(int) \n",
    "    y_pred_int = y_pred.astype(int)\n",
    "\n",
    "    # find true positives\n",
    "    true_positives = np.sum(y_true * y_pred_int)\n",
    "\n",
    "    #find true negatives\n",
    "    true_negatives = np.sum((1 - y_true) * (1 - y_pred_int))\n",
    "\n",
    "    # find false positives\n",
    "    false_positives = np.sum((1 - y_true) * y_pred_int)\n",
    "\n",
    "    # find false negatives\n",
    "    false_negatives = np.sum(y_true * (1 - y_pred_int))\n",
    "\n",
    "    # Calculate balanced error rate (1-accuracy)\n",
    "    bal_accuracy = balanced_accuracy_score(y, y_pred)\n",
    "    bal_error_rate = 1 - bal_accuracy\n",
    "    return true_positives, true_negatives, false_positives, false_negatives, bal_error_rate\n",
    "    FP = false_positives\n",
    "    TN = true_negatives\n",
    "    FN = false_negatives\n",
    "    TP = true_positives\n",
    "    BER = 0.5 * (FP / (TN + FP) + FN / (FN + TP))\n",
    "\n",
    "    #print(f\"sklearn:{balanced_err_rate}\")\n",
    "    #print(f\"comp:{BER}\")\n",
    "    print((TP, TN, FP, FN, BER))\n",
    "    \n",
    "\n",
    "Q5(datasetB,featureQ5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb03eec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'> 1.0\n",
      "<class 'float'> 0.8\n",
      "<class 'float'> 0.75\n",
      "<class 'float'> 0.7147\n",
      "!!![1.0, 0.75, 0.71, 0.7147]\n",
      "<class 'float'> 1.0\n",
      "<class 'float'> 0.75\n",
      "<class 'float'> 0.71\n",
      "<class 'float'> 0.7147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0, 0.75, 0.71, 0.7147]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Q6(dataset):\n",
    "    def featureQ5(datum):\n",
    "        # Feature vector for one data point\n",
    "        feature = len(datum['review/text'])\n",
    "        return [1] + [feature]\n",
    "\n",
    "    # build feature(X) and target(y) lists\n",
    "    X = np.array([featureQ5(d) for d in dataset])\n",
    "    y = np.array([d['review/overall'] >=4 for d in dataset])\n",
    "\n",
    "    # initalize sklearn logistic regression model\n",
    "    log_reg = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "    # fit logistic regression model\n",
    "    log_reg.fit(X,y)\n",
    "\n",
    "    # make predictions using log_reg model\n",
    "    y_pred = log_reg.predict(X)\n",
    "\n",
    "    # grab col 2 probablities since intrested where 'review/overall' >=4\n",
    "    probs = log_reg.predict_proba(X)[:, 1]\n",
    "\n",
    "    # sort in decending order based on probability from log_reg\n",
    "    sorted_indices = np.argsort(probs)[::-1]\n",
    "    \n",
    "    # sort true targets based on probs from log_reg\n",
    "    sorted_y = y[sorted_indices]\n",
    "\n",
    "    # define k values to test\n",
    "    k_vals = np.array([1,10,100,10000])\n",
    "    \n",
    "    # calc sum of correct predictions \n",
    "    true_counts = np.cumsum(sorted_y)\n",
    "    \n",
    "    # calc precision for each k using numpy linear algebra\n",
    "    precs = true_counts[k_vals - 1] / k_vals\n",
    "    precs = [float(i) for i in precs]\n",
    "    for i in precs:\n",
    "        print(type(i),i)\n",
    "    return precs\n",
    "def Q6_test(dataset):\n",
    "    X = [[1, len(d['review/text'])] for d in dataset]\n",
    "    y = [d['review/overall'] >= 4 for d in dataset]\n",
    "    \n",
    "    mod = LogisticRegression(class_weight='balanced')\n",
    "    mod.fit(X,y)\n",
    "    predictions = mod.predict(X) # Binary vector of predictions\n",
    "    scores = [x[1] for x in mod.predict_proba(X)]\n",
    "\n",
    "    sortedScores = list(zip(scores,y))\n",
    "    sortedScores.sort(reverse=True)\n",
    "    sortedLabels = [x[1] for x in sortedScores]\n",
    "    \n",
    "    precs = []\n",
    "    \n",
    "    for k in [1,100,1000,10000]: # Not efficient, but fine\n",
    "        precK = sum(sortedLabels[:k]) / k\n",
    "        precs.append(precK)\n",
    "        #print(\"Precision@\" + str(k) + \" = \" + str(precK))\n",
    "    print(f\"!!!{precs}\")\n",
    "    for i in precs:\n",
    "        print(type(i),i)\n",
    "    return precs\n",
    "\n",
    "Q6(datasetB)\n",
    "Q6_test(datasetB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "386a9061",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     60\u001b[39m     bal_error_rate = \u001b[32m1\u001b[39m - bal_accuracy\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m true_positives, true_negatives, false_positives, false_negatives, bal_error_rate\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m Q5(\u001b[43mdataset\u001b[49m,featureQ5)\n\u001b[32m     64\u001b[39m Q5(dataset,featureQ7)\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "def featureQ5(datum):\n",
    "    # Feature vector for one data point\n",
    "    feature = len(datum['review/text'])\n",
    "    return [1] + [feature]\n",
    "\n",
    "def featureQ7(datum):\n",
    "    # Implement (any feature vector which improves performance over Q5)\n",
    "    appearance = datum['review/appearance']\n",
    "    palate = datum['review/palate']\n",
    "    taste = datum['review/taste']\n",
    "    aroma = datum['review/aroma']\n",
    "    abv = datum.get('beer/ABV', 0) \n",
    "    \n",
    "    # calc weighted sum using results from PCA\n",
    "    quality_score = (0.41 * appearance + \n",
    "                     0.47 * palate + \n",
    "                     0.48 * taste + \n",
    "                     0.47 * aroma + \n",
    "                     0.32 * abv)\n",
    "    return [1]+ [quality_score]\n",
    "\n",
    "def Q5(dataset,feat_func):\n",
    "\n",
    "    # build feature(X) and target(y) lists\n",
    "    X = np.array([feat_func(d) for d in dataset])\n",
    "    y = np.array([d['review/overall'] >=4 for d in dataset])\n",
    "\n",
    "    # initalize sklearn logistic regression model\n",
    "    log_reg = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "    # fit logistic regression model\n",
    "    log_reg.fit(X,y)\n",
    "\n",
    "    # make predictions using log_reg model\n",
    "    y_pred = log_reg.predict(X)\n",
    "\n",
    "    #define true condition True:1 and False:0\n",
    "    y_true = y.astype(int) \n",
    "    y_pred_int = y_pred.astype(int)\n",
    "\n",
    "    # find true positives\n",
    "    true_positives = np.sum(y_true * y_pred_int)\n",
    "\n",
    "    #find true negatives\n",
    "    true_negatives = np.sum((1 - y_true) * (1 - y_pred_int))\n",
    "\n",
    "    # find false positives\n",
    "    false_positives = np.sum((1 - y_true) * y_pred_int)\n",
    "\n",
    "    # find false negatives\n",
    "    false_negatives = np.sum(y_true * (1 - y_pred_int))\n",
    "\n",
    "    # Calculate balanced error rate (1-accuracy)\n",
    "    bal_accuracy = balanced_accuracy_score(y, y_pred)\n",
    "    bal_error_rate = 1 - bal_accuracy\n",
    "    return true_positives, true_negatives, false_positives, false_negatives, bal_error_rate\n",
    "\n",
    "Q5(dataset,featureQ5)\n",
    "Q5(dataset,featureQ7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecc5a342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQ1():\n",
    "    theta1, MSE1 = homework1.Q1(dataset)\n",
    "    # The autograder will compare these answers to a reference solution\n",
    "    print(theta1)\n",
    "    print(MSE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4230136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQ2():\n",
    "    X, Y, MSE = homework1.Q2(dataset)\n",
    "    print((X[0],Y[0],MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bea736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQ3():\n",
    "    X, Y, MSE = homework1.Q3(dataset)\n",
    "    print((X[0],Y[0],MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae790154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQ4():\n",
    "    test_mse2, test_mse3 = homework1.Q4(dataset4)\n",
    "    print((test_mse2, test_mse3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede95f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQ5():\n",
    "    TP, TN, FP, FN, BER = homework1.Q5(datasetB, homework1.featureQ5)\n",
    "    print((TP, TN, FP, FN, BER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622bb102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQ6():\n",
    "    precs = homework1.Q6(datasetB)\n",
    "    print(precs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633cbbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQ7():\n",
    "    _, _, _, _, BER5 = homework1.Q5(datasetB, homework1.featureQ5)\n",
    "    _, _, _, _, BER7 = homework1.Q5(datasetB, homework1.featureQ7)\n",
    "    print((BER5,BER7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00360d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e56353ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'review'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtestQ1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mtestQ1\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtestQ1\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     theta1, MSE1 = \u001b[43mhomework1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mQ1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# The autograder will compare these answers to a reference solution\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(theta1)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dsc_256/fall_25/module_01/hw/homework1.py:50\u001b[39m, in \u001b[36mQ1\u001b[39m\u001b[34m(dataset)\u001b[39m\n\u001b[32m     48\u001b[39m maxLen = getMaxLen(dataset)\n\u001b[32m     49\u001b[39m X = [featureQ1(d,maxLen) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dataset]\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m y = [\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreview\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dataset]\n\u001b[32m     52\u001b[39m theta,residuals,rank,s = np.linalg.lstsq(X, y)\n\u001b[32m     53\u001b[39m MSE = residuals/\u001b[38;5;28mlen\u001b[39m(y)\n",
      "\u001b[31mKeyError\u001b[39m: 'review'"
     ]
    }
   ],
   "source": [
    "testQ1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4252f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "testQ2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f0ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testQ3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55aa483",
   "metadata": {},
   "outputs": [],
   "source": [
    "testQ4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a37086",
   "metadata": {},
   "outputs": [],
   "source": [
    "testQ5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a7b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "testQ6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76de3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "testQ7()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c84736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
