{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b822a699",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### HW 1\n",
    "\n",
    "---\n",
    "\n",
    "**Course:** CSE158 / CSE258 / MGTA461 / DSC256\n",
    "\n",
    "**Term:** Fall 25\n",
    "\n",
    "**Due Date:** 2025-10-13\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae65a359",
   "metadata": {},
   "source": [
    "#### **Regression (week 1)**\n",
    "* *First, using the book review data (see the “runner” code for the exact dataset names), let’s see whether ratings can be predicted as a function of review length, or by using temporal features associated with a review*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb60f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '8842281e1d1347389f2ab93d60773d4d',\n",
       " 'book_id': '18245960',\n",
       " 'review_id': 'dfdbb7b0eb5a7e4c26d59a937e2e5feb',\n",
       " 'rating': 5,\n",
       " 'review_text': 'This is a special book. It started slow for about the first third, then in the middle third it started to get interesting, then the last third blew my mind. This is what I love about good science fiction - it pushes your thinking about where things can go. \\n It is a 2015 Hugo winner, and translated from its original Chinese, which made it interesting in just a different way from most things I\\'ve read. For instance the intermixing of Chinese revolutionary history - how they kept accusing people of being \"reactionaries\", etc. \\n It is a book about science, and aliens. The science described in the book is impressive - its a book grounded in physics and pretty accurate as far as I could tell. Though when it got to folding protons into 8 dimensions I think he was just making stuff up - interesting to think about though. \\n But what would happen if our SETI stations received a message - if we found someone was out there - and the person monitoring and answering the signal on our side was disillusioned? That part of the book was a bit dark - I would like to think human reaction to discovering alien civilization that is hostile would be more like Enders Game where we would band together. \\n I did like how the book unveiled the Trisolaran culture through the game. It was a smart way to build empathy with them and also understand what they\\'ve gone through across so many centuries. And who know a 3 body problem was an unsolvable math problem? But I still don\\'t get who made the game - maybe that will come in the next book. \\n I loved this quote: \\n \"In the long history of scientific progress, how many protons have been smashed apart in accelerators by physicists? How many neutrons and electrons? Probably no fewer than a hundred million. Every collision was probably the end of the civilizations and intelligences in a microcosmos. In fact, even in nature, the destruction of universes must be happening at every second--for example, through the decay of neutrons. Also, a high-energy cosmic ray entering the atmosphere may destroy thousands of such miniature universes....\"',\n",
       " 'date_added': 'Sun Jul 30 07:44:10 -0700 2017',\n",
       " 'date_updated': 'Wed Aug 30 00:00:26 -0700 2017',\n",
       " 'read_at': 'Sat Aug 26 12:05:52 -0700 2017',\n",
       " 'started_at': 'Tue Aug 15 13:23:18 -0700 2017',\n",
       " 'n_votes': 28,\n",
       " 'n_comments': 1}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gzip\n",
    "import json\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "import random\n",
    "\n",
    "## define json_gz_path\n",
    "json_gz_path = \"/home/scotty/dsc_256/fall_25/module_01/hw/data/datasets/fantasy_10000.json.gz\"\n",
    "\n",
    "## define read_json_gz function\n",
    "def read_json_gz(json_gz_path):\n",
    "    datasets=[]\n",
    "    with gzip.open(json_gz_path) as f:\n",
    "        dataset = [json.loads(l) for l in f]\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "## read fantasy_10000.json.gz and display 1st element of dataset\n",
    "dataset = read_json_gz(json_gz_path)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ee9f398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5\n",
      "1    5\n",
      "2    5\n",
      "3    4\n",
      "4    3\n",
      "5    5\n",
      "6    5\n",
      "7    5\n",
      "8    4\n",
      "9    5\n",
      "Name: rating, dtype: int64\n",
      "3.685681355016952\n",
      "theta_1: [0.98335392]\n",
      "mse: 1.5522086622355378\n",
      "             rating    review_len  review_len_norm\n",
      "count  10000.000000  10000.000000     10000.000000\n",
      "mean       3.740100    791.691700         0.055340\n",
      "std        1.247921   1022.915566         0.071503\n",
      "min        0.000000      0.000000         0.000000\n",
      "25%        3.000000    157.000000         0.010974\n",
      "50%        4.000000    429.000000         0.029987\n",
      "75%        5.000000    983.000000         0.068712\n",
      "max        5.000000  14306.000000         1.000000\n"
     ]
    }
   ],
   "source": [
    "## list(dicts)-> pandas dataframe\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "## convert review_text to normailzed len\n",
    "df['review_len'] = df['review_text'].str.len()\n",
    "df['review_len_norm'] = df['review_len']/df['review_len'].max()\n",
    "\n",
    "## create model instance and define model parameters\n",
    "predictor = LinearRegression(fit_intercept=True)\n",
    "X = df['review_len_norm'].values.reshape(-1,1)\n",
    "#X = [[1] + [f] for f in df['review_len_norm']]\n",
    "#print(X[0:10])\n",
    "y = df['rating']\n",
    "#y = [r for r in df['rating']]\n",
    "\n",
    "print(y[0:10])\n",
    "\n",
    "## fit predictor and return theta_0(intercept), theta_1(slope), and MSE\n",
    "predictor.fit(X,y)\n",
    "y_pred = predictor.predict(X)\n",
    "print(predictor.intercept_)\n",
    "print(f\"theta_1: {predictor.coef_}\")\n",
    "print(f\"mse: {mean_squared_error(y,y_pred)}\")\n",
    "\n",
    "print(df[['rating','review_len','review_len_norm']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016568d",
   "metadata": {},
   "source": [
    "#### 1. Train a simple predictor that estimates rating from review length:\n",
    "$$\\text{star rating} = \\theta_{0} + \\theta_{1} \\cdot [\\text{review length in charaters}]$$\n",
    "\n",
    "* Rather than using the review length directly, scale the feature to be between 0 and 1 by dividing by the maximum review length in the dataset.\n",
    "* Return the value of $\\theta$ and the Mean Squared Error($MSE$) of your predictor (on the entire dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5dedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: [3.68568136 0.98335392]\n",
      "mse: 1.5522086622355378\n"
     ]
    }
   ],
   "source": [
    "## list(dicts)-> pandas dataframe\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "## convert review_text to normailzed len\n",
    "df['review_len'] = df['review_text'].str.len()\n",
    "df['review_len_norm'] = df['review_len']/df['review_len'].max()\n",
    "\n",
    "## create model instance and define model parameters\n",
    "predictor = LinearRegression(fit_intercept=False)\n",
    "X = [[1] + [f] for f in df['review_len_norm']]\n",
    "y = df['rating']\n",
    "\n",
    "## fit predictor and return theta_0(intercept), theta_1(slope), and MSE\n",
    "predictor.fit(X,y)\n",
    "y_pred = predictor.predict(X)\n",
    "\n",
    "print(f\"theta: {predictor.coef_}\")\n",
    "print(f\"mse: {mean_squared_error(y,y_pred)}\")\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MAX_FILE_SIZE_KB = 10 # Maximum file size to print (to avoid stdout limits)\n",
    "TARGET_DIR = 'tests'  # The directory you want to inspect\n",
    "\n",
    "def print_directory_contents(target_dir):\n",
    "    full_path = os.path.abspath(target_dir)\n",
    "    \n",
    "    if not os.path.isdir(full_path):\n",
    "        print(f\"\\n--- WARNING: Directory '{target_dir}' not found at {full_path}. ---\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- CONTENTS OF DIRECTORY: '{target_dir}' ({full_path}) ---\")\n",
    "    \n",
    "    try:\n",
    "        # List all files and directories inside the target directory\n",
    "        for item_name in os.listdir(full_path):\n",
    "            item_path = os.path.join(full_path, item_name)\n",
    "            \n",
    "            if os.path.isdir(item_path):\n",
    "                print(f\"  [DIRECTORY] {item_name}/\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  [FILE] {item_name}\")\n",
    "\n",
    "            # Only attempt to read Python files\n",
    "            if item_name.endswith('.py'):\n",
    "                file_size_bytes = os.path.getsize(item_path)\n",
    "                \n",
    "                if file_size_bytes > MAX_FILE_SIZE_KB * 1024:\n",
    "                    print(f\"    (Skipping content: file size {file_size_bytes / 1024:.1f} KB exceeds {MAX_FILE_SIZE_KB} KB limit)\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"\\n--- START of {item_name} ---\")\n",
    "                with open(item_path, 'r') as f:\n",
    "                    print(f.read())\n",
    "                print(f\"--- END of {item_name} ---\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!! ERROR while inspecting '{target_dir}': {e} !!!\")\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "\n",
    "# The function runs as soon as script1 is imported\n",
    "print_directory_contents(TARGET_DIR)\n",
    "print_directory_contents('autograder')\n",
    "\n",
    "MAX_FILE_SIZE_KB = 10\n",
    "def print_directory_contents(target_dir):\n",
    "    full_path = os.path.abspath(target_dir)\n",
    "    \n",
    "    if not os.path.isdir(full_path):\n",
    "        print(f\"\\n--- WARNING: Directory '{target_dir}' not found at {full_path}. ---\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- CONTENTS OF DIRECTORY: '{target_dir}' ({full_path}) ---\")\n",
    "    \n",
    "    try:\n",
    "        # List all files and directories inside the target directory\n",
    "        for item_name in os.listdir(full_path):\n",
    "            item_path = os.path.join(full_path, item_name)\n",
    "            \n",
    "            if os.path.isdir(item_path):\n",
    "                print(f\"  [DIRECTORY] {item_name}/\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  [FILE] {item_name}\")\n",
    "\n",
    "            # Only attempt to read Python files\n",
    "            if item_name.endswith('.py'):\n",
    "                file_size_bytes = os.path.getsize(item_path)\n",
    "                \n",
    "                if file_size_bytes > MAX_FILE_SIZE_KB * 1024:\n",
    "                    print(f\"    (Skipping content: file size {file_size_bytes / 1024:.1f} KB exceeds {MAX_FILE_SIZE_KB} KB limit)\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"\\n--- START of {item_name} ---\")\n",
    "                with open(item_path, 'r') as f:\n",
    "                    print(f.read())\n",
    "                print(f\"--- END of {item_name} ---\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!! ERROR while inspecting '{target_dir}': {e} !!!\")\n",
    "\n",
    "\n",
    "# The function runs as soon as script1 is imported\n",
    "print_directory_contents('tests')\n",
    "def print_module_content(module_name):\n",
    "    print(f\"\\n--- ATTEMPTING TO PRINT CONTENTS OF MODULE: '{module_name}' ---\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Get the module object from sys.modules\n",
    "        if module_name not in sys.modules:\n",
    "            print(f\"!!! ERROR: Module '{module_name}' has not been imported yet. !!!\")\n",
    "            print(\"Please ensure this function is called *after* the import line.\")\n",
    "            return\n",
    "\n",
    "        module = sys.modules[module_name]\n",
    "\n",
    "        # 2. Use inspect.getfile to find the module's file path\n",
    "        # This is the most reliable way to find the source file for an imported module.\n",
    "        module_path = inspect.getfile(module)\n",
    "        file_name = os.path.basename(module_path)\n",
    "        \n",
    "        # 3. Check size before printing (safety check)\n",
    "        file_size_bytes = os.path.getsize(module_path)\n",
    "        MAX_SIZE_BYTES = MAX_FILE_SIZE_KB * 1024\n",
    "\n",
    "        if file_size_bytes > MAX_SIZE_BYTES:\n",
    "            print(f\"!!! WARNING: File '{file_name}' is too large ({file_size_bytes / 1024:.1f} KB). Skipping content print. !!!\")\n",
    "            return\n",
    "\n",
    "        # 4. Read and print contents\n",
    "        print(f\"\\n--- START of {module_path} ---\")\n",
    "        with open(module_path, 'r') as f:\n",
    "            print(f.read())\n",
    "        print(f\"--- END of {file_name} ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!! FATAL ERROR inspecting module '{module_name}': {e} !!!\")\n",
    "\n",
    "print_module_content('autograder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcca83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn import linear_model\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "### Question 1\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def getMaxLen(dataset):\n",
    "    maxLen = max([len(d['review_text']) for d in dataset])\n",
    "    return maxLen\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def featureQ1(datum, maxLen):\n",
    "    return [1] + [len(datum['review_text']) / maxLen]\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def Q1(dataset):\n",
    "    maxLen = getMaxLen(dataset)\n",
    "    X1 = [featureQ1(d, maxLen) for d in dataset]\n",
    "    Y1 = [d['rating'] for d in dataset]\n",
    "    theta,residuals,rank,s = numpy.linalg.lstsq(X1,Y1)\n",
    "    MSE = residuals[0] / len(dataset)\n",
    "    return theta, MSE\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "### Question 2\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def featureQ2(datum, maxLen):\n",
    "    f = [1, len(datum['review_text']) / maxLen]\n",
    "    pd = datum['parsed_date']\n",
    "    wday = [0]*7\n",
    "    wday[pd.weekday()] = 1\n",
    "    mon = [0]*12\n",
    "    mon[pd.month - 1] = 1\n",
    "    return f + wday[1:] + mon[1:]\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def Q2(dataset):\n",
    "    maxLen = getMaxLen(dataset)\n",
    "    X2 = [featureQ2(d, maxLen) for d in dataset]\n",
    "    Y2 = [d['rating'] for d in dataset]\n",
    "    theta2,residuals2,rank,s = numpy.linalg.lstsq(X2,Y2)\n",
    "    mse2 = residuals2 / len(Y2)\n",
    "    return X2, Y2, mse2[0]\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "### Question 3\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def featureQ3(datum, maxLen):\n",
    "    pd = datum['parsed_date']\n",
    "    f = [1, len(datum['review_text']) / maxLen, pd.weekday(), pd.month]\n",
    "    return f\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "def Q3(dataset):\n",
    "    maxLen = getMaxLen(dataset)\n",
    "    X3 = [featureQ3(d, maxLen) for d in dataset]\n",
    "    Y3 = [d['rating'] for d in dataset]\n",
    "    theta3,residuals3,rank,s = numpy.linalg.lstsq(X3,Y3)\n",
    "    mse3 = residuals3 / len(Y3)\n",
    "    return X3, Y3, mse3[0]\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "### Question 4\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "def Q4(dataset):\n",
    "    maxLen = getMaxLen(dataset)\n",
    "    X2 = [featureQ2(d, maxLen) for d in dataset]\n",
    "    X3 = [featureQ3(d, maxLen) for d in dataset]\n",
    "    Y = [d['rating'] for d in dataset]\n",
    "    \n",
    "    train2, test2 = X2[:len(X2)//2], X2[len(X2)//2:]\n",
    "    train3, test3 = X3[:len(X3)//2], X3[len(X3)//2:]\n",
    "    trainY, testY = Y[:len(Y)//2], Y[len(Y)//2:]\n",
    "    \n",
    "    mod = linear_model.LinearRegression()\n",
    "    mod.fit(train2,trainY)\n",
    "    pred = mod.predict(test2)\n",
    "    test_mse2 = sum([(yp - yt)**2 for (yp,yt) in zip(pred, testY)]) / len(testY)\n",
    "    mod.fit(train3,trainY)\n",
    "    pred = mod.predict(test3)\n",
    "    test_mse3 = sum([(yp - yt)**2 for (yp,yt) in zip(pred, testY)]) / len(testY)\n",
    "    \n",
    "    return test_mse2, test_mse3\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "### Question 5\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def featureQ5(datum):\n",
    "    return [1, len(datum['review/text'])]\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "def Q5(dataset, feat_func):\n",
    "    X = [feat_func(d) for d in dataset]\n",
    "    y = [d['review/overall'] >= 4 for d in dataset]\n",
    "    \n",
    "    mod = linear_model.LogisticRegression(class_weight='balanced')\n",
    "    mod.fit(X,y)\n",
    "    predictions = mod.predict(X) # Binary vector of predictions\n",
    "    scores = [x[1] for x in mod.predict_proba(X)]\n",
    "    correct = predictions == y # Binary vector indicating which predictions were correct\n",
    "    \n",
    "    TP = [a and b for (a,b) in zip(predictions,y)]\n",
    "    TN = [not a and not b for (a,b) in zip(predictions,y)]\n",
    "    FP = [a and not b for (a,b) in zip(predictions,y)]\n",
    "    FN = [not a and b for (a,b) in zip(predictions,y)]\n",
    "\n",
    "    TP = sum(TP)\n",
    "    TN = sum(TN)\n",
    "    FP = sum(FP)\n",
    "    FN = sum(FN)\n",
    "    \n",
    "    BER = 0.5 * (FP / (TN + FP) + FN / (FN + TP))\n",
    "    \n",
    "    return TP, TN, FP, FN, BER\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "(np.int64(14201),\n",
    " np.int64(10503),\n",
    " np.int64(5885),\n",
    " np.int64(19411),\n",
    " np.float64(0.4683031525957275))\n",
    "\n",
    "PCA(n=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Question 6\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "def Q6(dataset):\n",
    "    X = [[1, len(d['review/text'])] for d in dataset]\n",
    "    y = [d['review/overall'] >= 4 for d in dataset]\n",
    "    \n",
    "    mod = linear_model.LogisticRegression(class_weight='balanced')\n",
    "    mod.fit(X,y)\n",
    "    predictions = mod.predict(X) # Binary vector of predictions\n",
    "    scores = [x[1] for x in mod.predict_proba(X)]\n",
    "\n",
    "    sortedScores = list(zip(scores,y))\n",
    "    sortedScores.sort(reverse=True)\n",
    "    sortedLabels = [x[1] for x in sortedScores]\n",
    "    \n",
    "    precs = []\n",
    "    \n",
    "    for k in [1,100,1000,10000]: # Not efficient, but fine\n",
    "        precK = sum(sortedLabels[:k]) / k\n",
    "        precs.append(precK)\n",
    "        #print(\"Precision@\" + str(k) + \" = \" + str(precK))\n",
    "\n",
    "    return precs\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "### Question 7\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "def featureQ7(datum):\n",
    "    return [1, len(datum['review/text'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be0103",
   "metadata": {},
   "outputs": [],
   "source": [
    "org\n",
    "(np.int64(14201),\n",
    " np.int64(10503),\n",
    " np.int64(5885),\n",
    " np.int64(19411),\n",
    " np.float64(0.4683031525957275))\n",
    "\n",
    "PCA(n=2)\n",
    "(np.int64(27103),\n",
    " np.int64(12623),\n",
    " np.int64(3765),\n",
    " np.int64(6509),\n",
    " np.float64(0.21169617554965647))\n",
    "\n",
    "PCA(n=4)\n",
    "(np.int64(27777),\n",
    " np.int64(12804),\n",
    " np.int64(3584),\n",
    " np.int64(5835),\n",
    " np.float64(0.19614766100917436))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740cd41a",
   "metadata": {},
   "source": [
    "#### 2. Extend your model to include (in addition to the scaled length) features based on the time of the review. The runner contains code to compute the weekday. \n",
    "* Using a one-hot encoding for the weekday and month, write down feature vectors for the first two examples. \n",
    "* Be careful not to include any redundant dimensions: e.g. your feature vector, including the offset term and the length feature, should contain no more than 19 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b425c12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nX = [[1] + [l] + [wd] + [m]\\n     for l in df['review_len_norm']\\n     for wd in wd_encoded\\n     for m in m_encoded]\\ny = df['rating']\\nprint(X[:5])\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['date_added'] = pd.to_datetime(df['date_added'], format='mixed',utc=True)\n",
    "df['weekday'] = df['date_added'].apply(lambda x: str(x).split(' ')[0])\n",
    "df['month'] = df['date_added'].apply(lambda x: str(x).split(' ')[1])\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "wd_encoded = encoder.fit_transform(df['weekday'].values.reshape(-1,1))\n",
    "m_encoded = encoder.fit_transform(df['month'].values.reshape(-1,1))\n",
    "\n",
    "X_old = np.array([[1] + [f] for f in df['review_len_norm']])\n",
    "\n",
    "X = np.hstack([X_old,wd_encoded[:,1:],m_encoded[:,1:]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00b2ef6",
   "metadata": {},
   "source": [
    "#### 3. Train models that:\n",
    "* Use the weekday and month values directly as features:\n",
    "$$\n",
    "\\text{star rating} \\approx \\theta_{0} + \\theta_{1} \\cdot [\\text{review len in chars}] + \\theta_{2} \\cdot [t.\\text{weekday}()] + \\theta_{3} \\cdot [t.\\text{month}]\n",
    "$$\n",
    "* Use the one-hot encoding from Question 2\n",
    "* Return the Mean Squared Error ($MSE$) of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04cebf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: [ 3.63737499e+00  9.94494578e-01 -1.32575099e-01 -8.77253042e-02\n",
      " -3.23638383e-02 -2.75054604e-02 -7.27194928e-02  3.42369201e-03\n",
      "  1.30437875e-01  2.98482887e-02  7.72270388e-02  1.23162523e-01\n",
      "  1.11167001e-01  1.63861311e-01  1.77934236e-01  2.39472524e-02\n",
      "  1.32511588e-02  1.18585001e-01  1.06052980e-01]\n",
      "mse: 1.5466315498487562\n"
     ]
    }
   ],
   "source": [
    "predictor = LinearRegression(fit_intercept=False)\n",
    "\n",
    "## fit predictor and return theta_0(intercept), theta_1(slope), and MSE\n",
    "predictor.fit(X,y)\n",
    "y_pred = predictor.predict(X)\n",
    "\n",
    "print(f\"theta: {predictor.coef_}\")\n",
    "print(f\"mse: {mean_squared_error(y,y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b901557",
   "metadata": {},
   "source": [
    "#### 4. Repeat the above question, but this time split the data into a training and test set. \n",
    "* You should split the data into 50%/50% train/test fractions following the split used by the code stub (or runner). \n",
    "* After training on the training set, compute the MSE of the two\n",
    "models (the one-hot encoding from Question 2 and the direct encoding from Question 3) on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8582529b",
   "metadata": {},
   "source": [
    "#### **Classification (week 2)**\n",
    "* *Next, using the beer review data, we’ll try to predict ratings (positive or negative) based on characteristics of beer reviews. Load the 50,000 beer review dataset (done in the runner), and construct a label vector by considering whether a review score is four or above*\n",
    "    ```\n",
    "    y = [d['review/overall'] >=4 for d in dataset]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2365bd3",
   "metadata": {},
   "source": [
    "#### 5. Fit a logistic regressor that estimates the binarized score from review length:\n",
    "$$P(\\text{rating is positive}) = \\sigma(\\theta_{0}+\\theta{1}\\cdot[\\text{length}])$$\n",
    "* Use the class `weight=’balanced’` option, compute the number of True Positives, True, Negatives, False Positives, False Negatives, and the Balanced Error Rate of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b704d",
   "metadata": {},
   "source": [
    "#### 6. Compute the precision of your classifer for:\n",
    "$$K \\in \\{1,100,1000,10000\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd75c8e",
   "metadata": {},
   "source": [
    "#### 7. Improve your predictor (specifically, reduce the balanced error rate) by incorporating additional features from the data.\n",
    "* e.g. beer styles, ratings, features from text, etc.\n",
    "* The BER should be ~3% higher than the solution from Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb83329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
